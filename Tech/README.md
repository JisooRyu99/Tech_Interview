- [#Statistics/Math](#statistics-math)
- [#Machine Learning](#Machine-Learning)
- [#Deep Learning](#deep-learning)
- [#Operating System](#operating-system)
- [#Data Structure](#data-structure)
- [#Computer Science](#computer-science)
- [#Python](#python)
- [#Database](#Database)

## #Statistics/Math
### “likelihood”와 “probability”의 차이는 무엇일까요?
**확률**은 어떤 시행에서 특정 결과가 나올 가능성을 말하며 총합은 1이다. 반면에 **가능도**는 실재가 바탕이 되어야 하고 어떤 시행을 충분히 수행한 뒤 그 결과를 토대로 겨우의 수의 가능성을 도출하는 것이다. 어디까지나 추이기 때문에 가능성의 합이 1이 되지 않을 수도 있다.

### 중심극한정리는 왜 유용한걸까요?
- 중심극한정리가 강력한 이유는 모집단의 형태가 어떻든지 간에 상관없이 표본 평균의 분포는 정규분포를 따르게 된다는 점에 있다.
- 모집단의 분포가 무엇이든, 분포를 모르더라도, 표본의 크기만 보장된다면 표본평균의 분포는 모수를 기반으로한 정규분포를 띄기에 이를 통해 모수를 추정하는데에 사용될 수 있기 때문

### 아웃라이어의 판단하는 기준은 무엇인가요?
- 이상치를 탐지하는 방법 중 가장 널리 사용되는 것은 IQR를 사용하는 것이다. 이 방법은 이사분범위를 바탕으로 Q3 + 1.5*IQR 이상, Q1–1.5*IQR 이하의 값을 outlier로 정의하는 것이다.
![image](https://user-images.githubusercontent.com/90206705/229514213-4ffb5d4e-3dce-4eb6-97f4-abe7b8ea5b4f.png)
- 평균에서 벗어날수록, 표준편차가 커질수록 이상치일 가능성이 높아진다.

### 모집단과 표본의 차이는 무엇인가요?
**모집단**은 어떤 정보를 얻고자 하는 전체 대상 또는 전체 집합을 의미하고 **표본집단**은 모집단으로부터 추출된 모집단의 부분 집합이다.


### 공분산과 상관계수는 무엇일까요?
분산은 한 개의 확률 변수가 주어질 때, 그 변수가 평균적으로부터 평균적으로 얼마나 떨어져있는지(분포되어 있는지)를 타나냅니다. 비슷하게 **공분산은 2개의 확률 변수가 주어질 때, 두 확률 변수가 얼마나 다른지(상관이 있는지) 알 수 있습니다.** 공분산이 양수인 경우, X와 Y는 서로 증감에 대해 비례 관계를 갖고 음수인 경우, X와 Y는 서로 증감에 대해 반비례 관계를 갖습니다.
공분산은 두 확률 변수가 얼마나 상관이 있는지를 나타내는 것이지만 각 확률변수의 단위를 포함하게 됩니다. 단위가 포함되면 객관적으로 얼마나 차이가 있는지 확인하기 어렵습니다. 이때 **상관 계수는 공분산을 X와 Y의 표준편차로 나누어준 값으로 공분산과 다르게 단위를 갖지 않습니다.** 상관계수는 -1~1 사이의 값을 가지게 되고 -1에 가까울 수록 음의 상관이 강해지고 1에 가까울수록 양의 상관에, 0에 가까운 값을 가지면 둘의 상관관계가 거의 존재하지 않습니다.

### 조건부 확률은 무엇일까요?
조건부 확률은 어떤 사건이 일어났다는 전제 하에 다른 사건이 일어날 확률을 말합니다.
- 꼬리문제
  - 조건부 확률과 관련된 유명한 문제 : 몬티 홀 문제 

### missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?
누락된 값이 많은 데이터셋으로 머신러닝 모델을 학습시키면 모델의 품질에 큰 영향을 미칩니다. 

[해결 방안]
이를 해결하기 위한 가장 쉬운 방법은 누락된 데이터를 제거하는 것입니다. 하지만 중요한 정보를 가진 데이터를 잃을 위험을 가지고 있습니다. 또 한 컬럼에 있는 missing value를 결측 되지 않은 다른 값의 평균이나 중앙값으로 대체하는 것입니다. 이 방법은 쉽고 작은 빠르다는 장점이 있지만 다른 feature 간의 상관관계가 고려되지 않고 인코딩 된 범주형 feature에 대해 안 좋은 결과를 제공한다는 단점을 가지고 있습니다. 최빈값, 0, 임의 값으로 채워넣을 수도 있습니다. 이 방식은 쉽고 범주형 feature에 잘 동작한다는 장점이 있지만 이것 또한 다른 feature 간의 상관관계가 고려되지 않고 데이터에 bias를 만든다는 단점이 있습니다.
머신러닝을 통해 해결하는 방법으로는 KNN으로 가까운 이웃간의 거리기반 분석을 통해 채워넣을 수 있습니다. mean, median이나 most fequent보다 정확할 때가 많지만 메모리가 많이 필요한 방법입니다.

###  엔트로피(Entropy)에 대해 설명해주세요. 가능하면 정보이득(Information Gain)도요.

엔트로피는 정보 이론에서 사용되는 개념으로 1로 갈수록 불순하고, 0으로 갈수록 불순하지 않다는 의미입니다. 어떤 시스템의 **불확실성** 혹은 **무질서도**를 나타내는 물리량입니다. 따라서 각 클래스에 대한 엔트로피가 낮을수록 해당 클래스의 예측이 확실하다는 것을 의미합니다.  
정보 이득은 어떤 속성을 기준으로 데이터를 분할할 때, 분할 이전과 이후의 엔트로피 차이로 **해당 속성이 얼만큼의 정보를 제공하는지**를 나타내는 개념입니다. 

### 로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.

취하는 경우는 **정규성**을 높이고 분석시에 정확한 값을 얻기 위함입니다. 단위수가 너무 큰 값들을 바로 회귀분석 할 경우, 결과를 왜곡할 우려가 있으므로 이를 방지하기 위해 로그함수를 취해줍니다. 또한 로그함수를 취함으로써 **비선형관계의 데이터를 선형으로** 만들 수 있습니다. 이때 로그 함수는 0~1 사이에서는 음수값을 가지므로 log(1+x)와 같은 방법으로 처리해주어야 합니다.

> 사례) 연령 같은 경우에는 숫자의 범위가 약 0세~120세 이하지만, 재산 보유액 같은 경우에는 0원에서 몇 조까지 올라갈 수 있습니다. 이럴 경우 데이터 간 단위가 달라지면 결과값이 이상해 질 수 있기 때문에 log를 이용해 큰 수를 같은 비율의 작은 수로 바꿔줍니다.

### 베이즈 정리에 대해 설명해주세요.

현재 주어진 모수(가정)에서 이 데이터가 관찰될 가능성인 가능도(Likelihood)와 데이터 전체의 분포인 증거(Evidence)를 바탕으로 가설에 대해 사전에 세운 확률인 사전확률을 실제로 가설이 성립할 확률인 사후확률로 업데이트하는 것입니다. 즉, 조건부 확률에 사전확률(prior)을 활용하여 통계적 추론을 하는 방법입니다.  
따라서 데이터가 주어지기 전에 이미 어느 정도 확률값을 예측하고 있을 때 이를 새로 수집한 데이터와 합쳐서 최종 결과에 반영할 수 있습니다.

### 신뢰 구간(Confidence Interval;CI)의 정의는 무엇인가요?

신뢰구간은 **모수가 실제로 포함될 것으로 예측되는 범위**입니다. 집단 전체를 연구하는 것을 불가능하므로, 샘플링된 데이터를 기반으로 모수의 범위를 추정하기 위해 사용됩니다. 따라서, 신뢰 구간은 샘플링된 표본이 연구중인 모집단을 얼마나 잘 대표하는지 측정하는 방법입니다. 일반적으로 95% 신뢰수준이 사용됩니다.

### 평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?

 대부분의 데이터가 몰려있는 상황에서는 평균을, 이상치가 많이 있는 상황에는 중앙값을 사용하는 것이 좋습니다. 평균은 모든 관측값을 반영하기 때문에 극단적인 이상치가 있는 경우, 이에 영향을 받을 수 있습니다. 따라서, 이러한 경우에는 데이터들의 가운데에 위치한 중앙값을 사용하는 것이 해당 집단을 대표하는 값이 될 수 있습니다.

### 필요한 표본의 크기를 어떻게 계산합니까?

![](https://user-images.githubusercontent.com/90206705/232524877-537946b1-2acd-4423-be72-4df05cf43c8d.png)

모집단의 크기 N을 구하고, 신뢰수준 z와 오차범위 e를 선정하여 표본의 크기를 구할수 있습니다. 이때, 오차범위는 작을 수록 모집단의 특성에 대한 유용한 정보를 제공하지만 모집단에 대한 추론이 틀릴 가능성도 높아지므로 10% 를 넘지 않게 해야 합니다. 일반적으로 신뢰도는 90%, 95%, 99%를, 표준편차는 0.5를 사용합니다. 이때 주의 할 점은 오차 한계를 더 작게 하려면 동일한 모집단에서 표본 크기가 더 커야 하고 더 높은 표집 신뢰 수준을 원한다면 표본 크기도 더 커져야 합니다.


---
## #Machine Learning
### 차원의 저주에 대해 설명하고, 이를 해결하기 위한 방법을 말해주세요.
차원의 저주란 차원이 증가하면서 학습데이터 수가 차원 수보다 적어져서 성능이 저하되는 현상을 일컫습니다. 차원이 증가할 수록 변수가 증가하고, 개별 차원 내에서 학습할 데이터 수가 적어집니다. 이때 주의할 점은 변수가 증가한다고 반드시 차원의 저주가 발생하는 것이 아니라 **관측치보다 변수 수가 많아지는 경우에 차원의 저주 문제가 발생**합니다.
이를 해결하기 위해 가장 대표적인 방법은 **Feature Selection**과 **Feature Extraction**이 있습니다. 우선 **Feature Selection**은 우리가 예측하고자 하는 타겟 변수와 관련이 높은 변수들을 추려내는 방법입니다. 이 방법을 이용해 불필요한 변수들을 처냄으로써 모델의 학습 시간을 줄이고 성능 또한 높일 수 있습니다. "Feature Extraction"은 데이터 특성을 가장 잘 표현하는 **주성분**을 추출해 데이터 양을 줄이는 방법이며, 이에 사용되는 대표적인 방법은 바로 **주성분 분석(PCA)**입니다.

### L1, L2 정규화에 대해 설명해주세요.
L1 Norm과 L2 Norm의 차이
![image](https://user-images.githubusercontent.com/90206705/230733708-35d4a699-d231-4c6e-bf7e-dba7ab6f99ed.png)<br>
- 검정색 두 점사이의 L1 Norm 은 빨간색, 파란색, 노란색 선으로 표현 될 수 있고
- L2 Norm 은 오직 초록색 선으로만 표현될 수 있습니다.
- L1 Norm 은 여러가지 path 를 가지지만 L2 Norm 은 Unique shortest path 를 가집니다.
- 예를 들어 p = (1, 0), q = (0, 0) 일 때 L1 Norm = 1, L2 Norm = 1 로 값은 같지만 여전히 Unique shortest path 라고 할 수 있습니다.

![image](https://user-images.githubusercontent.com/90206705/230850739-b7cd3091-4e97-4325-a385-ab7c2fad17da.png)<br>
L1 Norm은 두 개의 벡터를 빼고, 절대값을 취한 뒤, 합한 것입니다. L2 Norm은 두 개의 벡터의 각 원소를 빼고, 제곱을 하고, 합치고, 루트를 씌운 것입니다.

![image](https://user-images.githubusercontent.com/90206705/230852011-07c96ca3-d52b-4f8f-a584-a55de1bf5fef.png) <br>

**L1 Regulation(Lasso)**는 가중치의 절대값에 패널티를 주는 방법을 뜻합니다. L1 Regulation은 0이 아닌 모든 weight는 Cost에 더해지기 때문에 Gradient descent를 할 때, gradient에 비례하여 weight가 감소되기 때문에 weight 값이 양수 또는 음수로 존재하기만 하면 weight를 줄이고자합니다. L1 Regulationd은 절대값을 가지기 때문에 항상 기울기가 1또는 -1입니다.

**L2 Regulation(Ridge)**는 기존 Cost function에 가중치 제곱의 합을 더하는 형태입니다. wight의 크기에 따라 weight값이 큰 값을 더 빠르게 감소시키는 weight decay기법입니다. weight의 크기에 따라 가중치의 패널티 정도가 달라지기 때문에 가중치가 전반적으로 작어져 학습 효과가 L1 대비 더 좋게 나타납니다. 람다(λ)값에 따라 패널티의 정도를 조절할 수 있습니다.

![image](https://user-images.githubusercontent.com/90206705/230858833-b591855d-1ac9-4007-9e94-851605dafa36.png)

### Cross Validation은 무엇이고 어떻게 해야하나요? (각 종류와 장단점)
보통 train set으로 모델을 훈련을 한 후 test set으로 모델을 검증합니다. 하지만 이런 방식은 통해 고정된 test set을 통해 모델의 성능을 검증하고 수정하는 과정을 반복하면, test set에만 잘 동작하는 overfitting이 일어나게 됩니다. 이를 해결하기 위해 Cross Validation을 통해 train set과 Validation set을 분리한 후, Validation set을 사용해 검증하는 방식을 사용합니다.

Cross Validaion은 모덴 데이터 셋을 평가에 활용하여 과적합을 방지하고 정확도를 향상시킵니다. 또한, 적은 데이터에 대한 Validation 신뢰성이 상승한다는 장점을 가지고 있는 반면, iteration 횟수가 많기 때문에 모델 훈련 및 평가 소요시간이 증가한다는 단점이 있습니다.

Cross Validation 기법 종류에는 K-Fold Cross Validation, Stratified k-fold cross Validation, Hold-Out Cross Validation, Leave-p-Out Cross Validation, Leave-One-Out Cross Validation 등의 종류가 있습니다.
-꼬리질문
  - 기법에 대해 알기

### KNN 과 K-means 에 대해서 설명해주세요.
KNN과 K-means의 가장 큰 차이는 분류(Classifier)와 군집(Clustering)의 차이입니다.
둘은 모두 K개의 점을 지정하여 거리르 기반으로 구현되는 거리기반 분석 알고리즘입니다.
먼저 **KNN**(최근접 이웃방법)은 해당 데이터와 가장 가까이 있는 K개의 데이터를 확인하여 새로운 데이터 특성을 확인하는 방법입니다. 지도학습 알고리즘 중 하나로 K가 짝수면 1:1 대응이 될 수도 있기 때문에 K는 홀수를 쓰는 것이 보편적입니다. KNN은 회귀와 분류 모두 사용 가능합니다.

**K-means** 알고리즘은 데이터를 K개의 군집(cluster)으로 묶는 clustering 알고리즘입니다. 여기서 K는 묶을 그룹의 수를 말하고 Means는 데이터로부터 그 데이터가 속한 그룹의 중심까지의 평균 거리를 의미하며 이 값을 최소화 하는 것이 K-means입니다.

- 꼬리질문
  - 동작 방법
  - K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)

### XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?
**XGBoost**는 속도가 빠르고, 자원 효율성이 높아서 캐글에도 인기가 많습니다. **XGBoost**는 Extreme Gradient Boost의 준말이며 기존 Gradient Tree Boosting알고리즘에 과적합 방지를 위한 기법이 추가된 지도학습 알고리즘 입니다. **XGBoost**는 기본 학습기를 의사결정나무로 하며 Gradient Boosting과 같이 Gradient(잔차)를 이용하여 이전 모델의 약점을 보완하는 방식으로 학습니다. **XGBoost**는 과적합 방지가 잘되고 예측 성능이 좋다는 장점이 있지만 작은 데이터에 대해서는 과적합 가능성이 있다는 단점을 가지고 있습니다.
분류와 회귀문제에 모두 사용가능하며, XGBoost는 자체에 얼리스탑과 같은 과적합 규제 기능이 있어 강한 내구성을 지닙니다. 또한, 다양한 파라미터 옵션을 제공하는 Customizing이 용이하기도 합니다. 이러한 이유들로 기존 GBoost보다 XGBoost가 더 인기 있습니다.

1-1. 꼬리질문1 : XG boost의 하이퍼파라미터들을 아는데로 설명해주세요.
우선 일반 파라미터, 부스터 파라미터, 학습 파라미터 세가지 범주로 나눠볼 수 있습니다.

- 일반 파라미터 : 어떤 부스터를 쓸지(선형 or 트리) , 몇개의 스레드를 쓸지, verbosity(정보표시)값은 몇으로 줄지를 결정한다.
- 부스터 파라미터 : 해당 파라미터들은 일반 파라미터에서 어떤 부스터를 선택했느냐에 따라 다르다. 트리기준으로 봤을때, lr , weak learner 갯수 , max_depth 등 과적합을 조정할 수 있는 파라미터들이 있고,subsample로 학습 데이터 비율 고려, colsample_bytree로 각 트리별 피처의 비율을 고려해줄 수 있다. 두 값은 보통 낮을수록 과적합이 방지된다.
- 람다와 알파값은 L2,L1 규제에 쓰이는 값으로 피처갯수가 많을 때 고려할 수 있다. 두 값은 클수록 과적합을 방지한다.
- min_child_weight는 관측치에 대한 가중치 합의 최소를 말하며, gamma는 해당값보다 크게 감소할 때 분리한다. 두 값은 높을수록 과적합을 방지한다.

1-2. 꼬리질문2 : 자신만의 XGboost의 파라미터 튜닝 방법이 있나요?
우선 lr는 학습에 큰 영향을 미치므로, 큰값으로 고정시킨 후 다른 파라미터를 실험한 후, 마지막에 낮추는 방향으로 실험을 진행합니다.
또한 과적합을 방지하기위해 위에서 설명한 파라미터들을 튜닝합니다. 무엇보다 중요한 건 기록! 또 기록이기에 노션에 잘 기록합니다.

1-3. XGB의 학습과정은?

- 부스팅 계열 vs 배깅 계열 헷갈리지 않게 공부하기!! 
- 한줄 요약 : 부스팅은 처음에 학습하고 학습 잘 못한 파라미터들 가져다가 그 부분에 대해 학습.

### 회귀 / 분류시 알맞은 metric은 무엇인가요?
회귀 문제에서는 실제 값과 모델이 예측하는 값의 차이에 기반을 둔 metric(평가)을 사용합니다. 대표적으로 RSS(단순 오차 제곱 합), MSE(평균 제곱 오차), MAE(평균 절대값 오차)가 있습니다.

분류 문제에서는 분류 결과의 신뢰도를 나타낼 수 있는 metric을 사용합니다. 대표적으로는 Accuracy, Precision, Recall등이 있습니다.

- 꼬리질문
  - RSS(단순 오차 제곱 합) : 실제 값과 예측 값의 단순 오차 제곱 합, 값이 작을수록 모델의 성능이 높다고 평가
  - MSE(평균 제곱 오차) : RSS를 데이터의 개수만큼 나눈 값, 값이 작을수록 모델의 성능이 높다고 평가, 오차에 제곱이 되기 때문에 Outlier(이상치) 잡아내는 데 효과적, 루트를 씌우면 RMSE
  - MAE(평균 절대값 오차) : 예측값과 실제값의 오차의 절대값의 평균, 값이 작을 수록 모델의 성능이 높다고 평가, 변동치가 큰 지표와 낮은 지표를 같이 예측하는 데 효과적, 루트를 씌우면 RMAE
  - Accuracy : 
  - Precision : 
  - Recall : 

### SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?

SVM을 비선형 분류 모델로 사용하기 위해 **저차원 공간의 데이터를 고차원 공간으로 매핑**하여 선형 분리가 가능한 데이터로 변환하여 처리합니다. SVM이 좋은 이유는 SVM은 데이터들을 선형 분리하며 최대 마진의 초평면을 찾는 크게 복잡하지 않은 구조이며, 커널 트릭을 이용해 차원을 늘리면서 비선형 데이터들에도 좋은 결과를 얻을 수 있습니다. 또한 이진 분류 뿐만 아니라 수치 예측에도 사용될 수 있습니다. **Overfitting 경향이 낮으며** 노이즈 데이터에도 크게 영향을 받지 않습니다.

> SVM 회귀의 경우 SVM 분류와 반대로, 제한된 마진 오류(즉, 도로 밖의 샘플) 안에서 마진 안에 가능한 한 많은 샘플이 들어가도록 학습합니다.

###  ROC 커브에 대해 설명해주실 수 있으신가요?

ROC 커브는 거짓 긍정(False Positive)을 피하면서, 참 긍정(True Positive)을 탐지하는 것 사이의 트레이드오프를 관찰하기 위한 지표로 FPR(False Positive Rate)를 x축으로, TPR(True Positive Rate)를 y축으로 나타내는 곡선입니다.  
이러한 ROC curve는 왼쪽 상단에 가까울수록, 그래프의 아래 면적을 의미하는 AUC (Area Under the Curve)는 1에 가까울수록 좋은 성능이라고 판단합니다.

### 회귀 / 분류시 알맞은 손실함수와 이에 대한 설명

회귀 문제에서는 입력 변수(x)와 출력 변수(y) 사이의 관계를 찾는 문제로 MSE나 MAE 손실함수를 사용합니다.  MSE는 예측값과 실제값 간의 차이를 제곱한 값의 평균으로, 회귀 문제에서 가장 많이 사용되는 손실함수입니다. MAE는 예측값과 실제값 간의 차이의 절댓값의 평균으로, 이상치(outlier)가 있는 데이터에 민감하지 않다는 특징이 있습니다.  
분류 문제에서는 입력 변수(x)가 주어졌을 때, 이를 여러 클래스 중 하나로 분류하는 문제입니다. 이진 분류의 경우 Binary Cross Entropy를 다중 분류의 경우 Categorical Cross Entropy를 주로 사용합니다. 이외에도 불균형 데이터셋의 성능을 향상시키기 위해 사용되는 Focal loss가 있습니다. Focal loss 는 Categorical Cross Entropy에 (1 - y_pred)^γ를 곱하여  γ (gamma)값이 증가할수록 쉬운 샘플에 대한 가중치가 줄어들어 어려운 샘플에 대해 더 집중적으로 학습할 수 있게 합니다.

> 손실함수(loss funciton)는 딥러닝 모델이 실제 레이블과 가장 가까운 값이 예측되도록 훈련할 때, 모델의 예측값과 실제 레이블 간의 거리를 측정하기 위해 사용되는 함수입니다.

### 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?

뉴턴법은 현재 x값에서 접선이 x축과 만나는 지점으로 x를 이동시켜 가면서 점진적으로 해를 찾아가는 방법입니다. 초기값을 잘 주면 수렴 속도가 매우 빠르지만, 잘못 주면 시간이 오래 걸리거나 해를 찾지 못할 수 있습니다. 함수가 미분 가능해야하고 미분값이 0이 지점이 없어야합니다.   
반면 Gradient Descent는 현재 위치에서 함수의 기울기를 이용하여 극소점을 찾아가는 방법으로 local minimum에 빠질 수 있다는 문제가 있지만, 모든 차원과 모든 공간에서 적용이 가능합니다.  
Gradient Descent는 매 단계에서 함수의 기울기를 계산해야 하므로 계산 비용이 낮지만, 수렴 속도가 상대적으로 느릴 수 있습니다. Newton's Method는 매 단계에서 이차 도함수를 계산해야 하므로 계산 비용이 높아질 수 있지만, 수렴 속도가 빠를 수 있습니다.


### 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?

- 머신러닝은 모델에 대한 정교한 가정보다는 데이터의 다양한 피쳐를 사용하여 **높은 예측률**을 달성하고자 합니다. 따라서 머신러닝은 데이터의 복잡한 패턴을 인식하고 이를 통해 예측하는 능력이 뛰어나며, 데이터가 매우 큰 경우에도 높은 정확도로 예측이 가능합니다.  
반면 통계적 접근방법은 데이터의 분포와 가정을 통해 **신뢰 가능한 모델**을 만드는게 목적으로 다양한 통계 모델링 기법을 사용합니다. 통계적 접근방법은 데이터의 특성을 파악하고, 변수의 선택과 추정, 가설 검정 등을 수행하며 어떤 피쳐가 어떤 원인을 주는지 알 수 있다는 특징이 있습니다. 


---
## #Deep Learning
### TensorFlow, PyTorch 특징과 차이가 뭘까요?
- 모델 학습 부분에서의 차이
  - Tensorflow는 fit()함수로 간단하게 학습을 사용할 수 있음
  - Pytorch는 함수 속 for문을 정의하여 모델 train과 test코드르 구현해야함. 더욱 직관적이고 학습하기 용이함

- Tensorflow : 정적 그래프 생성, 메모리를 효율적으로 사용하기 어려움, 그래프 정의 후 실행
- Pytorch : 동적 그래프 생성, 디버깅이 쉬움, 동작 중에 그래프를 정의하거나 조작할 수 있음.

### ReLU로 어떻게 곡선 함수를 근사하나요?
ReLU는 선형 부분(y=x 부분)과 비선형 부분(y=0)이 결합된 모양이다. ReLU가 반복 적용되면서 선형 부분의 결합이 이루어지고, 곡선 함수를 근사할 수 있게 된다.

### Data Normalization은 무엇이고 왜 필요한가요?
Data Normalization은 입력 데이터의 최소, 최대값을 일정 범위(0~1) 내로 조절하는 것으로 특정 입력 데이터가 결과에 대해 과도한 영향을 미칠 수 있는 것을 방지하고 모델의 학습을 원할하게 만든다.

### 활성화 함수가 왜 필요한가요?
데이터를 비선형으로 바꾸기 위해서 활성화 함수를 사용한다. 망이 깊어질수록 장점이 많아지지만 선형시스템을 망에 적용시, 망이 깊어지지 않기 때문에 활성화 함수를 사용해 여러 층을 쌓아서 더 복잡한 표현을 한다.
![image](https://user-images.githubusercontent.com/90206705/229520019-9ad1ab82-5035-400d-b7a1-84bbf7749995.png)

### Gradient Descent에 대해서 중학생이 이해할 수 있게 쉽게 설명해주세요.
위 아래로 구불구불한 산길을 눈을 감고 지나면서 지금 밟고 있는 땅의 경사 방향을 토대로 가장 고도가 낮은 지점을 찾아나가는 것


### 오버피팅일 경우 어떻게 대처해야 할까요? 알고계신 방법들을 전부 말해주세요.
Overfitting을 막기 위해 여러 방법이 있습니다.
1. 데이터의 양이 적을 경우, 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현상이 발생할 확률이 늘어납니다. 이럴 경우 기존의 데이터를 증강시켜 데이터를 늘리고 데이터가 많을수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있습니다.
2. 복잡한 모델은 간단한 모델보다 과적합될 가능성이 높으므로 정규화를 통해서 복잡한 모델를 좀 더 간단하게 하는 방법이 있습니다.
3. Dropout을 사용해 학습과정마다 일정 비율의 뉴런만 사용하는 방법을 사용합니다.
4. 일정 횟수 이상 validation loss가 증가하는 시점부터 overfitting이 발생했다고 판단하고 이에 학습을 종료시킵니다,
5. Batch Normalization으로 데이터 분포를 통일 시켜줘서 과적합을 방지합니다. 

### Dropout은 무엇이며 왜 사용하는 지 말해주세요.
Dropout은 서로 연결된 layer에서 0에서 1사이의 확률로 뉴런을 제거(drop)하는 기법입니다. 꺼지는 뉴런의 종류와 개수는 오로지 랜덤하게 Dropout rate에 따라 결정이됩니다. Dropout Rate는 하이퍼파라미터이며 일반적으로 0.5로 설정합니다.
Dropout은 어떤 특정한 설명변수 Feature만을 과도하게 집중하여 학습함으로써 발생할 수 있는 overfitting을 방지하기 위해 사용됩니다.
- 꼬리질문
  - Dropout의 문제점과 해결방안을 말해주세요.
  Dropout은 신경망 학습 속도가 다소 느려지는 문제를 가지고 있지만 Batch Normalization과 Dropout을 사용해 학습속도를 증가시켰습니다.
  - Dropout의 효과 : Dropout은 노드들의 연결을 무작위로 끊는 방식으로, 하나의 노드가 너무 큰 가중치를 가져 다른 노드들의 학습을 방해하는 현상을 억제한다. 이를 통해 모델의 일반화 성능을 높이고, Overfitting을 방지한다.



### GD가 Local Minima 문제를 피하는 방법을 말해주세요.
원래는 loss function의 Gloabal minimum을 향해 가중치를 갱신해야 하지만, loss function의 Local minimum에 빠지는 문제가 발생할 수도 있다. 그럼에도 불구하고 momentum 개념등을 도입한 RMSProp, Adam 같은 다양한 Optimization 기법이 있기 때문에 Local minimum 문제를 어느 정도 피할 수 있다.

- 꼬리질문
  - GD가 local Minima 문제를 피하는 방법은?
  - 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?
  - Momentum, RMSprop, Adam과 같은 Optimization 개념 및 종류
  
### Batch Normalization은 무엇인가요?
batch normalization은 학습 과정에서 각 배치 단위 별로 데이터가 다양한 분포를 가지더라도 각 배치별로 평균과 분산을 이용해 정규화하는 것을 말합니다.
batch 단위나 layer에 따라서 입력 값의 분포가 모두 다르지만 정규화를 통하여 분포를 zero mean gaussian 형태로 만듭니다.
그러면 평균은 0, 표준 편차는 1로 데이터의 분포를 조정할 수 있습니다.
여기서 중요한 것은 Batch Normalization은 학습 단계와 추론 단계에서 조금 다르게 적용되어야 합니다.

- 꼬리 질문 : Batch Normalization의 효과
  - 은닉층에서의 입력값을 Normalize함으로써 입력 분포를 조정 할 수 있다. 은닉층의 입력 분포가 조정되면 학습 편의성이 개선되어 수렴 속도가 빨라지고, Local Optima를 피할 가능성이 높아진다.

BN 적용해서 학습 이후 실제 사용시에 주의할 점은 무엇인가요(코드적인 부분 포함)?
학습시에는 각 mini-Batch 단위의 평균과 분산을 이용해 Normalize하지만 실제 사용시에는 네트워크에 입력되는 Batch의 단위가 더 적을 수 있기 때문에 미리 학습 데이터에서 뽑아낸 평균, 분산을 이용해야한다.

Pytorch에서 BatchNormalization을 사용하는 대표적인 방법은 torch.nn.BatchNorm1d와 torch.nn.BatchNorm2d를 사용하는 것입니다.
이 때, 차이점은 BatchNorm1d의 경우 Input과 Output이 (N, C) 또는 (N, C, L)의 형태를 가지고 BatchNorm2d의 경우 Input과 Output이 (N, C, H, W)의 형태를 가집니다. 여기서 N은 Batch의 크기를 말하고 C는 Channel을 말합니다. BatchNorm1d에서의 L은 Length을 뜻하고 BatchNorm2d에서의 H와 W 각각은 height와 width를 뜻합니다.

### Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?

딥러닝에서 가중치를 잘 초기화하는 것은 기울기 소실이나 local minima 등의 문제를 방지 가능합니다.  
LeCun 초기화은 들어오는 노드 수를 고려한 정규 분포와 균등 분포를 따르는 방법입니다.  
Xavier 초기화는 들어오는 노드의 수와 나가는 노드 수를 고려하여 가중치를 초기화하는 방법입니다. sigmoid 나 tanh 함수와는 좋은 결과를 보여주지만 ReLU 함수와 사용할 경우 0에 수렴하는 문제가 발생하는 초기화 방법입니다.  
He 초기화는 ReLU 와 함께 많이 사용되는 방법으로, LeCun 방법과 같이 들어오는 노드의 수만을 고려하지만 상수 부분은 Xavier 초기화의 방법을 사용합니다.
그 외에도 Gaussian sampling 등의 방법이 있습니다.

### 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)

- **Sigmoid**는 **입력을 0 ~ 1 사이로 mapping**합니다. 하지만 입력값이 커질수록 미분값이 0에 수렴해 **gradient vanishing** 문제가 생기며 **zero-centered하지 않아** 학습이 느려집니다.  
**Tanh**는 sigmoid를 변형한 쌍곡선함수로, **입력을 -1 ~ 1 사이로 mapping해 zero-center 문제는 해결**했지만 gradient vanishing 문제는 해결하지 못했습니다.  
**ReLU**는 입력이 양수일 경우 **saturate 문제가 해결**되며, 단순히 max 함수를 사용해 **속도가 빠릅니다**. 입력이 음수일 경우에도 saturate 문제를 해결한 함수로는 **LeakyReLU**가 있습니다.  
분류 문제에는 **softmax**를 사용하기도 합니다.


### 효율적인 GPU의 사용을 위해 어떻게 dataloader를 조절할 수 있는가?

PyTorch의 DataLoader는 **`batch_size`** 로 batch의 크기를 조절하여 데이터를 미니 배치로 분할하고, 이들을 병렬적으로 로딩하고 전처리하는 함수로 데이터를 GPU로 로드할 수 있으며, 이를 통해 모델 학습 시간을 단축할 수 있습니다.  
DataLoader의 **`num_workers`** 하이퍼파라미터 값이 높을수록 DataLoader는 더 많은 프로세스를 생성하여 데이터 로딩 및 전처리를 병렬로 처리합니다. 이때, 각 워커는 CPU를 사용하여 데이터를 로드하고 전처리합니다. 이를 GPU 메모리로 이동시킬 때, **`pin_memory`** 파라미터를 사용하여 CPU 메모리와 GPU 메모리 간 데이터 이동을 최적화할 수 있습니다. **`pin_memory=True`** 로 설정하면 DataLoader는 Tensor를 CUDA 고정 메모리에 복사하여 GPU 메모리로 전송합니다.

### 하이퍼 파라미터는 무엇인가요?

하이퍼 파라미터(Hyper-parameter)는 모델링할 때, **사용자가 직접 세팅해주는 값**으로 learning rate, epoch나 SVM에서의 C, sigma 값, KNN에서의 K값 등이 있습니다.
하이퍼 파라미터는 정해진 최적의 값이 없으며, 사용자의 선험적 지식을 기반으로 설정(휴리스틱)합니다. 이러한 하이퍼 파라미터 튜닝 기법에는 Manual Search, Grid Search, Random Search, Bayesian Optimization 등이 있습니다.

### 미니배치를 작게 할때의 장단점은?

전체 데이터를 쪼개서 여러 번 학습하기 때문에, 전체 Training 데이터 셋을 배치로 사용것보다 계산량이 적어(즉 메모리 사용량이 적어), 학습 속도가 빠르다는 장점이 있습니다. 반면에 big batch보다 느리고 loss function의 최솟값을 찾기 위해 자주 step의 방향을 바꿔야 하므로 학습이 불안정한 단점이 있습니다.

> 하지만 요즘엔 Adam optimizer나 batch normalization(BN)등의 기법을 사용함으로써 학습안정화가 정말 잘 되는 네트워크가 많습니다. 따라서 batch size가 커도 local minimum을 잘 지나쳐 global minimum으로 수렴할 수 있게되었습니다. 따라서 최근에는 batch size를 줄 수 있는만큼 최대한 크게줘야 좋다고 할 수 있습니다. batch size가 클수록 BN을 더욱 정확하게 계산할 수 있어 BN의 효과를 더욱 잘 누릴 수 있게되고 그렇게 되면 학습속도가 빨라지며 learning rate의 hyper parameter 설정에서 꽤나 자유로워질 수 있게됩니다.

---

## #Operating System
### 프로세스와 스레드의 차이(Process vs Thread)를 알려주세요.
프로세스는 메모리 상에서 실행중인 프로그램을 말하며, 스레드는 프로세스 안에서 실행되는 흐름 단위를 말함.
프로세스는 자신만의 고유 공간과 자원을 할당받아 사용하고 스레드는 다른 스레드와 공간과 자원을 공유하면서 사용.

### 멀티 프로세스와 멀티 스레드를 사용하는 이유를 각각 설명해주세요.
- 멀티 프로세스는 두 개 이상 다수의 프로세서가 협력적으로 하나 이상의 작업을 동시에 처리하는 것으로 각 프로세스 간 메모리 구분이 필요하거나 독립된 주소 공간을 가져야 할 경우 사용한다. 독립된 구조로 안전성이 높은 장점을 가지고 있지만 작업량이 많을 수록 오버헤드가 발생하여 성능 저하가 발생할 수 있다는 단점을 가지고 있다.
- 멀티 스레드는 하나의 프로세스를 여러 스레드로 자원을 공유하며 작업을 나누어 수행하는 것이다. 시스템 자원 소무를 감소하고 자원을 효율적으로 관리할 수 있지만, 병목현상, 데드락 등 자원을 공유하기에 동기화 문제가 발생할 수 있다. 또한 하나의 스레드에 문제가 생기면 전체 프로세스가 영향을 받는다.

- 멀티 스레드 vs 멀티 프로세스
  - 멀티 스레드는 멀티 프로세스보다 작은 메모리 공간을 차지하고 Context Switching이 빠른 장점이 있지만, 동기화 문제와 하나의 스레드 장애로 전체 스레드가 종료될 위험을 갖고 있다.
  - 멀티 프로세스는 하나의 프로세스가 죽더라도 다른 프로세스에 영향을 주지 않아 안정성이 높지만, 멀티 스레드보다 많은 메모리 공간과 CPU 시간을 차지하는 단점이 있다.

### 동기와 비동기의 차이는 무엇인가요?
동기와 비동기는 어떤 작업을 처리하고자 하는 시각의 차이다. 동기는 동시에 일어나는 것, 비동기는 동시에 일어나지 않는 것이다.

### 캐시의 지역성에 대해 설명해주세요.

캐시 메모리는 적중률(Hit rate)을 극대화하기 위해 데이터 지역성(Locality)의 원리를 사용합니다. 지역성(Locality)이란 기억 장치 내의 정보를 균일하게 액세스하는 것이 아닌 특정 부분을 집중적으로 참조하는 특성으로, 프로그램이 소규모의 특정 데이터 및 명령어 집합에 반복적으로 액세스하는 경향이 있다는 사실을 의미합니다.  
최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성인 **시간 지역성**과 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성인 **공간 지역성**이 있습니다.

> 캐시 메모리는 CPU의 처리 속도와 메모리의 속도 차이로 인한 병목현상을 완화하기 위해 사용하는 고속 버퍼 메모리입니다. 주기억장치에 있는 데이터를 액세스하려면 비교적 오랜 시간이 걸리게 되는데 이를 줄이기 위해 데이터를 빠르게 액세스할 수 있도록 중간에 캐시 메모리를 사용합니다. 따라서 데이터 요청이 들어오면, 원본 데이터가 담긴 곳에 접근하기 전에 먼저 캐시 내부부터 찾는데, 이때 원하는 데이터가 캐시에 있는 경우에 대한 비율을 캐시 메모리 적중률이라고 합니다.
---

## #Data Structure
### 배열(array)와 연결리스트(linked list)의 각각의 특징과 장단점을 설명해주세요.
Array는 연속된 메모리 주소를 할당받게 되는데 이때 index를 갖게된다. Array는 index를 가지고 임의 접근이 가능하고 접근과 탐색에 용이하다는 장점을 가지고 있지만 크기를 미리 정해놓았기 때문에 해당 배열 크기 이상의 데이터를 저장할 수 없다는 단점이 있다.
Linked List는 동적 자료구조로 크기를 정할 필요가 없고 배열처럼 연속된 메모리 주소를 할당받지 않는다. 대신 노드(Node) 안에 데이터가 있고, 다음 데이터를 가리키는 주소를 가지고 있다. Linked List는 크기의 제한이 없으므로 데이터 추가, 삭제가 자유롭다는 장점이 있지만, 메모리 주소르 할당받지 않았기 때문에 임의로 접근하는 것은 불가능하여 데이터를 탐색할 때 순차적으로 접근해야 한다는 단점이 있다.
![image](https://user-images.githubusercontent.com/90206705/229535091-e2cfef85-5a05-4e46-9a9a-075d469d0902.png)

## #Computer Science
### 0.1 + 1.1 = 1.2 가 false인 이유를 설명해주세요. 
부동 소수점의 문제로 컴퓨터가 실수를 저장할 때, 지수부와 가수부로 나누어 저장하기 때문이다. 
![image](https://user-images.githubusercontent.com/90206705/229536125-a3790d3e-92de-4443-8a60-92a0ec51523b.png)

## #Python
### python은 어떤 특징을 가진 언어인가요?
파이썬은 스크립트 언어이고 (변수의 자료형을 지정하지 않고 선언하는)동적 타입 언어이다. 또한 파이썬은 대부분의 운영체제에서 모두 작동된다.
빠른 개발 속도, 활발한 생태계, 간결하고 쉬운 문법을 장점으로 가지고 있다.

## #Database
### NoSQL과 RDBMS의 차이점을 설명해주세요.
**RDBMS**는 관계형 데이터베이스 관리 시스템을 의미합니다. 다른 테이블과 관곌르 맺고 모여있는 집합체로 이해할 수 있습니다. 이러한 관계를 나타내기 위해 외래 키(foreign key)를 사용한 테이블 간 Join이 가능하다는게 RDBMS의 가장 큰 특징입니다. 정해진 스키마에 따라 데이터를 저장하여야 하므로 정확한 데이터 구조를 보장하는 장점이 있습니다. 반면에 테이블간 관계를 맺고 있어 시스템이 커질 경우 JOIN문이 많은 복잡한 쿼리가 만들어질 수 있고 스키마로 인해 데이터가 유연하지 못해 나중에 스키마가 변경 될 경우 번거롭고 어렵다는 단점이 있습니다.
**NoSQL**은 RDBMS와는 달리 테이블 간 관계를 정의하지 않습니다. 데이터 테이블은 그냥 하나의 테이블이며 따라서 일반적으로 테이블 간 Join도 불가능합니다. 빅테이터의 등장으로 인해 데이터와 트래픽이 기하급수적으로 증가함에 따라 RDBMS에 단점인 성능을 향상시키기 위해 등장했고 데이터 일관성은 포기하되 비용을 고려하여 여러 대의 데이터에 분산하여 저장하는 Scale-Out을 목표로 등장했습니다. 스키마가 없기 때문에 유연하며 자유로운 데이터 구조를 가질 수 있고 언지든 저장된 데이터를 조정하고 새로운 필드를 추가할 수 있다는 장점이 있습니다. 반면에, 스키마가 존재하지 않기에 명확한 데이터 구조를 보장하지 않으며 데이터 구조 결정하기가 어려울 수 있다는 단점이 있습니다.
