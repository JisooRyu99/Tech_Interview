## #Statistics/Math

### “likelihood”와 “probability”의 차이는 무엇일까요?

Probability는 어떤 trial에서 특정 sample이 나올 가능성을 말합니다. 즉, **시행 전 모든 경우의 수의 가능성은 정해져있고**, 그 총합은 1입니다.  
Likelihood는 **trial을 충분히 수행한 후 그 sample을 토대로 경우의 수의 가능성을 도출**하는 것입니다. 아무리 충분히 수행해도 어디까지나 추론inference이기 때문에 가능성의 합이 1이 되지 않을 수 있습니다.

> 확률은 어떤 시행에서 특정 결과가 나올 가능성을 말하며 총합은 1이다. 반면에 가능도는 실재가 바탕이 되어야 하고 어떤 시행을 충분히 수행한 뒤 그 결과를 토대로 경우의 수의 가능성을 도출하는 것이다. 어디까지나 추론이기 때문에 가능성의 합이 1이 되지 않을 수도 있다.

> 확률(Probability)은 관측값 또는 관측 구간이 **주어진 확률분포** 안에서 얼마나 나타날 수 있는가에 대한 값입니다. 즉, **특정 사건이 일어날 가능성**을 수치화한 것입니다.
반면, 가능도(Likelihood)은 어떤 특정한 값을 관측할 때, 이 관측치가 어떠한 확률분포에서 나왔는가에 관한 값입니다. 즉, 특정 모델이 특정 데이터 세트에 적합한지 측정하는 척도라고 할 수 있습니다.

<br/>

### 중심극한정리는 왜 유용한걸까요?


**중심극한정리**란 모집단이 정규분포를 따르지 않는 표본이더라도, **표본들의 수가 30개 이상이면 근사적으로 정규분포를 따른다는 이론**입니다.  
중심극한정리가 유용한 이유는 **모집단의 형태에 관계없이 모분산을 모르는 경우에도,** 표본의 수 n이 30 이상인 경우 **표본 평균의 분포가 정규분포를 따르기 가정**할 수 있기 때문입니다.


<br/>

### 아웃라이어의 판단하는 기준은 무엇인가요?

아웃라이어를 판단하는 기준은 여러 가지가 있을 수 있습니다.

통계학적으로는 임계값을 설정하고, Z-score이 이 값보다 크다면 이상치로 판단하는 방법과, IQR(Inter Quantile Range) 기법으로 데이터를 오름차순으로 정렬했을 때, **IQR는 75%의 지점 – 25%의 지점**인데, 이 “**75%의 지점 + IQR * 1.5**”의 이상이거나 “**25%의 지점 - IQR * 1.5**”의 이하인 데이터들을 이상치라고 판단하는 방법이 있습니다.  
또는 **도메인 지식을 이용**하여, 데이터가 수집된 분야의 전문적인 지식을 바탕으로 아웃라이어를 판단하거나, **EDA**를 통해 데이터에서 동떨어진 극단값을 판별할 수 있습니다.

<br/>

### 모집단과 표본의 차이는 무엇인가요?

모집단은 통계적 관찰의 대상이 되는 **집단 전체**를 말합니다. 표본은 모집단 중 **관측된 부분집합**을 말합니다.

<br/>

### 공분산과 상관계수는 무엇일까요?

공분산과 상관계수는 **두 변수 간의 관련성**을 측정하는 데 사용되는 통계적 개념입니다. **공분산**은 두 변수의 변화량이 함께 얼마나 많이 변하는지를 나타내는 값으로,  변수 간에 **양의 상관관계가 있는지, 음의 상관관계**가 있는지를 알 수 있습니다. **상관계수**는 공분산을 **각 변수의 표준편차로 나누어, 각 변수의 척도에 대한 영향을 제거**한 값입니다. **1과 1 사이의 값**을 가지며, 1은 완전한 양의 상관관계, -1은 완전한 음의 상관관계를 나타내며, 0은 상관관계가 없음을 나타냅니다.

</br>

### 조건부 확률은 무엇일까요?

**조건부 확률**은 **하나의 사건이 일어났을 때, 다른 사건이 일어날 확률**입니다.

> 사건 A가 일어났다는 전제 하에 사건 B가 일어날 확률은 사건 A와 B가 동시에 일어날 확률에 사건 A가 일어날 확률을 나누어 준 값입니다.

</br>

### Missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?

결측값이 많지 않은 경우에는 제거해주면 되지만, 결측값이 많거나, 데이터가 적은 경우에는 이를 **적절한 값으로 채워 가능한 데이터의 양을 확보**해야 합니다. 이를 통해 모델이 보다 정확한 예측을 하도록 할 수 있습니다. 결측값은 **0이나 평균, 중앙값, 최빈값** 등으로 대체할 수 있으며, **회귀 분석, K-NN 등의 머신러닝 알고리즘을 사용**하여 예측값으로 대체하는 방법이 있습니다.


> 이를 해결하기 위한 가장 쉬운 방법은 누락된 데이터를 제거하는 것입니다. 하지만 중요한 정보를 가진 데이터를 잃을 위험을 가지고 있습니다. 또 한 컬럼에 있는 missing value를 결측 되지 않은 다른 값의 평균이나 중앙값으로 대체하는 것입니다. 이 방법은 쉽고 작은 빠르다는 장점이 있지만 다른 feature 간의 상관관계가 고려되지 않고 인코딩 된 범주형 feature에 대해 안 좋은 결과를 제공한다는 단점을 가지고 있습니다. 최빈값, 0, 임의 값으로 채워넣을 수도 있습니다. 이 방식은 쉽고 범주형 feature에 잘 동작한다는 장점이 있지만 이것 또한 다른 feature 간의 상관관계가 고려되지 않고 데이터에 bias를 만든다는 단점이 있습니다.  
머신러닝을 통해 해결하는 방법으로는 KNN으로 가까운 이웃간의 거리기반 분석을 통해 채워넣을 수 있습니다. mean, median이나 most fequent보다 정확할 때가 많지만 메모리가 많이 필요한 방법입니다.  
Pandas에서는 dropna, fillna를 이용해 누락된 데이터를 제거하거나 채울 수 있습니다.  

</br>

###  엔트로피(Entropy)에 대해 설명해주세요. 가능하면 정보이득(Information Gain)도요.

엔트로피는 정보 이론에서 사용되는 개념으로 1로 갈수록 불순하고, 0으로 갈수록 불순하지 않다는 의미입니다. 어떤 시스템의 **불확실성** 혹은 **무질서도**를 나타내는 물리량입니다. 따라서 각 클래스에 대한 엔트로피가 낮을수록 해당 클래스의 예측이 확실하다는 것을 의미합니다.  
정보 이득은 어떤 속성을 기준으로 데이터를 분할할 때, 분할 이전과 이후의 엔트로피 차이로 **해당 속성이 얼만큼의 정보를 제공하는지**를 나타내는 개념입니다. 

</br>

### 로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.

취하는 경우는 **정규성**을 높이고 분석시에 정확한 값을 얻기 위함입니다. 단위수가 너무 큰 값들을 바로 회귀분석 할 경우, 결과를 왜곡할 우려가 있으므로 이를 방지하기 위해 로그함수를 취해줍니다. 또한 로그함수를 취함으로써 **비선형관계의 데이터를 선형으로** 만들 수 있습니다. 이때 로그 함수는 0~1 사이에서는 음수값을 가지므로 log(1+x)와 같은 방법으로 처리해주어야 합니다.

> 사례) 연령 같은 경우에는 숫자의 범위가 약 0세~120세 이하지만, 재산 보유액 같은 경우에는 0원에서 몇 조까지 올라갈 수 있습니다. 이럴 경우 데이터 간 단위가 달라지면 결과값이 이상해 질 수 있기 때문에 log를 이용해 큰 수를 같은 비율의 작은 수로 바꿔줍니다.

</br>

### 베이즈 정리에 대해 설명해주세요.

현재 주어진 **모수(가정)**에서 이 데이터가 관찰될 가능성인 **가능도(Likelihood)와** 데이터 전체의 분포인 **증거(Evidence)를 바탕으로** 가설에 대해 사전에 세운 확률인 **사전확률을** 실제로 가설이 성립할 확률인 **사후확률로 업데이트**하는 것입니다. 즉, 조건부 확률에 사전확률(prior)을 활용하여 통계적 추론을 하는 방법입니다.  
따라서 데이터가 주어지기 전에 이미 어느 정도 확률값을 예측하고 있을 때 이를 새로 수집한 데이터와 합쳐서 최종 결과에 반영할 수 있습니다.

> **베이즈 정리는 새로운 정보를 토대로 어떤 사건이 발생했다는 주장에 대한 신뢰도를 갱신해 나가는 방법**

</br>

### 신뢰 구간(Confidence Interval;CI)의 정의는 무엇인가요?

신뢰구간은 **모수가 실제로 포함될 것으로 예측되는 범위**입니다. 집단 전체를 연구하는 것을 불가능하므로, 샘플링된 데이터를 기반으로 모수의 범위를 추정하기 위해 사용됩니다. 따라서, 신뢰 구간은 샘플링된 표본이 연구중인 모집단을 얼마나 잘 대표하는지 측정하는 방법입니다. 일반적으로 95% 신뢰수준이 사용됩니다.

</br>

### 평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?

 대부분의 **데이터가 몰려있는 상황에서는 평균을, 이상치가 많이 있는 상황에는 중앙값을 사용**하는 것이 좋습니다. 평균은 모든 관측값을 반영하기 때문에 극단적인 이상치가 있는 경우, 이에 영향을 받을 수 있습니다. 따라서, 이러한 경우에는 데이터들의 가운데에 위치한 중앙값을 사용하는 것이 해당 집단을 대표하는 값이 될 수 있습니다.

</br>

### 필요한 표본의 크기를 어떻게 계산합니까?

![](https://user-images.githubusercontent.com/90206705/232524877-537946b1-2acd-4423-be72-4df05cf43c8d.png)

**모집단의 크기 N을 구하고, 신뢰수준 z와 오차범위 e를 선정**하여 표본의 크기를 구할수 있습니다. 이때, 오차범위는 작을 수록 모집단의 특성에 대한 유용한 정보를 제공하지만 모집단에 대한 추론이 틀릴 가능성도 높아지므로 10% 를 넘지 않게 해야 합니다. 일반적으로 신뢰도는 90%, 95%, 99%를, 표준편차는 0.5를 사용합니다. 이때 주의 할 점은 오차 한계를 더 작게 하려면 동일한 모집단에서 표본 크기가 더 커야 하고 더 높은 표집 신뢰 수준을 원한다면 표본 크기도 더 커져야 합니다.

</br>

### p-value를 모르는 사람에게 설명한다면 어떻게 설명하실 건가요?
- p-value는 실험결과가 우연적으로 발생한 것인지 그렇지 않은지 판단할 때 사용하는 수치입니다. 즉, p-value의 통계학적 정의는 **'귀무가설 하에서 관찰된 통계량만큼의 극단적인 값이 관찰될 확률'** 이고, p-value의 p는 probability입니다.

> - p-value는 가설검정을 할 때 쓰이는 기준이라고 할 수 있습니다. 가설검정은 기존의 주장인 귀무가설과 입증하고자 하는 가설인 대립가설을 설정하여 진행합니다. 이때 귀무가설이 참인데 기각한 경우를 1종 오류라고 하는데, 이러한 1종 오류를 범할 확률이 p-value입니다. 가설검정을 할 때에는 1종 오류를 범할 최대확률인 유의수준을 설정하여 유의 수준보다 p-value가 작다면 실험의 오류가 상한선보다 작으므로 귀무가설을 기각하고 입증하고자 하는 가설인 대립가설을 채택하게 됩니다.

</br>

### R square의 의미는 무엇인가요?

**회귀분석의 성능 평가 척도** 중 하나로, **결정력(결정계수)** 라고도 합니다. R-squared는 **독립변수가 종속변수를 얼마나 잘 설명하는 지**를 나타냅니다. R-squared는 0과 1 사이 값을 가집니다.  
MSE, RMSE, MAE의 경우 작을수록 좋지만 R-squared 는 클수록 좋습니다. 즉 **1에 가까울수록 독립변수가 종속변수를 잘 설명할 수 있다**는 뜻입니다.

> - $R^2=\frac{SSR}{SST} = 1-\frac{SSE}{SST}$
> - SST: 종속 변수의 총 변동성을 나타내는 제곱 합
> - SSR: 독립 변수들이 설명할 수 있는 종속 변수의 변동성
> - SSE: 회귀식 추정 y의 편차제곱의 합
> - R-squared는 독립변수의 설명력에 관계없이 독립변수가 많으면 많을수록 높아집니다. 이를 해결하기 위한 방법으로는 독립변수 개수에 대한 패널티를 부여하는 Adjusted R-squared (조정 설명계수)가 있습니다. 


</br>

### 고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요?

**정방행렬인 A에 임의의 벡터 x를 곱한 값이 상수값인 λ(람다)에 임의의 벡터 x를 곱한 값과 같을 때**의 x를 고유 벡터, 람다를 고유값이라고 합니다.
기하하적인 입장에서 보면 **고유값 λ는 변화되는 크기**를 의미하며, **고유벡터 x는 변화되는 방향**을 의미하게 됩니다.
이러한 고유값과 고유벡터는 정방행렬을 분해하는 고유값 분해(EVD), 직사각행렬도 분해할 수 있는 특이값 분해(SVD), 데이터들을 차원 축소시킬 때 기존의 의미를 잘 보존시키는 **주성분 분석(PCA) 등에 활용**할 수 있어 중요하다고 할 수 있습니다.

> PCA는 Eigen value decomposition(고유값 분할)을 통해 새로 만든 '축(관점)'으로 데이터를 바라보는 것입니다. 데이터들의 평균으로 원점을 가정하고 고유값 분할을 통해서 공분산행렬, 고유값, 고유벡터를 구하게 됩니다. 이렇게 고유값 분할을 통해 얻게 된 새로운 다양한 축(고유벡터) 관점에서 가장 큰 분산을 가질 때의 축을 기준으로 데이터를 바라보게 됩니다.


</br>

### Variance를 구할 때, N대신에 N-1로 나눠주는 이유는 무엇인가?

**표본의 분산은 모집단의 분산보다 작게 측정되기 때문**에 분모를 n이 아니라 n-1로 나눠 보정해줍니다. n-1로 나누는 이유는 표본 분산을 구할 때, 표본 평균과 n-1개의 변수를 알고 있으면 마지막 1개의 변수의 값은 고정되어 자유도가 n-1이 되기 때문입니다.

</br>


### Null space란 무엇인가요?

Null space는 선형 변환에서 입력 벡터 x를 영벡터로 매핑하는 해를 가지는 x의 집합입니다. 즉, Ax = 0을 만족하는 모든 x의 값들로 이루어져 있습니다. Null space에 속하는 벡터들은 선형 변환 후에 0 벡터로 매핑되므로, 아무리 선형 변환을 적용해도 그 값은 변하지 않습니다.

</br>

### 최대우도법에 대해 설명해주세요.

**최대우도법(maximum likelihood estimation, MLE)** 이란 어떤 확률변수에서 표집한 값들을 토대로 각 가설마다 계산된 우도값 중 가장 큰 값을 고르는 통계적 추정방법입니다. 쉽게 말하면 우리가 알고 싶은 데이터 모수가 있다고 할 때, 여러 관측치들을 통해서 그러한 관측치가 나오게 하는 가장 그럴 듯한 값(가능성이 높은 값)을 추정하는 것입니다. 이를 사용하여 데이터를 가장 잘 설명할 수 있는 하이퍼 파라미터 값을 찾아갈 수 있습니다.

</br>

## p-value 구하는 과정이 어떻게 되는지

먼저, 가설을 설정합니다. 이는 귀무 가설(H0)과 대립 가설(H1)로 구성됩니다. 귀무 가설은 틀릴 것이라고 가정하고, 대립 가설은 귀무 가설이 틀리다는 것을 입증하고자 하는 가설입니다.  
그 다음, 적절한 검정 통계량을 계산합니다. 검정 통계량은 선택한 검정 방법에 따라 달라지며, 일반적으로 t-통계량 또는 F-통계량을 사용합니다.  
마지막으로, 계산된 검정 통계량과 가설 검정에 사용되는 확률 분포를 이용하여 p-value를 계산합니다. p-value는 주어진 검정 통계량 이상 또는 이하로 귀무 가설을 기각할 확률을 의미합니다.


</br>

## p-value 의 허점이 무엇인지?

p-value는 주어진 데이터에 기반하여 계산되기 때문에 다른 데이터나 재실험 시 다른 결과를 얻을 수 있다는 재현성 문제가 있습니다. 따라서 p-value 값은 일관되지 않을 수 있습니다. 특히, 표본의 크기를 어떻게 하는지에 따라 p-value값이 달라지며 유의성이 달라질 수 있습니다. 즉, p-value만 두고서 연구 결과를 결정해서는 안됩니다. p-value는 귀무가설을 얼마나 지지하는지만 나타내기 때문에 반드시 데이터로부터 구한 추정치, 신뢰구간, 표준오차와 같은 통계값과 함께 살펴보고 결정해야합니다.

</br>


---
## Machine Learning

### 차원의 저주에 대해 설명하고, 이를 해결하기 위한 방법을 말해주세요.

**Feature에 비해 데이터가 너무 적어서 생기는 현상**으로 과적합의 발생 가능성이 증가하게 됩니다. 이를 해결하기 위해서는 더 많은 데이터를 추가하거나 PCA와 같이 피처를 함축적으로 잘 설명할 수 있도록 저차원으로 매핑하는 차원축소를 하거나, 중요한 변수만 선택하여 차원을 줄이는 방법이 있습니다.

> 이를 해결하기 위해 가장 대표적인 방법은 **Feature Selection**과 **Feature Extraction**이 있습니다. 우선 **Feature Selection**은 우리가 예측하고자 하는 타겟 변수와 관련이 높은 변수들을 추려내는 방법입니다. 이 방법을 이용해 불필요한 변수들을 처냄으로써 모델의 학습 시간을 줄이고 성능 또한 높일 수 있습니다. **Feature Extraction**은 데이터 특성을 가장 잘 표현하는 **주성분**을 추출해 데이터 양을 줄이는 방법이며, 이에 사용되는 대표적인 방법은 바로 **주성분 분석(PCA)** 입니다.

</br>

### L1, L2 정규화에 대해 설명해주세요.

L1 정규화와 L2 정규화는 가중치(w)에 대한 제약 조건을 추가하여 모델의 과적합을 방지하고 일반화 성능을 향상시키는 방법입니다. 즉, 가중치의 크기를 제한함으로써 모델의 복잡도를 감소시켜 새로운 데이터에 대해 더 일반화된 예측을 하도록 하는 것입니다.  
L1 정규화는 **가중치 값들의 절댓값의 합을 최소화**하여, 불필요한 가중치 값들을 0으로 만들어 모델에서 특정한 입력에만 반응하는 가중치 값들을 제거할 수 있습니다. L2정규화는 **가중치 값들의 제곱합을 최소화**하여 가중치 값들 간의 차이를 줄여줄 수 있습니다.

</br>

### Cross Validation은 무엇이고 어떻게 해야하나요? (각 종류와 장단점)

Cross Validation은 test 데이터에 overfitting을 방지하기 위해, test와 분리한 train 데이터를 다시 **train-validation 세트로 나누어 실험**을 해 그 평균을 검증값으로 이용하는 것을 말합니다.  
Cross Validation의 종류로는 n개의 data를 균등하게 섞고 k개의 그룹으로 나눠서 1개만 test set, k-1개는 train set으로 사용하는 K-fold CV, 1개의 data sample만 test set, n-1개는 train set으로 사용하는 LOOCV, training set에도 cross validation을 다시 적용해서 best parameter를 찾아 training set에 적용해 train하는 Nested CV 등이 있습니다.  
계층별 k-겹 교차검증(Stratified K-Fold Cross Validation)은 원본 데이터의 레이블 분포를 먼저 고려하여, 이 분포와 동일하게 학습과 검증 데이터 세트를 분할하는 방식입니다.
Group K-Fold Cross Validation은 데이터를 그룹으로 나누어, 특정 그룹에 속한 모든 데이터를 한 폴드의 검증 데이터셋으로 사용하는 방식입니다.

</br>

### KNN 과 K-means 에 대해서 설명해주세요.

KNN은 지도학습으로, 주변의 **가장 가까운 K개의 데이터를 보고 데이터가 속할 그룹을 판단**하는 classification 알고리즘입니다.  
K-means는 비지도학습으로, **별도의 레이블이 없는 데이터 안에서 데이터 간 유사도를 기준으로 묶는** clustering 알고리즘입니다.

> KNN과 K-means의 가장 큰 차이는 지도학습과 비지도학습입니다. 둘은 모두 K개의 점을 지정하여 거리르 기반으로 구현되는 거리기반 분석 알고리즘입니다.  
먼저 **KNN**(최근접 이웃방법)은 해당 데이터와 가장 가까이 있는 K개의 데이터를 확인하여 새로운 데이터 특성을 확인하는 방법입니다. 지도학습 알고리즘 중 하나로 K가 짝수면 1:1 대응이 될 수도 있기 때문에 K는 홀수를 쓰는 것이 보편적입니다. KNN은 **회귀와 분류 모두 사용 가능**합니다.  
**K-means** 알고리즘은 데이터를 K개의 군집(cluster)으로 묶는 clustering 알고리즘입니다. 여기서 K는 묶을 그룹의 수를 말하고 Means는 데이터로부터 그 데이터가 속한 그룹의 중심까지의 평균 거리를 의미하며 이 값을 최소화 하는 것이 K-means입니다.

</br>

### XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?

XGBoost는 **Gradient boosting**을 이용한 모델입니다. 병렬 연산을 지원하고 GPU를 사용할 수 있는 옵션이 있기 때문에 학습하는데 소요되는 시간을 절약할 수 있습니다.  
XGBoost는 **빠르고** 다양한 커스텀 옵션으로 **유연성**이 좋습니다. 또한 그리디 알고리즘으로 과적합 방지가 가능해 캐글에서 좋은 성능을 보이고 있습니다.

> **XGBoost**는 속도가 빠르고, 자원 효율성이 높아서 캐글에도 인기가 많습니다. **XGBoost**는 기존 Gradient Tree Boosting 알고리즘에 과적합 방지를 위한 기법이 추가된 지도학습 알고리즘 입니다. **XGBoost**는 기본 학습기를 의사결정나무로 하며 Gradient Boosting과 같이 **Gradient(잔차)를 이용**하여 이전 모델의 약점을 보완하는 방식으로 학습니다. **XGBoost**는 과적합 방지가 잘되고 예측 성능이 좋다는 장점이 있지만 작은 데이터에 대해서는 과적합 가능성이 있다는 단점을 가지고 있습니다.
분류와 회귀문제에 모두 사용가능하며, XGBoost는 자체에 얼리스탑과 같은 과적합 규제 기능이 있어 강한 내구성을 지닙니다. 또한, 다양한 파라미터 옵션을 제공하는 Customizing이 용이하기도 합니다. 이러한 이유들로 기존 GBoost보다 XGBoost가 더 인기 있습니다.

</br>

### 회귀 / 분류시 알맞은 metric은 무엇인가요?

회귀 문제에서는 실제 값과 모델이 예측하는 값의 차이에 기반을 둔 metric(평가)을 사용합니다. 대표적으로 **RSS(단순 오차 제곱 합), MSE(평균 제곱 오차), MAE(평균 절대값 오차)** 가 있습니다.  
분류 문제에서는 분류 결과의 신뢰도를 나타낼 수 있는 metric을 사용합니다. 대표적으로는 **Accuracy, Precision, Recall와 이를 조합합 F1 score**등이 있습니다.

> [Why is cross entropy not a common evaluation metric for model performance?](https://stats.stackexchange.com/questions/370861/why-is-cross-entropy-not-a-common-evaluation-metric-for-model-performance)

</br>

### SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?

SVM을 비선형 분류 모델로 사용하기 위해 **저차원 공간의 데이터를 고차원 공간으로 매핑**하여 선형 분리가 가능한 데이터로 변환하여 처리합니다. SVM이 좋은 이유는 SVM은 데이터들을 선형 분리하며 최대 마진의 초평면을 찾는 크게 복잡하지 않은 구조이며, **커널 트릭을 이용**해 차원을 늘리면서 비선형 데이터들에도 좋은 결과를 얻을 수 있습니다. 또한 이진 분류 뿐만 아니라 수치 예측에도 사용될 수 있습니다. **Overfitting 경향이 낮으며** 노이즈 데이터에도 크게 영향을 받지 않습니다.

> SVM 회귀의 경우 SVM 분류와 반대로, 제한된 마진 오류(즉, 도로 밖의 샘플) 안에서 마진 안에 가능한 한 많은 샘플이 들어가도록 학습합니다.

</br>

###  ROC 커브에 대해 설명해주실 수 있으신가요?

ROC 커브는 거짓 긍정(False Positive)을 피하면서, 참 긍정(True Positive)을 탐지하는 것 사이의 트레이드오프를 관찰하기 위한 지표로 FPR(False Positive Rate)를 x축으로, TPR(True Positive Rate)를 y축으로 나타내는 곡선입니다.  
이러한 ROC curve는 왼쪽 상단에 가까울수록, 그래프의 아래 면적을 의미하는 AUC (Area Under the Curve)는 1에 가까울수록 좋은 성능이라고 판단합니다.

</br>

### 회귀 / 분류시 알맞은 손실함수와 이에 대한 설명

회귀 문제에서는 **MSE나 MAE** 손실함수를 사용합니다.  MSE는 예측값과 실제값 간의 차이를 제곱한 값의 평균으로, 회귀 문제에서 가장 많이 사용되는 손실함수입니다. **MAE는** 예측값과 실제값 간의 차이의 절댓값의 평균으로, **이상치(outlier)가 있는 데이터에 민감하지 않다**는 특징이 있습니다.  
분류 문제에서는 이진 분류의 경우 Binary **Cross Entropy**를 다중 분류의 경우 Categorical Cross Entropy를 주로 사용합니다. 이외에도 불균형 데이터셋의 성능을 향상시키기 위해 사용되는 Focal loss가 있습니다. Focal loss 는 Categorical Cross Entropy에 (1 - y_pred)^γ를 곱하여  γ (gamma)값이 증가할수록 쉬운 샘플에 대한 가중치가 줄어들어 어려운 샘플에 대해 더 집중적으로 학습할 수 있게 합니다.

> 손실함수(loss funciton)는 딥러닝 모델이 실제 레이블과 가장 가까운 값이 예측되도록 훈련할 때, 모델의 예측값과 실제 레이블 간의 거리를 측정하기 위해 사용되는 함수입니다.

</br>

### 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?

**뉴턴법은 현재 x값에서 접선이 x축과 만나는 지점으로 x를 이동시켜 가면서 점진적으로 해를 찾아가는 방법**입니다. 초기값을 잘 주면 수렴 속도가 매우 빠르지만, 잘못 주면 시간이 오래 걸리거나 해를 찾지 못할 수 있습니다. 함수가 미분 가능해야하고 미분값이 0이 지점이 없어야합니다.   
반면 **Gradient Descent는 현재 위치에서 함수의 기울기를 이용하여 극소점을 찾아가는 방법**으로 local minimum에 빠질 수 있다는 문제가 있지만, 모든 차원과 모든 공간에서 적용이 가능합니다.  
Gradient Descent는 매 단계에서 함수의 기울기를 계산해야 하므로 계산 비용이 낮지만, 수렴 속도가 상대적으로 느릴 수 있습니다. Newton's Method는 매 단계에서 이차 도함수를 계산해야 하므로 계산 비용이 높아질 수 있지만, 수렴 속도가 빠를 수 있습니다.


</br>

### 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?

**머신러닝**은 모델에 대한 정교한 가정보다는 데이터의 다양한 피쳐를 사용하여 **높은 예측률**을 달성하고자 합니다. 따라서 머신러닝은 데이터의 복잡한 패턴을 인식하고 이를 통해 예측하는 능력이 뛰어나며, 데이터가 매우 큰 경우에도 높은 정확도로 예측이 가능합니다.  
반면 **통계적 접근방법**은 데이터의 분포와 가정을 통해 **신뢰 가능한 모델**을 만드는게 목적으로 다양한 통계 모델링 기법을 사용합니다. 통계적 접근방법은 데이터의 특성을 파악하고, 변수의 선택과 추정, 가설 검정 등을 수행하며 **어떤 피쳐가 어떤 원인을 주는지 알 수 있다**는 특징이 있습니다. 

</br>

### 앙상블 방법엔 어떤 것들이 있나요?

앙상블 방법엔 **voting, bagging, boosting, stacking**이 있습니다.  
**Voting**은 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식으로 서로 다른 알고리즘을 여러 개 결합하여 사용합니다.  
**Bagging**은 각각의 데이터로 모델 여러개를 독립적으로 만들어서 각각 모델에서 예측값의 평균이나 최다투표값을 사용합니다.   
**Boosting**은 먼저 모델을 만들고, 그 모델이 약한 데이터에 대해서 새로운 모델(weak learner) 을 만든 후, weak learner를 합쳐서 strong learner를 만듭니다.  
이 외에 모델의 output을 새로운 독립변수로 사용하는 **stacking**이 있습니다.

> **앙상블(Ensemble)** 은 여러개의 모델을 조합해서 더 나은 예측 성능을 달성하는 머신러닝 기법입니다. 이러한 앙상블 방법에는 Voting, Bagging, Boosting, Stacking 등의 방법이 있습니다.  
**보팅(Voting)** 은 여러 개의 예측 모델을 결합하여 다수결 투표를 하는 방법입니다. 보팅을 사용하면 여러 모델이 조합되어 더 일반화된 성능을 보일 수 있습니다.  
**배깅(Bagging, Bootstrap Aggregation)** 이란 샘플을 여러번 뽑아(Bootstrap = 복원 랜덤 샘플링) **여러개의 독립적인 모델을 학습**시켜 각 모델의 예측 결과를 집계하는 **병렬적인** 방법입니다. 이때, 카테고리형 데이터는 투표 방식(Votinig)으로 결과를 집계하며, 연속형 데이터는 평균으로 집계합니다. Bagging을 사용한 대표적인 알고리즘으로는 랜덤 포레스트(Random Forest)가 있습니다. 이러한 배깅을 사용하게 되면, 학습 데이터가 충분하지 않더라도 충분한 학습효과를 주어 높은 bias의 underfitting 문제나, 높은 variance로 인한 overfitting 문제를 해결하는데 도움을 준다는 장점이 있습니다.  
**부스팅(Boosting)** 이란 이전 모델의 잘못 예측한 샘플에 가중치를 높여서 다음 모델을 학습하는 **순차적인** 방법입니다. 오답에 더 집중하여 학습시키기 떄문에 일반적으로 배깅에 비해 정확도가 높은 편이지만, 틀렸던 부분에 대해 반복적으로 학습하므로 오버피팅의 문제가 있으며, 이상치에 취약하고, 속도가 느리다는 단점이 있습니다. Boosting을 사용한 대표적인 알고리즘으로는 그래디언트 부스팅(Gradient Boosting)과 XGBoost, AdaBoost 가 있습니다.  
**스태킹(Stacking)** 이란 여러 개별 모델이 예측한 결과값을 다시 학습 데이터셋으로 사용해서 모델을 만드는 방법입니다. 그러나 같은 데이터셋을 통해 예측한 결과를 기반으로 다시 학습하게 되면 오버피팅 문제점이 있습니다. 따라서 이전 모델의 훈련 데이터에서 검증 데이터를 나누어 학습하는 Cross Validation 방식을 도입하여 이를 해결할 수 있습니다. 이를 통해, 첫 번째 단계에서 학습한 모델들이 하나의 검증 데이터에 대해 과적합되는 것을 방지하고, 더욱 일반화된 모델을 만들 수 있습니다.

</br>

### 불균형 데이터를 어떻게 해결할 수 있을까요?

불균형 데이터 상태 그대로 예측하게 된다면 **과적합 문제가 발생**할 수 있습니다. 모델은 가중치가 높은 클래스를 더 예측하려고 하기 때문에 accuracy는 높아질 수 있지만 분포가 작은 값에 대한 precision은 낮을 수 있고, 분포가 작은 클래스의 재현율이 낮아지는 문제가 발생할 수 있습니다. 이를 해결하기 위해 일반적으로 대표적으로 **Under Sampling과 Over Sampling**이 있습니다. 이 외에도 소수 클래스 데이터에 높은 가중치를 부여하는 방법, 소수 클래스 데이터를 기반으로 합성 데이터를 생성하여 데이터를 보강하는 방법이 있습니다.

> **Under Sampling**은 Down Sampling라고도 불리며 데이터의 분포가 높은 값을 낮은 값으로 맞춰주는 작업을 거치는 것을 말합니다. 데이터 분포를 확인 후 높은 class를 낮은 class크기에 맞춰주는 작업을 거치게 됩니다. 이런 과정을 거치면 유의미한 데이터만 남길 수 있지만 정보가 유실되는 문제가 생길 수 있습니다. 대표적인 방법으로는 Random Under Sampling, Tomek link, CNN 등이 있습니다.  
**Over sampling**은 Up Sampling라고도 불리며 분포가 작은 클래스의 값을 분포가 큰 클래스로 맞춰주는 샘플링 방법입니다. 분포가 작은 클래스 값을 일련의 과정을 거쳐 생성하는 방법을 뜻합니다. 이런 과정을 거치면 정보의 손실을 막을 수 있지만 여러 유형의 관측치를 다수 추가하기 때문에 오히려 오버피팅을 야기할 수 있습니다. (따라서 새로운 데이터, Test dataset에서의 성능이 나빠지는 결과를 초래할 수 있습니다.) 대표적인 방법으로는 Random Over Sampling, ADASYN(Adaptive Synthetic Sampling), SMOTE 등이 있습니다.

</br>

### K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)

K-means clustering은 중심값 선정, 거리로 데이터 분류, 분류 완료까지 반복의 과정을 거치기 때문에 **중앙값 초기화를 잘못 할 경우 local minima에 빠질 위험**이 있습니다. 또한 모든 데이터를 거리로만 판단하기 때문에 사전에 주어진 목적이 없어 **결과 해석이 어렵다**는 단점이 있습니다. 노이즈가 많은 경우 효과적이지 않으며, 특히 하나의 군집이 큰 경우에는 해당 군집이 쪼개지면 비슷한 크기들의 클러스터로 형성될 수 있습니다. 데이터의 분포모양이 특이한 경우에도 군집이 잘 이루어지지 않습니다.


</br>

### A/B 테스트는 무엇인가요?

A/B 테스트란 두 제품 혹은 절차 중 어느 것을 택할지 결정하기 위해 귀무가설과 대립가설을 세워 실험을 진행합니다. A/B test의 진행은 조사 수행, 가설 세우기, 변형, 테스팅, 결과 분석 및 결론 도출 순으로 진행됩니다.

</br>

### Normalization과 regularization의 차이는?

Normalization은 데이터를 스케일링하여 **값의 범위를 균일화** 해주는 과정이며, 데이터의 상대적 크기 차이를 줄이고 분산을 고르게 분배하여 모델의 성능을 개선합니다.  
반면, Regularization은 모델의 복잡도를 제한하고 일반화 성능을 향상시키는 방법으로, **모델의 가중치를 제한하여 overfitting을 방지**합니다.   
Normalization은 데이터 전처리 과정 중 하나이며, Regularization은 모델 훈련 과정에서 수행됩니다.

</br>

### F1 score 사용하는 이유를 말해주세요.

F1 score는 **데이터의 라벨이 불균형 구조일 때 모델의 성능을 정확하게 평가**할 수 있고 성능을 하나의 숫자로 나타낼 수 있습니다. Recall과 precision의 **조화평균을 이용**해 산술평균보다 둘 중 높은 값의 영향을 줄일 수 있습니다.

</br>

### (Super-, Unsuper-, Semi-Super)vised learning이란 무엇인가?

Supervised learning(지도학습)은 입력 데이터와 이에 대응되는 레이블 데이터(정답)를 이용하여 모델을 학습시키는 기계 학습 방법입니다. 즉, 입력과 출력 사이의 관계를 학습하여 새로운 입력 데이터에 대한 출력을 예측하는 모델을 생성합니다. Classification, regression 등이 있습니다.  
Unsupervised learning(비지도학습)은 입력 데이터에 대한 레이블이 없는 상태에서 패턴이나 구조를 발견하는 기계 학습 방법입니다. Clustering, 이상탐지, autoencoder 등이 이에 해당합니다.  
Semi-supervised learning(반지도학습)은 레이블이 있는 데이터와 레이블이 없는 데이터를 모두 사용하여 모델을 학습시키는 기계 학습 방법입니다. 레이블이 있는 데이터를 통해 모델을 학습시킨 후, 레이블이 없는 데이터의 예측 결과에 대해서 직접 검토하여, 잘못된 분류 결과를 수정하거나 레이블을 부여할 수 있습니다.

</br>

### Feature space에서 feature 간의 distance 측정 방법을 설명해주세요.

Feature space에서 distance를 측정하는 방법에는 Euclidian, Cosine, Manhattan 등 여러가지 방법이 있습니다.  
**Euclidian distance**는 가장 흔히 사용되는 방법으로, 각 feature의 차이를 제곱하여 합한 후 제곱근을 취한 값입니다. 차원이 높아질 경우 계산량이 급격히 증가하기 때문에 **2~3차원 정도의 저차원 데이터**에서 쓰일 수 있습니다.  
**Cosine distance**는 내적공간의 두 벡터간 각도와 코사인값을 이용하여 측정된 **벡터간 유사한 정도**입니다. 거리는 고려하지 않고 방향만 고려하기 때문에 **2차원보다 높은 차원의 데이터, 또 벡터의 크기가 중요하지 않은 데이터**에 주로 사용됩니다.   
**Manhattan distance**는 **각 좌표의 차이의 절댓값을 모두 더한 것**을 거리로 사용합니다. 이 거리 측정 방법은 특징 벡터의 각 차원이 서로 독립적인 값을 가지는 경우나 범주형 데이터에 사용될 수 있습니다.   
이 외에도 Hamming, Chebyshev, Jaccard 등 다양한 distance 측정 방법이 있습니다.
> Feature space(특성 공간): 데이터의 특성을 나타내는 변수들로 이루어진 공간
<br/>

### 확률론적 모델링(Probabilistic Modeling)이란 무엇인가요?

데이터의 확률적인 구조와 패턴을 모델링하는 것을 의미합니다. 확률론적 모델링은 일반적으로 확률 분포를 가정하고, 주어진 데이터에 가장 잘 맞는 모델 파라미터를 추정하는 과정을 거칩니다. 대표적인 확률론적 모델링 방법에는 확률적 회귀, 나이브 베이즈, 히든 마르코프 모델, 베이지안 네트워크 등이 있습니다.
<br/>


### 머신러닝 모델의 결과를 해석하는 방법에는 어떤 것들이 있나요?

머신러닝 모델의 결과를 해석하는 방법에는 일반적으로 사용되는 방법에는 다음과 같은 것들이 있습니다.   
feature 중요도를 파악하여 어떤 feature가 모델을 예측하는 데에 많이 기여하는지 분석할 수 있습니다.   
feature 히트맵을 통해 각 특성이 모델의 예측에 미치는 영향을 파악할 수 있습니다.  
의사 결정 트리와 같이 해석이 가능한 모델을 사용하여 모델의 분기 기준들을 파악할 수 있습니다.  
이 외에도 모델의 정확도와 정밀도, 재현성들을 통해 모델의 학습 시간에 따른 성능을 파악할 수 있습니다.
> 머신러닝 모델의 결과를 해석하기 위해서는 **PDP(Partial Dependence Plot)나 SHAP Value**를 사용할 수 있습니다.
PDP는 예측모델을 만들었을 때, 어떤 특성이 예측모델의 타겟변수에 어떤 영향을 미쳤는지 알기 위한 그래프입니다. 특성과 타겟변수의 관계를 전체적으로만 파악할 수 있을뿐 개별 관측치에 대한 설명을 하기에는 부족하다는 한계가 있습니다.  
SHAP Value는 게임이론을 바탕으로 하나의 특성 대한 중요도를 알기위해 여러 특성들의 조합을 구성하고 해당 특성의 유무에 따른 평균적인 변화를 통해 얻어낸 값입니다.  
<br/>

### Ridge와 Lasso에 대해 설명해주세요.
Ridge Regression과 Lasso Regression은 선형 회귀의 일종으로, 과적합을 방지하기 위해 가중치값을 loss term에 포함하는 제약조건을 추가하는 방법입니다. 이때  alpha라는 하이퍼파라미터를 통해 제약조건의 강도를 조절할 수 있습니다. alpha 값이 커질수록 제약조건의 강도가 강해지며, 회귀 계수의 크기가 더욱 축소됩니다.  
**Ridge Regression은 선형 회귀의 손실 함수에 L2 노름의 제곱을 더하는 제약조건을 추가**합니다. 이를 통해 회귀 계수의 크기를 제한하고, 과적합을 방지할 수 있습니다. Ridge Regression은 모든 특성(feature)을 사용하므로, 특성들 사이의 상호작용이 중요한 경우에 유용합니다.   
**Lasso Regression은 선형 회귀의 손실 함수에 L1 노름을 제약조건으로 추가**합니다. L1 노름은 계수를 절댓값으로 취하기 때문에, 특정 feature들을 0으로 만들 수 있습니다. 이를 통해 특성 선택(feature selection)이 가능해지며, 불필요한 특성이 많은 경우에 유용합니다.
</br>


### PCA의 동작 방식을 설명해주세요.
PCA는 데이터의 평균으로 원점을 옮긴 후, 데이터의 공분산행렬, 고유값, 고유벡터를 구합니다. 이때 고유벡터를 기저라고 가정하고 데이터를 보면, 가장 큰 분산을 관점으로 데이터를 보게 되는데, 이 가장 큰 분산을 가진 주성분으로 새로운 축을 선택함으로써, 기존의 데이터의 정보를 가능한 유지하면서 차원 축소를 할 수 있게 됩니다.

> **PCA는 다음과 같은 단계로 이루어집니다.** 
먼저 학습 데이터셋에서 분산이 최대인 축(axis)을 찾습니다. 이렇게 찾은 첫번째 축과 직교(orthogonal)하면서 분산이 최대인 두 번째 축을 찾습니다. 그리고 첫 번째 축과 두 번째 축에 직교하고 분산을 최대한 보존하는 세 번째 축을 찾는 방식을 처음부터 반복하며 데이터셋의 차원(feature 수)만큼의 축을 찾습니다.


### PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?
PCA는 가장 큰 분산을 가진 주성분들로 새로운 축을 선택하게 되므로, 피쳐의 수가 줄어들기 때문에 차원이 축소되며, 기존보다 압축된 데이터를 가지게 됩니다. 또한 이때 주요한 의미를 가지는 피쳐를 선택한 것이기 때문에, 상대적으로 설명력이 낮은 컬럼을 배제하여 노이즈를 제거할 수 있게됩니다.


### SVM에서 Margin을 최대화하면 어떤 장점이 있나요?
Support Vector Machines (SVM)에서 Margin을 최대화함으로써 새로운 데이터에 대한 예측이 더 명확해 질 수 있으며, 이상치에 영향을 덜 받게 되기 때문에 일반화 성능이 향상되게 됩니다. 또한, 마진이 커질수록 모델이 간단해지면서 과적합을 방지하게 됩니다.

### Metric과 Loss의 차이를 설명해주세요.
**Loss**는 학습 과정에서 모델의 예측과 실제 값 사이의 차이를 나타내는 함수입니다. 손실함수로서 모델을 훈련시킬 때 이 손실 함수를 최소로 만들어주는 가중치들을 찾는 것을 목표로 합니다.
**Metric**은 평가지표로서 주로 검증셋에서 훈련된 모델의 성능을 평가할 때 사용합니다.
metric과 loss를 비교하면서 모델이 과대적합인지 과소적합인지의 여부를 확인할 수 있습니다.


### 특이도가 뭔가요?
특이도(Specificity)는 이진 분류 문제에서 실제 Negative인 샘플 중에서 모델이 Negative로 예측한 비율을 나타내는 지표입니다. 이러한 특이도는 거짓 양성(False Positive)을 최소화하는 것에 초점을 두는 경우에 중요한 지표입니다.
<br/>
$Specificity = \frac{True Negatives}{True Negatives + False Positives}$

<br/>

## 회귀에서 SST, SSR, SSE 가 무엇인지, 의미

SST : 종속 변수의 총 변동성. 모든 관측값과 평균값 간의 제곱합  
SSR : 독립 변수들이 설명할 수 있는 종속 변수의 변동성. 예측값과 평균값 간의 제곱합  
SSE : 설명하지 못한 종속 변수의 변동성. 실제 관측값과 회귀 모델로 예측한 값 간의 제곱합

>가장 쉽게 그려낼 수 있는 회귀선은 데이터의 평균으로 예측하는 것입니다. 
그렇기에 평균값으로 예측했을 때의 오차와 구해낸 회귀식으로 예측했을 때의 오차를 비교하고, 얼마나 잔차를 줄여냈는지 관찰하게 됩니다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdTPP9B%2FbtrTAMUSLX2%2FEgXfQd9g2wZ3PVuLC4P9i0%2Fimg.png">

<br/>

---
## Deep Learning

### TensorFlow, PyTorch 특징과 차이가 뭘까요?

Tensorflow는 단일 데이터 흐름으로, **그래프를 만들고 그래프 코드를 성능에 맞게 최적화한 다음 모델을 학습하는 define-and-run** 방식이기 때문에 더 쉽게 다른 언어나 모듈에 적용이 가능합니다.  
PyTorch는 **각 반복 단계에서 즉석으로 그래프를 재생성하는 define-by-run** 방식이라 모델 그래프를 만들 때 고정 상태가 아니므로 데이터에 따라 조절이 가능한 유연성을 갖고 있습니다.

> Tensorflow는 **Define and Run** 방식으로 코드를 직접 돌리는 환경인 세션을 만들고, placeholder를 선언하고 이것으로 계산 그래프를 생성한 후에(Define), 코드를 실행하는 시점에 데이터를 넣어 실행하는(Run) 방식입니다. 이는 모델 구조가 미리 정의되어 있기 때문에, 모델 배포가 쉽다는 장점이 있지만, 모델 구조를 변경하려면 새로운 모델을 정의해야 하기 때문에 유연성이 떨어진다는 단점이 있습니다.  
반면, PyTorch는 **Define by Run** 방식으로 계산 그래프의 선언과 동시에 데이터를 집어넣고 세션도 필요없이 돌리면 되기때문에 코드가 간결하고, 더 유연하게 수정하거나 실험할 수 있다는 장점이 있습니다. 그렇지만, 입력 데이터마다 새로운 계산 그래프를 정의하여 사용한다는 단점이 있습니다.

> TensorFlow에서는 Grappler라는 기본 그래프 최적화 시스템이 있습니다. Grappler는 그래프 모드 (tf.function 내)에서 최적화를 적용하여 그래프 단순화 및 함수 본문 인라인과 같은 기타 고급 최적화를 통해 TensorFlow 계산 성능을 향상하여 절차 간 최적화를 가능하게 합니다

<br/>

### ReLU로 어떻게 곡선 함수를 근사하나요?

ReLU는 **선형(y=x)과 비선형(y=0)의 결합**이기 때문에 ReLU가 반복해 적용되면 선형부분의 결합으로 곡선 함수를 표현할 수 있습니다. ReLU를 여러 개 결합하면, **특정 지점에서 특정 각도만큼 선형 함수를 구부릴 수 있습니다**. 이 성질을 이용하여 곡선 함수 뿐만 아니라 모든 함수에 근사를 할 수 있게 됩니다.


<br/>


### Data Normalization은 무엇이고 왜 필요한가요?

**Feature들의 분포(scale)을 조절하여 균일하게** 만드는 방법입니다. 즉, 개별 피처의 크기를 모두 똑같은 단위로 변경하는 것입니다.

정규화를 하는 이유는 피쳐들간의 스케일이 심하게 차이가 나는 경우, 값이 큰 피처가 더 중요하게 여겨질 수 있기 때문입니다. 데이터 정규화를 하게 되면 **학습속도가 개선되며, 오버피팅을 억제**할 수 있다는 장점을 얻을 수 있습니다.

이러한 정규화하는 방법으로는 대표적으로 **최소-최대 정규화(min-max norm)와  Z-점수 정규화**가 있습니다.

<br/>

### 활성화 함수가 왜 필요한가요?

Activation function은 **선형 함수를 비선형함수로** 만들어 표현력을 더 키워주는 함수입니다. Activation function이 없을 경우 layer를 깊게 쌓아도 선형함수기 때문에 의미가 없습니다. Activation function을 이용해 **모델의 복잡도를 높이고** 복잡한 비선형적인 문제를 해결할 수 있게 만듭니다.

<br/>

### Gradient Descent에 대해서 중학생이 이해할 수 있게 쉽게 설명해주세요.

Gradient Descent는 **함수의 최솟값을 찾기 위해 기울기(경사도)를 이용**하여 함수를 내려가는 것입니다. 이때, 경사도의 반대 방향으로 내려가면서 최솟값에 점점 가까워질 수 있습니다.

딥러닝에서 Gradient Descent는 모델의 가중치(weight)를 업데이트하는 데에 사용됩니다. 모델의 손실(loss)을 최소화하기 위해, Gradient Descent를 사용하여 모델의 가중치를 업데이트하면서 최적의 모델을 학습할 수 있습니다.


<br/>

### 오버피팅일 경우 어떻게 대처해야 할까요? 알고계신 방법들을 전부 말해주세요.

Overfitting이 일어날 경우에는 augmentation를 이용해 **데이터의 절대적인 양을 늘리거나 regularization, dropout을 사용하고, early stopping**으로 일정 횟수 이상 validation loss가 증가하는 시점부터 overfitting이 발생했다고 판단하고 이에 학습을 종료시킵니다. 또는 **Batch Normalization**으로 데이터 분포를 통일 시켜줘서 과적합을 방지합니다.

> Overfitting을 막기 위해 여러 방법이 있습니다. 
> 1. 데이터의 양이 적을 경우, 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현상이 발생할 확률이 늘어납니다. 이럴 경우 기존의 데이터를 증강시켜 데이터를 늘리고 데이터가 많을수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있습니다.
> 2. 복잡한 모델은 간단한 모델보다 과적합될 가능성이 높으므로 정규화를 통해서 복잡한 모델를 좀 더 간단하게 하는 방법이 있습니다.
> 3. Dropout을 사용해 학습과정마다 일정 비율의 뉴런만 사용하는 방법을 사용합니다.
> 4. 일정 횟수 이상 validation loss가 증가하는 시점부터 overfitting이 발생했다고 판단하고 이에 학습을 종료시킵니다,
> 5. Batch Normalization으로 데이터 분포를 통일 시켜줘서 과적합을 방지합니다. 

</br>

### Dropout은 무엇이며 왜 사용하는 지 말해주세요.

Dropout은 서로 연결된 layer에서 **0에서 1사이의 확률로 뉴런을 제거(drop)** 하는 기법입니다. 꺼지는 뉴런의 종류와 개수는 오로지 랜덤하게 Dropout rate에 따라 결정이됩니다.
Dropout은 어떤 특정한 설명변수 Feature만을 과도하게 집중하여 학습함으로써 발생할 수 있는 overfitting을 방지하기 위해 사용됩니다. 학습 데이터에 의해 각 node들이 **co-adaptation**되는 현상을 방지하고 매 학습마다 drop 되는 뉴런이 달라지기 때문에 서로 다른 모델들을 **앙상블 하는 것과 같은 효과**가 있습니다.

> co-adaptation: 어느 시점에서 같은 층의 두 개 이상의 노드의 입력 및 출력 연결강도가 같아지면, 아무리 학습이 진행되어도 그 노드들은 같은 일을 수행하게 되어 불필요한 중복이 생기는 문제

</br>

### GD가 Local Minima 문제를 피하는 방법을 말해주세요.

초기값을 잘 선정하거나, LambdaLR, CosineAnnealingLR 등 **학습률을 조절하는 학습률 스케줄링(learning rate scheduling)** 기법을 사용할 수 있습니다. 또는 Momentum을 사용하여 이전에 이동했던 방향과 크기를 고려하여 local minima에 빠지는 것을 방지할 수 있습니다. 이 외도 Gradient descent에서 파생된 Adagrad, Adadelta, RMSprop, Adam 등 다른 optimizer를 사용하는 방법이 있습니다.

</br>

### Batch Normalization은 무엇인가요?

학습 과정에서 **Mini-batch마다 평균과 분산을 활용하여 데이터의 분포를 정규화**를 하고, scale factor와 shift factor를 이용하여 새로운 값을 생성하는 것입니다. 이때 scale factor와 shift factor는 다른 레이어에서 weight를 학습하듯이 역전파에서 학습됩니다.
기존에는 learning rate를 높게 잡을 경우 gradient가 explode/vanish 하거나, local minima에 빠지는 문제가 있었습니다. 그러나 Batch Normalization을 사용할 경우 **parameter의 scale에 영향을 받지 않게 되어, learning rate를 크게 잡을 수 있어 안정적으로 빠른 학습**을 할 수 있습니다.   
여기서 중요한 것은 Batch Normalization은 학습 단계와 추론 단계에서 조금 다르게 적용되어야 합니다.

</br>

### BN 적용해서 학습 이후 실제 사용시에 주의할 점은 무엇인가요(코드적인 부분 포함)?

Inference 시 input을 이용해 BN을 하면 모델이 train에서 input의 분포를 추정한 의미가 없어지기 때문에, inference 시에는 결과를 deterministic하게 만들기 위해 **미리 저장해둔 mini-batch의 moving average를 이용**해 정규화를 합니다.  
Batch normalization을 적용할 때는 **activation function 앞에 적용**해야 합니다. 일반적으로 Hidden Layer - Batch Normalization - Activiation Function 순서로 적용합니다. Batch Normalization 적용 후 ReLU와 같은 활성화 함수를 적용하면 데이터의 절반가량이 음수이기에 의미 없는 값을 가지게 될 수 있습니다.  

Pytorch에서 BatchNormalization을 사용하는 대표적인 방법은 torch.nn.BatchNorm1d와 torch.nn.BatchNorm2d를 사용하는 것입니다.  
이 때, 차이점은 BatchNorm1d의 경우 Input과 Output이 (N, C) 또는 (N, C, L)의 형태를 가지고 BatchNorm2d의 경우 Input과 Output이 (N, C, H, W)의 형태를 가집니다. 여기서 N은 Batch의 크기를 말하고 C는 Channel을 말합니다. BatchNorm1d에서의 L은 Length을 뜻하고 BatchNorm2d에서의 H와 W 각각은 height와 width를 뜻합니다.
Inference 시에는 BATCHNORM함수에 track_running_stats라는 파라미터에 False의 값을 주어야 합니다.

</br>

### Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?

딥러닝에서 가중치를 잘 초기화하는 것은 기울기 소실이나 local minima 등의 문제를 방지 가능합니다.  
**LeCun** 초기화은 **입력 노드 수를 고려**한 정규 분포와 균등 분포를 따르는 방법입니다.  
**Xavier** 초기화는 **입력 노드의 수와 출력 노드 수를 고려**하여 가중치를 초기화하는 방법입니다. Sigmoid 나 tanh 함수와는 좋은 결과를 보여주지만 ReLU 함수와 사용할 경우 0에 수렴하는 문제가 발생하는 초기화 방법입니다.  
**He** 초기화는 ReLU 와 함께 많이 사용되는 방법으로, LeCun 방법과 같이 **입력 노드의 수만을 고려하지만 상수 부분은 Xavier 초기화의 방법을 사용**합니다.

</br>

### 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)

**Sigmoid**는 **입력을 0 ~ 1 사이로 mapping**합니다. 하지만 입력값이 커질수록 미분값이 0에 수렴해 **gradient vanishing** 문제가 생기며 **zero-centered하지 않아** 학습이 느려집니다.  
**Tanh**는 sigmoid를 변형한 쌍곡선함수로, **입력을 -1 ~ 1 사이로 mapping해 zero-center 문제는 해결**했지만 gradient vanishing 문제는 해결하지 못했습니다.  
**ReLU**는 입력이 양수일 경우 **saturate 문제가 해결**되며, 단순히 max 함수를 사용해 **속도가 빠릅니다**. 입력이 음수일 경우에도 saturate 문제를 해결한 함수로는 **LeakyReLU**가 있습니다.  
분류 문제에는 **softmax**를 사용하기도 합니다.


</br>

### 효율적인 GPU의 사용을 위해 어떻게 dataloader를 조절할 수 있는가?

PyTorch의 DataLoader는 **`batch_size`** 로 batch의 크기를 조절하여 데이터를 미니 배치로 분할하고, 이들을 병렬적으로 로딩하고 전처리하는 함수로 데이터를 GPU로 로드할 수 있으며, 이를 통해 모델 학습 시간을 단축할 수 있습니다.  
DataLoader의 **`num_workers`** 하이퍼파라미터 값이 높을수록 DataLoader는 더 많은 프로세스를 생성하여 데이터 로딩 및 전처리를 병렬로 처리합니다. 이때, 각 워커는 CPU를 사용하여 데이터를 로드하고 전처리합니다. 이를 GPU 메모리로 이동시킬 때, **`pin_memory`** 파라미터를 사용하여 CPU 메모리와 GPU 메모리 간 데이터 이동을 최적화할 수 있습니다. **`pin_memory=True`** 로 설정하면 DataLoader는 Tensor를 CUDA 고정 메모리에 복사하여 GPU 메모리로 전송합니다.

</br>

### 하이퍼 파라미터는 무엇인가요?

하이퍼 파라미터(Hyper-parameter)는 모델링할 때, **사용자가 직접 세팅해주는 값**으로 learning rate, epoch나 SVM에서의 C, sigma 값, KNN에서의 K값 등이 있습니다.
하이퍼 파라미터는 정해진 최적의 값이 없으며, 사용자의 **선험적 지식을 기반으로 설정(휴리스틱)** 합니다. 이러한 하이퍼 파라미터 튜닝 기법에는 Manual Search, Grid Search, Random Search, Bayesian Optimization 등이 있습니다.

</br>

### 미니배치를 작게 할때의 장단점은?

전체 데이터를 쪼개서 여러 번 학습하기 때문에, 전체 training 데이터 셋을 배치로 사용것보다 계산량이 적어(즉 메모리 사용량이 적어), **학습 속도가 빠르다**는 장점이 있습니다. 반면에 big batch보다 느리고 loss function의 최솟값을 찾기 위해 자주 step의 방향을 바꿔야 하므로 **학습이 불안정**한 단점이 있습니다.

> 하지만 요즘엔 Adam optimizer나 batch normalization(BN)등의 기법을 사용함으로써 학습안정화가 정말 잘 되는 네트워크가 많습니다. 따라서 batch size가 커도 local minimum을 잘 지나쳐 global minimum으로 수렴할 수 있게되었습니다. 따라서 최근에는 batch size를 줄 수 있는만큼 최대한 크게줘야 좋다고 할 수 있습니다. batch size가 클수록 BN을 더욱 정확하게 계산할 수 있어 BN의 효과를 더욱 잘 누릴 수 있게되고 그렇게 되면 학습속도가 빨라지며 learning rate의 hyper parameter 설정에서 꽤나 자유로워질 수 있게됩니다.


</br>

### Adam Optimizer의 동작은?

Momentum과 RMSProp를 합친 방식입니다. **Momentum** 방식과 같이 현재 기울기와 이전 기울기의 가중합을 이용하여 파라미터 업데이트를 수행하여 **기울기 방향을 조절**하고, **RMSProp**과 같이 기울기 제곱값의 이동 평균을 이용하여 **학습 속도를 조절**합니다. 따라서 기존의 최적화 알고리즘에 비해 더 빠르고 안정적으로 수렴할 수 있습니다. 오래전 time step에서의 값은 적게 반영하고 최근 step의 값을 많이 반영하기 위한 moving average를 하이퍼파라미터로 사용합니다. 

</br>

### 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?

딥러닝은 머신러닝 알고리즘 중 하나인 **인공신경망(Artificial Neural Network, ANN)을 사용하는 모델**입니다. 머신러닝은 데이터의 특성을 사람이 직접 추출한 후 모델을 학습시키는 반면, 딥러닝은 **모델이 스스로 데이터의 특성을 추출하여 학습을 한다는 차이**가 있습니다.

</br>

### Objective Function, Loss Function, Cost Function의 차이는 무엇인가요?

Loss function은 하나의 input 데이터의 오차를 계산하는 함수입니다. Cost function은 이러한 Loss를 총 데이터에 대해 평균을 낸 값입니다. Objective function은 가장 일반화된 용어로 학습을 통해 최적화하려는 모든 종류의 함수를 의미합니다. 

> **Loss function ⊂ Cost Function ⊂ Objective Function**.  
> - 모델을 학습할 때는 비용(cost), 즉 오류를 최소화하는 방향으로 진행하게 됩니다. 비용이 최소화되는 곳이 성능이 잘 나오는 부분이며, 가능한 비용이 적은 부분을 찾는 것이 최적화 방법입니다. 이 비용 혹은 손실이 얼마나 있는지 나타내는 것이 비용함수, 손실함수라고 할 수 있습니다.
목적 함수는 매개변수를 최적화하는 함수입니다. 손실함수는 데이터 각각에 대해 학습한 매개 변수로 이루어지는 확률 함수 결과 확률 값과 실제 확률 값의 오차를 측정하는 함수입니다. 비용함수는 데이터 집합에 대한 오차 측정 함수입니다. 
즉, Objective Function가 가장 상위 개념이고 Cost Function과 Loss Function은 Object Function의 한 예라 볼 수 있습니다.


</br>

### SGD에서 Stochastic의 의미는?

"Stochastic"이라는 단어는 임의의 확률과 연결된 시스템 또는 프로세스를 의미합니다. 따라서 SGD에서는 각 반복에 대한 전체 데이터 세트 대신 **무작위로 하나의 샘플이 선택**됩니다. 이 방법은 고차원 최적화 문제에서 매우 높은 계산 부담을 줄여 더 빠른 반복을 달성하면서 수렴 속도가 낮아집니다. 이것은 실제 그라디언트(전체 데이터 세트에서 계산된)를 그것의 추정치(무작위로 선택된 데이터의 하위 집합에서 계산된)로 대체하는 경사 하강법 최적화의 확률적 근사치로 볼 수 있습니다.

</br>

### Test 세트가 오염되었다는 말의 뜻은?

Test data에 Training data와 일치하거나, 매우 유사한 데이터들이 포함되어, **test에 overfit하여 unseen data에 대한 성능이 떨어질 수 있다**는 의미입니다.

</br>

### Bias는 왜 있는걸까?

Bias는 타겟과 예측값이 얼마나 멀리 떨어져 있는가입니다. 모델의 복잡도가 낮거나 학습이 덜 된 경우 bias가 커집니다.   
반면 학습 단계에서 Bias는 모델에서 데이터의 편향을 보정하는 역할을 합니다. Bias를 통해 Activation Function을 좌우로 움직일 수 있어, 더욱 좋은 학습을 시킬 수 있습니다.

</br>

### Back Propagation에 대해서 쉽게 설명 한다면?

Back Propagation(역전파)은 신경망에서 가중치와 바이어스를 조정하기 위해 **예측값과 실제값의 차이를 역으로 전파하는 알고리즘**입니다. 따라서 먼저, 입력층에서 출력층까지 순전파를 수행하여 예측값을 구한 후, 예측값과 실제값의 차이를 계산하여 손실(loss)을 구합니다. 이 손실값을 역으로 전파하면서 각 레이어의 가중치와 바이어스를 조정합니다. 이 과정을 반복하면 손실이 최소화되는 최적의 가중치와 바이어스를 찾을 수 있습니다. Back Propagation은 딥러닝에서 학습의 핵심 알고리즘이며, 경사 하강법과 함께 사용되어 네트워크의 가중치를 조정하여 예측 성능을 향상시킬 수 있습니다.


</br>

### 원-핫 인코딩이란?

원-핫 인코딩은 각 범주형 변수를 이진 벡터로 변환하는 방식입니다. 따라서 각 범주에 대해 하나의 이진 변수를 만들고, 해당하는 범주에 해당하는 변수만 1로 표시하고 나머지는 0으로 표시하는 방식입니다. 이러한 원-핫 인코딩은 범주 간의 관계를 없애고 각 범주를 독립적으로 처리할 수 있도록 도와주며, 데이터 형태는 0-1로 이루어졌기 때문에 컴퓨터가 인식하고 학습하기에 용이합니다. 그러나 원-핫 인코딩은 범주의 개수에 따라 변수의 차원이 증가하며, 고차원 데이터에 대해 메모리와 계산 비용이 증가할 수 있는 단점이 있습니다.

</br>

### Ordinal Encoding과 OneHotEncoding는 언제 사용해야할까요? 어떠한 특징들이 있나요?

Ordinal Encoding은 범주형 데이터를 순서에 따라 숫자로 매핑하는 방법입니다. 반면, OneHotEncoding은 각 카테고리에 대해 하나의 이진 변수를 만들고, 해당하는 카테고리에 해당하는 변수만 1로 표시하고 나머지는 0으로 표시하는 방식입니다.  
Ordinal Encoding은 이해하고 구현하기 쉽기 때문에 데이터에 순서가 있는 경우 유용합니다. 그러나 데이터에 순서가 없는 경우 오류가 발생할 수 있습니다. OneHotEncoding은 카테고리간 독립성을 가정하기 때문에 데이터에 순서가 없는 경우에 적합하지만, 데이터가 많으면 계산 비용이 많이 들고 데이터에 불필요한 차원을 추가할 수도 있습니다.

</br>

### n-gram은 무엇인가요?

n-gram은 텍스트 또는 문장을 n개의 연속된 단어 또는 문자의 그룹으로 분할하는 방법입니다. 여기서 "n"은 연속된 요소의 개수를 의미합니다. n-gram은 이전 단어들을 기반으로 다음 단어를 예측하는 데 사용할 수 있으며, 텍스트 분류, 문서 유사도 계산, 텍스트 생성 등의 작업에서도 유용하게 활용됩니다. 그러나 큰 n 값은 데이터의 희소성 문제를 야기할 수 있으며, 작은 n 값은 단어 간 상관 관계를 캡처하지 못할 수 있습니다. 따라서, 적절한 n 값 선택은 n-gram을 사용할 때 고려해야 합니다. 또한, 앞 문장의 맥락을 놓처 문장 내 전체 단어를 고려하는 언어 모델과 비교했을 때 정확도가 낮다는 한계가 있습니다.

</br>

### Non-Linearity라는 말의 의미와 그 필요성은?

선형함수는 입력값과 출력값이 비례하는 함수(1차 함수)입니다. 이러한 선형함수를 제외한 함수는 비선형 함수입니다. 이때 비선형성은 모델이 여러 레이어를 거치면서 보다 복잡한 패턴을 학습할 수 있게 해줍니다. 만약 선형함수로 여러 레이어를 쌓는다면, 이는 결국 하나의 선형 레이어를 거친 모델과 동일하게 됩니다.

</br>

### GD 중에 때때로 Loss가 증가하는 이유는?

손실이 증가하는 이유는 몇 가지가 있습니다. 학습률 크기로 인해 파라미터를 너무 크게 조정할 수 있으며, 지역 최소값에 도달하여 손실이 증가할 수 있습니다. 또한 모델이 학습 데이터에 지나치게 적합되어 새로운 데이터에 대한 일반화 성능이 저하되는 경우 손실이 증가할 수 있고, 그래디언트 계산의 불안정성으로 인해 그래디언트 소실 또는 폭주 문제가 발생할 수 있습니다.  
이 문제를 해결하기 위해서는 적절한 학습률 설정, 초기 파라미터 선택, 정규화 등의 방법을 고려해야 합니다.  
이 외에도 각 optimization 전략에 따라 gradient가 양수인 방향으로도 parameter update step을 가져가는 경우가 생길 수 있으며, 이 경우에는 Loss가 일시적으로 증가할 수 있습니다.

</br>

### Batch size와 learning rate의 관계에 대해 설명해주세요.

일반적으로 배치 크기가 커질수록 한 번의 업데이트 당 사용되는 데이터의 양이 증가하기 때문에 그레디언트의 추정이 더 정확해집니다. 따라서 더 큰 학습률을 사용할 수 있습니다. 그러나 이는 모델과 데이터에 따라 다를 수 있으며, 최적의 배치 크기와 학습률을 찾기 위해서는 실험적인 접근이 필요합니다.

> - batch size 작은데, lr이 큰 경우 :  작은 배치사이즈는 noise를 가지고 있어 이를 큰 학습률을 적용하면 수렴이 어려울 수 있습니다.  
> -  batch size 큰데, lr이 작은 경우 : 많은 데이터를 학습한 결과를 작은 보폭으로 학습하기에 과적합이 발생할 수 있습니다.  
> - batch size가 크고 lr이 큰경우 : 많은 데이터를 학습한 결과를 큰 학습률로 적용하기에 앞선 경우에 비해 적절합니다.  
> - batch size 작고 lr이 작은 경우 : noise가 큰 결과를 작은 학습률로 적용하기에 앞선 경우보다 적절합니다.

</br>

### Feature selection 방법을 설명해주세요.
output을 예측하는데에 상관이 없는 변수들이 존재한다면 overfitting이 발생할 수 있습니다. Feature Selection을 하면 학습 시간을 줄일 수 있고, 모델의 분산을 줄임으로써 보다 robust하게 학습되며 모델이 간소화가 되면서 결과 해석이 더 쉬워지는 장점이 있습니다.
Feature Selection의 주된 목적은 독립 변수중에서 중복되거나 종속변수 y와 관련없은 변수들을 제거하여 y를 가장 잘 예측하는 변수들의 존합을 찾아내는 것입니다. Feature Selection에는 크게 3가지 방법이 있습니다. 
**Wrapper method**
- Feature의 조합을 바꿔가며 반복적으로 성능을 체크하여 feature Selection을 수행하는 방법
- ex) Forward Selection(전진 선택), Backward Elimination(후방 제거), Stepwise Selection(단계별 선택)

**Filter Method**
- 전처리 과정에서 미리 통계적 방법으로 feature Selection을 수행하는 방법
- ex) Information Gain(트리 모델), chi-square test (카이제곱), correlation coefficient(상관계수)

**Embedded method**
- 알고리즘을 통해 feature Selection을 수행하는 방법
- ex) LASSO, Ridge, Elastic Net

</br>

### convolution, depthwise-separable convolution의 차이가 무엇인가요?
Convolution은 입력 이미지와 커널 간의 행렬 연산으로, 모든 입력 채널에 동일한 필터를 적용하여 출력 값을 계산합니다.
Depthwise-Separable Convolution은 입력 이미지의 각 채널 별로 독립적인 필터 연산을 수행한 후, 1x1 컨볼루션을 통해 다양한 채널 간의 선형 결합을 수행합니다. 이를 통해 파라미터 수를 크게 줄이고, 계산 효율성을 향상시킬 수 있습니다.
따라서 Depthwise-Separable Convolution은 기존의 Convolution에 비해 더욱 효율적인 방식으로 모델의 파라미터 수를 줄이고, 계산 속도를 향상시킬 수 있다는 장점이 있습니다.

![Depthwise-Separable Convolution](https://editor.analyticsvidhya.com/uploads/95621Depthwise-Separable-Convolution-Input-channels-are-separated-and-each-is-convolved-with.png)

![](https://velog.velcdn.com/images/ann9902/post/743f641c-c411-4ccf-af1f-4a8e8dfe8427/image.png)

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FKy3Le%2FbtqAmtTZrEs%2FyUlkWdPU1HNa8STmjJe1NK%2Fimg.jpg)


### CNN이 이미지 정보 처리에 적합하고 RNN이 시계열 정보 처리에 적합한 이유가 뭘까요?
CNN이 이미지 처리에 적합한 이유는 CNN은 합성곱 레이어를 통해 입력 이미지의 지역적인 패턴을 학습할 수 있기 때문입니다. 또한, 가중치 공유를 통해 이미지의 모든 위치에서 동일한 feature를 사용할 수 있어 학습 파라미터를 효율적으로 사용할 수 있습니다.
RNN은 시계열 데이터 처리에 적합한 이유는 RNN은 메모리 셀을 가지고 있어 이전 시점의 정보를 기억하면서 현재 시점의 데이터를 처리할 수 있기 때문입니다. 이를 통해 시계열 데이터의 순차적인 패턴을 학습할 수 있습니다.

### backpropagation시 더하기, 곱하기, 활성화함수 등의 각 노드에서 어떻게 연산되는지?

Backpropagation은 연쇄법칙을 이용하여 upstream gradient에서 들어온 값에 local gradient값을 곱하여 그래디언트 계산을 합니다. 
덧셈일 경우 $z = x+y$의 미분은 $x$에 대해 미분해도 1, $y$에 대해 미분해도 1이기 떄문에 덧셈 노드의 역전파는 입력 값을 그대로 흘려보냅니다.
곱셈일 경우 $z = xy$의 미분은 $x$에 대해 미분하면 $y$, $y$에 대해 미분하면 $x$가 됩니다. 
그렇기에 forward에서의 입력값을 서로 바꾼 후, upstream gradient을 곱하여 계산됩니다. 
활성화 함수의 경우에는 
- relu에서는 음수일때는 0, 양수일때는 1의 미분값을 곱해주면 됩니다.
- sigmoid는 simoid결과값을 y라고 두면 y(1-y)이 미분값이기에 이를 곱하면 됩니다.
- tanh는 tanh결과값을 y라고 두면 1-y^2이 미분값이기에 이를 곱하면 됩니다.  

\* upstream gradient: 입력으로 받은 이전 gradient의 값  
\* local gradient: 현재 노드에서의 미분값

</br>

### zero-centered가 아닐 경우 무슨 문제가 발생하나요?

zero centered가 되지 않는 경우 결과값이 모두 양수를 가지게 되고, 이는 역전파시 모든 가중치의 그래디언트가 + 혹은 -로 되어 최적의 해를 곧바로 찾는 것이아니라 zig-zag형태로 움직이며 찾게 되어 비효율적입니다. 이러한 이유로 일반적으로 zero-mean data를 원하고 input x가 양수/음수를 모두 가지고 있으면 gradient w가 전부 Positive/Negative로 움직이는 것을 막을 수 있습니다.

> [zero-centered + gradient vanshing 문제]
> - 활성화 함수가 zero-centered가 아니면, 그래디언트가 양수 또는 음수 부분에 한쪽으로 치우쳐질 수 있습니다. 이러한 활성화 함수를 여러 레이어로 거치게 되면, 그래디언트가 소실이나 폭주와 같은 문제가 발생할 수 있습니다.

> [[모두를 위한 cs231n] Lecture 6. Activation Functions에 대해 알아보자](https://deepinsight.tistory.com/113)

</br>

## 딥러닝 혹은 머신러닝에서 MLE는  어떻게 사용되나요?
최대우도법(maximum likelihood estimation, MLE) 이란 각 가설마다 계산된 우도값 중에 가장 큰 값을 고르는 통계적 추정방법입니다. 
딥러닝에서는 최적화를 위해 경사하강법을 사용하여 log-likelihood 함수를 최대화하는 파라미터 값을 찾습니다. 이때 경사하강법은 최소점을 찾는 방식이기에 기존의 log-likelihood에 -를 붙여준 negative log-likelihood (NLL)이 됩니다. 따라서 딥러닝의 최적화 과정에서는 초기 파라미터 값을 설정하고, 반복적으로 파라미터를 negative log-likelihood (NLL)를 최소화하는 방향으로 업데이트합니다.


> [MSE 손실 함수와 MLE](https://kh-kim.github.io/nlp_with_deep_learning_blog/docs/2-02-probabilistic_perspective/06-appendix_mse_loss/)
> [MLE, 딥러닝 관련 ](https://walwalgabu.tistory.com/entry/MLE%EC%99%80-MAP-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B3%BC%EC%9D%98-%EA%B4%80%EA%B3%84%EC%97%90-%EB%8C%80%ED%95%B41?category=945132)

----

## MLOps

### 쿠버네티스와 도커의 차이점은 무엇인가요?
도커는 컨테이너 런타임의 종류로, 컨테이너를 다루는 일을 하고 쿠버네티스는 컨테이너 오케스트레이션 툴의 한 종류로 다수의 컨테이너 실행을 관리 및 조율하는 역할을 합니다.
즉, **도커**는 한 개의 컨테이너를 관리하는 데 최적화되어있고, **쿠버네티스**는 여러 개의 컨테이너를 서비스 단위로 관리하는 데 최적화되어있습니다.


---

## Network

### OSI 7계층을 설명하시오
OSI 7 계층은 **네트워크에서 통신이 일어나는 과정을 7단계**로 나눈 것을 말합니다. 7계층으로 나눈 이유는 통신이 일어나는 과정을 단계별로 파악하기 위함과 **통신 과정 중에 특정한 곳에 이상이 생길 경우**에 다른 단계의 장비 및 소프트웨어 등을 건드리지 않고 **통신 장애를 일으킨 단계에서 해결할 수 있기 때문**입니다. 계층으로는 물리 계층, 데이터 링크 계층, 네트워크 계층, 전승 계층, 세션 계층, 표현 계층, 응용 프로그램 계층이 있습니다.

> OSI(Open Systems Interconnection Reference Model) 7계층이란, 통신 접속에서 완료까지의 과정을 7단계로 정의한 국제 통신 표준 규약입니다.  
**물리** 계층(physical layer)은 0과 1의 전기적 신호를 전송하는데 필요한 기능을 제공합니다. 장비로는 통신 케이블, 허브가 있습니다.  
데이터링크 계층(Data-Link Layer)은 프레임(Frame)이라는 단위로 MAC 주소를 가지고 같은 네트워크에 있는 컴퓨터들이 데이터를 주고받게 해주는 모듈입니다. 장비로는 브릿지, 스위치가 있습니다.  
네트워크 계층(Network Layer)은 목적지 컴퓨터로 데이터를 전송하기 위해 네트워크 간의 IP 주소를 통해 최적의 경로로 데이터 전달하는 라우팅 기능을 제공합니다. 장비로는 라우터, L3 스위치가 있습니다.  
\* IP : 각 컴퓨터들이 갖는 고유한 주소    
전송 계층(Transport Layer)은 데이터 전송을 위해서 Port 번호를 사용하여, 목적지 컴퓨터의 최종 도착지라고 할 수 있는 프로세스까지 데이터가 전송될 수 있게 하는 모듈입니다.  
세션 계층(Session Layer)은 통신 장치 간 상호작용과 동기화를 제공하여, 데이터 교환과 에러 발생 시 복구를 관리해주는 모듈입니다.  
표현 계층(Presentation Layer)은 세션 계층 간의 주고받는 인터페이스를 일관성있게 제공하는 역할을 합니다.  
애플리케이션 계층(Application Layer)은 사용자가 다른 컴퓨터 간의 통신을 가능하게 하며, 이를 위해 다양한 프로그램과 프로토콜을 사용합니다.

</br>

### TCP/IP의 각 계층을 설명해주세요.

TCP/IP 4계층은 애플리케이션 계층, 전송 계층, 인터넷 계층, 네트워크 접근 계층으로 이루어져있습니다. TCP/IP는 2개의 계층으로 구분됩니다. 메세지나 파일을 작은 패킷으로 나누거나 재조립하여 송수신에 반영하는 일을 담당하는 TCP(상위계층), 각 패킷의 주소 부분들을 처리하여 패킷들이 목적지로 정확히 송수신되도록 기능하는 IP(하위계층)으로 나눠집니다.  
하지만, 인터넷 개발 이후 꾸준히 표준이 갱신되면서 하위 레이어가 다시 세분화되었고, 오른쪽의 TCP/IP Updated 모델이 탄생했습니다. TCP/IP Updated의 5계층 모델은 Link를 다시 두 레이어로 세분화하고, Internet 명칭을 Network로 다시 변경했다는 차이가 있습니다.

</br>

### OSI 7계층와 TCP/IP 계층의 차이를 설명해주세요.

OSI와 TCP/IP는 **둘 다 네트워크 통신 모델의 표준**입니다.  
**OSI** 7계층은 네트워크 전송 시 데이터 표준을 정리한 개념적 모델로 통신에는 **실질적으로 사용되지 않습니다**.  
**TCP/IP는 OSI을 상업적이고 실무적으로 이용될 수 있도록 단순화**한 것이라고 할 수 있습니다. 따라서 현대의 인터넷은 TCP/IP 모델을 표준으로 따르고 있습니다.


</br>

### TCP와 UDP의 차이는?

TCP와 UDP는 모두 데이터를 보내기 위해 사용하는 프로토콜입니다.  
TCP는 패킷을 전송하기 위한 논리적 경로를 배정하는 연결형 서비스로, 3-way handshaking으로 전송의 신뢰성을 보장하지만 속도는 UDP보다 느립니다. 파일 전송 등 연속성보다 **신뢰성이 중요할 때 사용**됩니다.  
UDP는 데이터를 데이터그램 단위로 처리하기 때문에 각각 패킷이 서로 다른 경로로 독립적으로 처리됩니다. UDP헤더의 checksum으로 최소한의 오류만을 검출하기 때문에 신뢰성이 낮지만, TCP보다 속도가 빠르고 네트워크 부하가 적습니다. 신뢰성보단 **연속성이 중요한 스트리밍 등에 사용**됩니다.

> - 3-way handshake는 TCP/IP 프로토콜에서 연결을 설정하는 과정으로, 클라이언트가 서버에게 SYN 패킷을 보내고, 서버가 클라이언트에게 SYN-ACK 패킷을 보내고, 클라이언트가 다시 서버에게 ACK 패킷을 보내는 과정입니다. 3-way handshake를 이용하는 이유는 TCP/IP 프로토콜에서 두 호스트 간에 신뢰성 있는 연결을 설정하기 위해 사용하는 방법으로, 양쪽 호스트가 상대방과 통신 가능한 상태인지 확인하고 연결을 설정하기 때문입니다.

</br>

### REST와 RESTful의 개념을 설명하고 차이를 말해주세요.

REST(Representational State Transfer)란 HTTP 프로토콜을 기반으로 분산 시스템에서 웹 서비스를 구현하기 위한 아키텍처 스타일을 말합니다. RESTful은 REST 아키텍처 스타일을 따르는 웹 서비스를 구현하는 것을 말하며, 이를 구현하기 위해 몇 가지 규칙을 준수해야 합니다. RESTful 웹 서비스는 자원을 URL로 표현하고, HTTP 메서드를 사용하여 자원에 대한 CRUD(Create, Read, Update, Delete) 작업을 수행합니다. 또한, 클라이언트-서버 구조, 상태가 없음(Stateless), 캐시가능(Cacheable), 계층화(Layered System) 등의 제약 조건을 준수하여야 합니다.


</br>

### HTTP GET과 POST 메서드를 비교/설명해주세요

**GET은 서버의 리소스에서 데이터를 요청**할 때, **POST는 서버의 리소스 변경을 요청**할 때 사용합니다.  
GET을 통한 요청은 URL 주소 끝에 `?` 이후로 데이터를 추가하여 전송합니다. 이 방식은 url 이라는 공간에 담겨가기 때문에 전송할 수 있는 데이터의 크기가 제한적이며, 보안이 필요한 데이터에 대해서는 **데이터가 그대로 url에 노출**되는 문제가 있습니다. 그러나 **GET 요청은 브라우저에서 캐시를 사용하므로, 같은 요청을 반복할 경우 빠르게 응답할 수 있습니다.**   
POST는 전송할 데이터를 HTTP 메시지 body 부분에 담아서 서버로 보냅니다. **POST 요청은 URL에 정보가 노출되지 않으므로**, GET 요청과 달리 보안 취약점이 적습니다. 또한 POST 요청은 데이터를 전송할 수 있기 때문에, 서버에 데이터를 변경하는 요청에 적합합니다. 하지만 POST 요청도 개발자 도구나 fiddler 같은 툴로 요청 내용을 확인할 수 있기 때문에 민감한 데이터의 경우에는 반드시 암호화해 전송해야 합니다.

</br>

### 3-way handshake에 대해 설명해주세요.

![](https://gmlwjd9405.github.io/images/network/3-way-handshaking.png)

3-way handshake는 TCP/IP 프로토콜에서 연결을 설정하는 과정으로, **클라이언트가 서버에게 SYN 패킷을 보내고, 서버가 클라이언트에게 SYN-ACK 패킷을 보내고, 클라이언트가 다시 서버에게 ACK 패킷을 보내면 연결이 성립**되는 과정 입니다. 3-way handshake를 이용하는 이유는 TCP/IP 프로토콜에서 **두 호스트 간에 신뢰성 있는 연결을 설정하기 위해 사용하는 방법**으로, **양쪽 호스트가 상대방과 통신 가능한 상태인지 확인하고 연결을 설정**하기 때문입니다.
- SYN : SYNchronize sequence number ; 클라이언트가 서버에 연결 요청을 보낼 때 사용
- ACK : ACKnowledgement ; 수신 측이 데이터를 올바르게 수신했음을 송신 측에 알리는 역할

> + **왜 2 way handshake는 안될까?**  
  TCP는 양방향 연결이기 때문에 클라이언트가 서버에게 존재를 알리고 서버에서도 클라이언트에게 존재를 알리고 대답을 얻어야된다.   
  1 단계. 클라이언트가 서버에게 존재를 알린다.  
  2단계. 서버가 클라이언트의 존재를 알았다고 대답을 하면서 클라이언트에게 내 존재를 알린다.   
3단계. 클라이언트가 서버의 존재를 알았다고 대답을 한다.

</br>

### HTTP와 HTTPS에 대해서 설명하고 차이점에 대해 설명해주세요.

HTTP (Hypertext Transfer Protocol)와 HTTPS (Hypertext Transfer Protocol **Secure**)는 **인터넷에서 데이터를 전송하기 위한 프로토콜**입니다. 특히, **HTTPS는 HTTP에 데이터 암호화가 추가된 프로토콜**입니다.   
즉, **HTTP 통신하는 소켓 부분을 `SSL(Secure Socket Layer)` 또는 `TLS(Transport Layer Security)`라는 프로토콜로 대체**한 것이라고 볼 수 있습니다. 따라서 HTTP 는 원래 TCP 와 직접 통신했지만, HTTPS 에서 HTTP 는 SSL 과 통신하고 **SSL 이 TCP 와 통신** 하게 됩니다. 이러한 SSL (Secure Sockets Layer) 또는 TLS (Transport Layer Security) 프로토콜을 사용하여 데이터를 암호화하여, **데이터가 전송되는 동안 중간에 누군가가 데이터를 볼 수 없도록 보호**합니다.   
이러한 HTTPS를 이용하면 암호화/복호화의 과정이 필요하기 때문에 HTTP보다 속도가 느립니다. 또한 HTTPS는 인증서를 발급하고 유지하기 위한 추가 비용이 발생합니다.  
하지만 최근에는 하드웨어의 발달로 인해 HTTPS를 사용하더라도 속도 저하가 거의 일어나지 않게 되면서, 모든 웹페이지에서 HTTPS를 적용하는 방향으로 바뀌어가고 있다고 알고있습니다.

</br>

### client와 server의 차이점을 설명해주세요.
클라이언트(Client)는 서버(Server)에게 서비스나 정보를 요청하는 시스템이며, 서버는 클라이언트의 요청을 받아서 응답해주는 시스템입니다. 일반적으로 클라이언트는 서비스를 제공받는 사용자 또는 프로그램을 의미하며, 서버는 데이터나 서비스를 제공하는 시스템 또는 프로그램을 의미합니다. 따라서 클라이언트와 서버는 서로 요청과 응답을 주고받으며, 이를 통해 원하는 정보나 서비스를 얻게 됩니다.
> 클라이언트/서버 개념에서 중요한 개념은 서비스를 요청하는 그 무엇인가는 무조건 클라이언트가 되는 것이고 요청에 대한 응답이나 처리를 담당하는 것은 모두 서버가 되는것입니다. 서버 컴퓨터가 다른 서버 컴퓨터에게 서비스를 요청하면 서비스를 요청하는 서버도 클라이언트가 되는 것입니다.

</br>

### DNS가 무엇인가요?
인터넷에 연결된 모든 기기들은 IP주소를 가지고 네트워크 통신을 합니다. 하지만 이러한 IP주소는 사람들이 기억하기 어렵기 때문에, 도메인 이름을 사용합니다. DNS는 이러한 도메인 이름을 IP주소로 변환하는 역할을 합니다.  
따라서 DNS(Domain Name Server 또는 Domain Name Service 모두를 의미)는 숫자로 이루어진 IP 주소와 일정한 형식을 가진 도메인을 서로 매핑 시킨 정보를 가지고 있습니다.

> DNS 서버는 어떻게 동작하나요?  
DNS 서버는 도메인 이름과 해당하는 IP 주소를 저장하고, 클라이언트의 요청에 따라 이를 찾아서 응답합니다. 일반적으로 DNS 계층 구조에서 가장 위에 위치한 루트 DNS 서버에서부터 시작하여 하위 도메인을 차례로 검색하면서 목적지 IP 주소를 찾아갑니다.

</br>

### CORS가 무엇인가요?
CORS(Cross-Origin Resource Sharing)는 서로 다른 출처(origin) 간의 리소스 공유를 허용하는 보안 정책입니다. 웹 애플리케이션은 다른 출처의 리소스에 액세스할 때에, 공격자가 악성 리소스를 사용하여 웹 애플리케이션을 손상시킬 수 있습니다. CORS는 이러한 공격을 방지하기 위해 웹 애플리케이션이 다른 출처의 리소스에 액세스할 수 있는 조건을 제한합니다.

> \* Origin이란 출처를 말하며, Protocol + Host + Port 를 합친 것을 의미합니다.  
> CORS 에러는 서버에서 응답 헤더에 특정 헤더를 포함하는 방식으로 해결할 수 있습니다.
예를 들어 Access-Control-Allow-Origin을 통해 특정 브라우저가 리소스에 접근이 가능하도록 허용할 수 있습니다.

</br>


### 비대칭(공개키) 암호에 대해 설명해주세요.
**비대칭키**는 대칭키의 키교환 문제를 해결하기 위해 등장한 것입니다. 키가 공개되어있기 때문에 키를 교환할 필요가 없어지며 비대칭키는 모든 사람이 접근 가능한 키이고 개인키는 각 사용자만이 가지고 있습니다. 개인키를 가지고 있는 수신자만이 암호화된 데이터를 복호화할 수 있으므로 일종의 인증기능을 제공한다는 장점이 있지만 속도가 느립니다.

> 대칭키 암호화 방식은 암복호화에 사용하는 키가 동일한 암호화 방식을 말합니다. 그와 달리, 비대칭키 암호화 방식은 암복호화에 사용하는 키가 서로 다르며 따라서 비대칭키 암호화라고 합니다.
**대칭키**는 암복호화 키가 동일하며 해당키를 아는 사람만이 문서를 복호화해 볼 수 있습니다. 속도가 빠르다는 장점이 있지만, 키를 교환해야하는 문제가 발생하여 키를 교환하는 중 키가 탈취될 수 있습니다.

> 비대칭키 암호화는 인터넷과 같은 안전하지 않은 채널을 통한 보안 통신에 특히 유용합니다. 예를 들어, 암호화된 메시지를 발송할 때 발신인은 수신인의 공용 키를 사용하여 메시지를 암호화할 수 있습니다. 그런 다음 수신자는 개인 키를 사용하여 메시지의 암호를 해독할 수 있습니다. 개인 키는 비밀로 유지되므로 수신자만 메시지의 암호를 해독할 수 있으므로 통신이 안전하고 기밀로 유지됩니다.
비대칭 암호화는 디지털 서명에도 사용되며, 이를 통해 메시지의 신뢰성과 무결성을 확인할 수 있습니다. 메시지를 보낸 사람은 개인 키를 사용하여 디지털 서명을 생성한 다음 메시지에 추가할 수 있습니다. 수신인은 발신인의 공용 키를 사용하여 서명을 확인하고 전송 중에 메시지가 변경되지 않았는지 확인할 수 있습니다.

</br>


### 쿠키(Cookie)와 세션(Session)을 설명해주세요.

쿠키는 서버에서 **클라이언트의 컴퓨터에 저장하는 작은 기록 정보 파일**이고,
세션은 클라이언트와 서버 간의 연결 상태를 유지하기 위한 기술입니다.
쿠키는 만료 기간이 있지만, 세션은 브라우저를 닫으면 자동으로 삭제됩니다. 따라서, 세션은 일시적으로 클라이언트와 서버 간의 연결 상태를 유지하는 것입니다.
쿠키와 세션은 각각의 특징을 고려하여 사용되며, 보안에 대한 요구 사항에 따라 선택되어야 합니다. 보안성이 중요한 경우에는 세션이 쿠키보다 더 안전합니다.

> 쿠키의 사용 예
>- 방문 사이트에서 로그인 시, "아이디와 비밀번호를 저장하시겠습니까?"
> - 쇼핑몰의 장바구니 기능
>- 자동로그인, 팝업에서 "오늘 더 이상 이 창을 보지 않음" 체크, 쇼핑몰의 장바구니

> 세션의 사용 예
> - 로그인 같이 보안상 중요한 작업을 수행할 때 사용


> **왜 세션이 쿠키보다 안전한가요?**  
쿠키는 클라이언트 측에 저장되고 사용자가 변경할 수 있기 때문에 쿠키를 사용하면 보안상의 문제가 발생할 수 있습니다. 반면에 세션은 서버 측에서 관리되고 클라이언트 측에서는 세션 ID만 가지고 있기 때문에 보안성이 높습니다.

</br>

## WAS란 무엇인가요?

WAS(web application server)란 DB 조회 혹은 다양한 로직 처리를 요구하는 동적 컨텐츠를 제공하기 위해 만들어진 Application 서버이다. HTTP 프로토콜을 기반으로 사용자 컴퓨터나 장치에 애플리케이션을 수행해주는 미들웨어로서, 주로 데이터베이스 서버와 같이 수행된다.
이러한 WAS는 웹 서버의 기능들을 구조적으로 분리하여 처리하고자 하는 목적으로 제시되었다. 분산 트랜잭션, 보안, 메시징, 쓰레드 처리 등의 기능을 처리하는 분산 환경에서 사용된다. WAS는 프로그램 실행 환경과 DB 접속 기능을 제공하고, 여러 개의 트랜잭션을 관리 가능하다. 또한 비즈니스 로직을 수행할 수 있다.

\* 비즈니스 로직 : 업무에 필요한 데이터 처리를 수행하는 로직

</br>

## UDP의 동작 과정에 대해 설명해주세요.

먼저, 데이터그램이 생성되고 송신자는 수신자의 IP 주소와 포트 번호를 설정하여 목적지를 명시합니다. 그런 다음, 데이터는 데이터그램에 담겨 네트워크로 전송됩니다. 이때, 데이터그램은 패킷 단위로 분할되어 전송될 수 있습니다.  
그 후, 수신자는 소켓을 열어 송신자가 명시한 IP 주소와 포트 번호로 들어오는 UDP 패킷을 수신 대기 상태로 만들고, 네트워크로부터 도착한 UDP 패킷을 수신합니다. 수신된 패킷은 패킷 검사를 거쳐 데이터의 무결성을 확인합니다. 이때 UDP는 신뢰성을 보장하지 않으므로 애플리케이션에서 패킷 손실이나 순서 변경 등의 문제를 처리해야 합니다.  
이렇게 패킷 검사가 완료되면 UDP 패킷에서 데이터를 추출하게 됩니다.

> UDP는 비연결성 서비스를 지원하는 프로토콜로, TCP와 비교하여 호스트 간 완전성 또는 신뢰성이 없는 데이터 전달을 합니다.
각 프로세스가 시작되면 운영체제로부터 Port 번호를 부여받게 됩니다. 이때 UDP는 해당 프로세스를 식별할 수 있는 포트와 통신하기 위해 버퍼 역할을 하는 두 개의 큐를 만들고 연결하게 됩니다.
큐는 송신용 목적인 송신 큐(Outgoing Queue), 수신용 목적인 수신 큐(Incoming Queue)로 구분됩니다.
프로세스가 데이터를 외부로 전송하기 위해 송신 큐에 데이터를 밀어넣으면, UDP는 프로세스에서 보낸 데이터를 하나씩 읽고 헤더를 붙여 3계층(IP)로 넘겨준 후 송신 큐에서 제거합니다.
수신 큐에서는 3계칭(IP)에서 수신된 패킷이 있다면, 이를 분석하여 통신하고자 하는 프로세스의 포트 번호와 연결을 시도하고 만약 현재 컴퓨터에 해당 포트 번호를 가진 프로세스가 있다면
패킷에서 통신 과정에 필요했던 헤더 정보를 벗겨낸 후 이를 프로세스로 보내게 됩니다.

</br>

## RESTful을 충족시키기 위한 각 조건들에 대해 설명해주세요.
1. 클라이언트/서버 구조  
    클라이언트와 서버는 독립적으로 개발되어야 하며, 서로 간의 의존성이 없어야 합니다. 클라이언트는 사용자 인터페이스와 사용자 상호작용에 집중하고, 서버는 데이터 저장, 처리, 보안 등의 기능을 처리합니다. 
    
2. 상태 없음 (Statelessness)  
    서버는 각각의 요청(request)을 완전히 별개의 것으로 인식하고 처리해야 하며, 클라이언트의 상태를 서버에서 유지하지 않아야 합니다.
    
3. 캐시 가능 (Cacheable)  
    요청에 대한 서버의 응답에 데이터가 캐시 가능한지 불가능한지 여부가 명시되어 있어야 합니다. 보통 HTTP Header에 cache-control 헤더를 이용합니다.
    
4. 계층화 (Layered System)  
    서버는 중간 계층 프록시 서버나 캐시 서버 등을 이용하여 확장성과 보안성을 개선할 수 있습니다.
    
5. 통합 인터페이스 (Uniform Interface)  
    RESTful 시스템은 통일된 인터페이스를 갖추어야 합니다. 이는 리소스 식별을 위한 고유한 URI, 리소스 조작을 위한 표준 HTTP 메서드(GET, POST, PUT, DELETE 등) 등의 원칙을 따릅니다. 이를 통해 클라이언트와 서버 간의 상호작용이 단순하고 일관성 있게 유지됩니다.
   
---

## Database

### NoSQL과 RDBMS의 차이점을 설명해주세요.

**RDBMS**는 관계형 데이터베이스 관리 시스템을 의미합니다. 다른 테이블과 관계를 맺고 모여있는 집합체로 이해할 수 있습니다. 이러한 관계를 나타내기 위해 **외래 키(foreign key)** 를 사용한 테이블 간 Join이 가능하다는게 RDBMS의 가장 큰 특징입니다. 정해진 스키마에 따라 데이터를 저장하여야 하므로 **정확한 데이터 구조를 보장**하는 장점이 있습니다. 반면에 테이블간 관계를 맺고 있어 시스템이 커질 경우 JOIN문이 많은 복잡한 쿼리가 만들어질 수 있고 스키마로 인해 데이터가 유연하지 못해 나중에 스키마가 변경 될 경우 번거롭고 어렵다는 단점이 있습니다.  
**NoSQL**은 RDBMS와는 달리 테이블 간 관계를 정의하지 않습니다. 데이터 테이블은 그냥 하나의 테이블이며 따라서 일반적으로 테이블 간 Join도 불가능합니다. 빅테이터의 등장으로 인해 데이터와 트래픽이 기하급수적으로 증가함에 따라 RDBMS에 단점인 성능을 향상시키기 위해 등장했고 데이터 일관성은 포기하되 비용을 고려하여 여러 대의 데이터에 분산하여 저장하는 Scale-Out을 목표로 등장했습니다. **스키마가 없기 때문에 유연하며 자유로운 데이터 구조**를 가질 수 있고 언지든 저장된 데이터를 조정하고 새로운 필드를 추가할 수 있다는 장점이 있습니다. 반면에, 스키마가 존재하지 않기에 명확한 데이터 구조를 보장하지 않으며 데이터 구조 결정하기가 어려울 수 있다는 단점이 있습니다.

</br>

### 데이터베이스를 설계할 때 가장 중요한 것이 무엇이라고 생각하나요?  

**데이터베이스의 목적과 사용자 요구 사항을 잘 이해하고 이를 바탕으로 효율적이고 일관된 데이터 모델을 설계**하는 것입니다. 따라서 데이터베이스를 설계할 때는 어떤 데이터가 어떻게 사용되는지, 어떤 종류의 데이터가 필요한지, 어떤 연산이 자주 수행되는지 등을 고려해야 합니다.
또한 데이터베이스의 설계는 **데이터의 일관성, 무결성, 보안** 등을 보장해야 합니다. 이를 위해, **테이블 간의 관계**를 잘 설정하고, **적절한 제약 조건**을 설정하여 **데이터의 무결성**을 유지하고, 보안 측면에서도 **적절한 접근 권한 설정**을 고려해 주어야 합니다.
마지막으로 데이터베이스는 변화하는 요구 사항에 대응할 수 있도록 설계되어야 합니다. 이를 위해 유지보수 및 확장성을 고려하여 데이터베이스를 설계해야 합니다.

</br>

### 최소 신장 트리에 대해서 설명해 주세요.

신장트리는 **최소한의 간선으로 그래프 내의 모든 정점을 포함하도록 하는 트리**입니다. 최소 신장 트리란 spanning tree 중 간선의 **weight의 합이 최소가 되는 트리**를 말합니다. 사이클이 존재해서는 안되며 최단 경로를 보장하지 않는다는 특징이 있습니다. 통신망, 유통망 등 **길이, 구축 비용, 전송 시간 등을 최소로 구축하는 경우에 사용**할 수 있으며 **Kruskal 등의 알고리즘을 이용해 구현**할 수 있습니다.


</br>

### 데이터 베이스에서 인덱스(색인)이란?

데이터베이스에서 인덱스란 테이블에 대한 검색 속도를 향상시켜주는 자료구조로, 테이블의 컬럼 값과 해당 레코드의 물리적인 주소를 매핑하는 것입니다. 이 매핑 정보를 이용하여 데이터베이스에서 특정 값을 빠르게 찾을 수 있습니다. 이러한 인덱스는 Btree, B+tree, Hash, Bitmap로 구현됩니다. 데이터를 추가/변경/삭제할 경우 이에 대응하는 인덱스를 수정해야 하기 때문에 데이터베이스의 성능에 많은 부담을 줍니다.


</br>

### 데이터 베이스에서 정규화는 무엇인가요?

**정규화**는 데이터베이스 설계에서 **중복을 최소화하고 데이터를 구조화하는 과정**으로, 데이터의 무결성과 일관성을 유지하며 데이터 저장 및 검색 효율성을 향상시키기 위해 수행됩니다.  
1차 정규화부터 5차 정규화까지 존재하며, 각 정규화 단계마다 특정한 조건을 만족시켜야 합니다. 이때 정규화를 수행하게 될수록 제약이 많아지며, 보다 데이터의 무결성과 일관성을 유지할 수 있습니다.  
이러한 정규화는 3차까지 수행하면 대부분의 중복과 종속성 문제를 해결할 수 있기 때문에, **일반적으로 3차 정규화이나 BCNF(Boyce-Codd; 보이스-코드 정규화)까지 수행**합니다. 더 높은 정규화를 적용하면 데이터베이스의 구조가 복잡해져서 유지보수가 어려워지고, 새로운 요구사항에 대응하기도 어려워질 수 있기 때문입니다.

</br>

### 반정규화에 대해 설명해주세요.

반정규화(Denormalization)란 정규화된 엔터티, 속성, 관계에 대해 **시스템의 성능향상과 개발과 운영의 단순화를 위해 중복, 통합, 분리 등을 수행하는 데이터 모델링의 기법**을 말합니다. 반정규화를 할 경우 **조회(select) 속도를 향상시키지만, 데이터 모델의 유연성은 낮아**집니다.  
데이터를 조회할 때 디스크 I/O량이 많아서 성능이 저하되거나 경로가 너무 멀어 조인으로 인한 성능저하가 예상되거나 칼럼을 계산하여 읽을 때 성능이 저하될 것이 예상되는 경우 반정규화를 수행하게 됩니다.

</br>

### 왜 인덱스에서 대부분 B+tree를 사용하나요?

B+tree는 오른쪽 서브트리의 높이와 왼쪽 서브 트리의 높이 차이가 1 이하인 균형 이진트리로 구성되며, 순서대로 정렬된 데이터를 저장합니다.  
B+tree는 모든 데이터가 leaf 노드에 저장된다는 특징이 있습니다. 이때, leaf 노드는 linked list로 연결되어 있어서 범위 쿼리 (range query)를 수행하는 데 효율적입니다. 또한 중복 키를 허용한다는 특징이 있습니다.

> - https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html
> - https://www.cs.usfca.edu/~galles/visualization/BTree.html
![](https://velog.velcdn.com/images%2Fwoo00oo%2Fpost%2F0a962d7d-a9bd-414f-a2df-559e6d225f76%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-25%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%206.53.43.png)

</br>

### 트랜잭션(Transaction)은 무엇인가요?

트랜잭션이란 SELECT, INSERT, UPDATE, DELETE 등과 같은 행동을 통해 데이터베이스의 상태를 변화시키기 위해 수행하는 **작업의 논리적 단위**입니다. 트랜잭션에는 4가지 특징이 있으며, 특징으로 원자성 (Atomicity), 일관성 (Consistency), 독립성 (Isolation), 지속성 (Durability) 있습니다.

> - 원자성: 트랜잭션이 데이터베이스에 모두 반영되던가, 아니면 전혀 반영되지 않아야 한다는 것
> - 일관성 : 트랜잭션의 작업 처리 결과가 항상 일관성이 있어야 한다는 것
> - 독립성 : 둘 이상의 트랜잭션이 동시에 실행되고 있을 경우 어떤 하나의 트랜잭션이라도, 다른 트랜잭션의 연산에 끼어들 수 없어야 한다는 것
> - 지속성 : 트랜잭션이 성공적으로 완료됬을 경우, 결과는 영구적으로 반영되어야 한다는 점

</br>

### RDB의 char와 varchar의 차이는 무엇일까요?

**CHAR**는 고정 길이 문자열 타입입니다. 즉, CHAR 데이터 유형으로 선언된 열은 항상 동일한 길이를 갖습니다. 검색속도 및 읽히는 속도가 VARCHAR에 비해 빠르다는 장점이 있습니다. 

**VARCHAR**는 가변 길이 문자열 타입입니다. 즉, VARCHAR 데이터 유형으로 선언된 열은 최대 길이가 지정되지만 저장되는 실제 문자열의 길이에 따라 공간이 유동적으로 조정됩니다. 저장 측면에서 CHAR 유형보다 작은 영역에 저장할 수 있다는 장점이 있습니다.

</br>

### redis가 무엇인가요?
Redis(Remote Dictionary Server)는 고성능 키-값 저장소로서 String, list, hash, set, sorted set 등의 자료 구조를 지원하는 NoSQL입니다. 디스크가 아닌 메모리에 데이터를 저장하므로 매우 빠른 데이터 액세스 속도를 제공합니다. Redis는 높은 성능과 다양한 데이터 구조 지원으로 인해 웹 애플리케이션, 캐싱 시스템, 메시징 시스템 등 다양한 분야에서 활용되고 있습니다.

</br>

### redis의 특징이 무엇인가요?
Redis는 메모리 기반 데이터 저장소로 작동하기 때문에 매우 빠른 응답 시간을 제공합니다. 또한 Redis는 문자열, 리스트, 해시, 셋 등 다양한 데이터 구조를 지원합니다. 이러한 Redis는 마스터-슬레이브 구조를 통해 데이터의 복제와 분산을 지원하여, 고가용성과 확장성을 제공할 수 있으며, 다양한 프로그래밍 언어에 대한 클라이언트 라이브러리를 제공하여, 애플리케이션과의 연동이 용이하다는 특징이 있습니다. 싱글 스레드 방식이어서 연산을 하나씩 처리하기에 thread safe하고, key-value방식이기에 쿼리없이 결과를 얻을 수 있어 속도가 빠릅니다 
* 마스터-슬레이브 구조란? 장치나 프로세스(마스터)가 하나 이상의 다른 장치나 프로세스(슬레이브)를 통제하고 통신 허브 역할을 하는 비대칭 통신 및 제어 모델입니다.
  
</br>

###  redis와 mongdb의 차이점을 설명해주세요.

Redis는 메모리 기반 데이터베이스이기 때문에 MongoDB보다 속도가 빠릅니다. 또한 Redis는 Key-Value 구조를 기반으로 하여 데이터를 빠르게 저장하고 검색할 수 있습니다. 반면 MongoDB는 JSON 형식의 문서 기반으로 유연하고 복잡한 데이터 구조를 지원하며, 디스크 기반의 데이터베이스 시스템으로 작동하여 대용량 데이터 저장 및 복잡한 쿼리 처리를 지원합니다. 따라서 Redis는 캐싱, 세션 관리와 같은 실시간 애플리케이션에 적합하며, MongoDB는 웹 애플리케이션, 빅 데이터 분석에 적합합니다.

</br>

### DB에서 View(가상 테이블)은 무엇인가요?
View는 DB에서 하나 이상의 테이블로부터 유도된 가상 테이블로, 기존 테이블을 기반으로 새로운 가상 테이블을 생성하여 실제 데이터를 변경하지 않으면서 다양한 방식으로 데이터를 가공하고 조회할 수 있는 유용한 도구입니다. 이때 가상 테이블은 실제 데이터를 저장하는 것이 아닌 SQL을 저장하여 사용하는 테이블입니다. 이러한 view는 항상 최신의 데이터를 조회할 수 있도록 보장하기 위해, 조회될 때마다 기존 테이블에 대한 링크를 생성하여 데이터를 가져오는 동적인 링크를 유지합니다. 이러한 동적인 링크가 있기 때문에 뷰에서 insert, update, delete와 같은 DML 작업을 하게 되면 기존 테이블에도 영향을 끼치게 됩니다.

### 기본키(primary key)에 대해 설명해주세요.
레코드를 유일(unique)하게 식별할 수 있는 컬럼입니다. 기본키는 최소성과 유일성을 만족하는 후보키 중에 안전성과 의미론적 일관성의 측면에서 제일 적합한 것을 선정한 키입니다. 유일성은 해당 속성집합이 테이블 내에서 중복된 값이 없다는 것이고, 최소성은 해당 속성집합이 테이블 내에서 행을 식별하는 집합 중 가장 최소화된 키들로 구성된 집합이라는 것 입니다. Primary Key가 있으면 데이터 일관성을 유지할 수 있고 레코드 검색에서 빠르게 원하는 데이터를 찾을 수 있습니다.

</br>


### 인덱스는 크게 Hash 인덱스와 B-Tree 인덱스가 있습니다. 이것은 무엇일까요?
**B-Tree 인덱스**
- equal(=)뿐만 아니라 >, >=, <, <= 또는 between 연산자 사용 가능
- B-Tree는 항상 정렬된 상태를 유지하기 때문에 조회 속도는 빠르지만 삽입과 수정, 제거 작업은 느리다.
> - B-Tree는 정렬된 트리이므로 탐색이 $O(logN)$의 시간 복잡도를 갖는다.
  
**Hash 인덱스**
- 동등 비교 검색에는 최적화돼 있지만 범위를 검색하거나 정렬된 결과를 가져오지는 못한다.
- DBMS에서 해시 인덱스는 메모리 기반의 테이블에 주로 구현되어 있으며 디스크 기반의 데용량 테이블로용으로는 거의 사용되지 않는다.
- 정렬할 필요가 없으니 삽입/삭제가 빠를 수 있다.


## #Operation System 
### 페이지 교체 알고리즘과 어떤 종류들이 있는지 설명해주세요.

페이징 기법으로 메모리를 관리하는 운영체제에서 필요한 페이지가 주기억장치에 적재되지 않았을 시 어떤 페이지 프레임을 선택해 교체할 것인지 결정하는 방법을 페이지 교체 알고리즘이라고 합니다. 
종류로는 OPT, FIFO, LRU, LFU 등이 있습니다.
> - OPT - Optimal: 앞으로 가장 오랫동안 사용되지 않을 페이지 교체
> - FIFO - First In First Out: 가장 먼저 들어온 페이지를 교체
> - LRU - Least Recently Used: 가장 오랫동안 사용되지 않은 페이지를 교체
> - LFU - Least Frequently Used: 참조 횟수가 가장 적은 페이지 교체
> - MFU - Most Frequently Used: 참조 횟수가 가장 많은 페이지 교체
> - NUR - Not Used Recently: 최근에 사용하지 않은 페이지 교체
> - SCR - Second Chance Replacement: FIFO에서 한 번 더 기회를 주고 교체
> - 클럭 알고리즘: SCR과 동일



### 프로세스와 스레드의 차이(Process vs Thread)를 알려주세요.

프로세스는 프로그램을 실행해 운영체제로부터 자원을 할당받은 **작업의 단위**이고, 스레드는 프로세스가 할당받은 자원을 이용하는 **실행 흐름의 단위**입니다. 스레드는 프로세스 내 여러개 생길 수 있으며, **자원 공유가 가능**합니다.

> 프로세스는 운영체제로부터 시스템 자원을 할당받는 작업의 단위로 **메모리에 올라와 실행되고 있는 프로그램의 인스턴스**를 의미합니다. 프로세스는 각각 독립된 메모리 영역(**Code, Data, Stack, Heap**의 구조)을 할당받습니다.  
스레드는 프로세스가 할당받은 자원을 이용하는 **실행의 단위**로 프로세스와는 다른 **더 작은 실행 단위 개념**입니다. 스레드는 프로세스의 코드에 정의된 절차에 따라 실행되는 특정한 수행 경로입니다. 이러한 스레드는 컴퓨터의 중앙 처리 장치(CPU)의 서로 다른 코어에서 프로그램의 여러 부분을 동시에 실행할 수 있기 때문에 병렬 처리를 위해 사용될 수 있습니다.

<br/>

### 멀티 프로세스와 멀티 스레드를 사용하는 이유를 각각 설명해주세요.

멀티 프로세스는 안정성이 높고 병렬 처리에 특화되어 있으며, 멀티 스레드는 빠른 속도와 경제성을 보유하고 있습니다.

> 멀티 프로세스는 **안정성이 높아** 하나의 프로세스가 잘못돼도 다른 프로세스는 작동하지만, **context switching** 비용이 발생합니다.  
멀티 스레드는 시스템 자원 소모 감소, 처리비용 감소, 스레드간 **자원공유** 등의 장점이 있지만 디버깅이 어렵고, **동기화 이슈발생**, 하나의 스레드의 오류로 전체 프로세스에 문제가 생길 수 있다는 단점이 있습니다.

> **멀티 프로세스는** 하나의 응용프로그램을 **여러 개의 프로세스에서 동시에 실행하여 다중 작업(multi-tasking)을 지원**하는 방법입니다. 각 프로세스는 별도의 메모리 공간을 할당받으므로 안정성이 높고, 하나의 프로세스에 문제가 생겨도 다른 프로세스는 정상적으로 동작한다는 장점이 있습니다.  
**멀티 스레드는** 하나의 응용프로그램을 **하나의 프로세스 안에서 여러 개의 스레드를 동시에 실행하여 다중 작업을 지원**하는 방법입니다. 스레드 간 메모리를 공유하므로 자원을 효율적으로 관리할 수 있지만, 동시에 메모리에 접근하는 경우 데이터 불일치 문제가 발생할 수 있습니다.   
멀티 프로세스와 멀티 스레드 모두 여러 작업을 동시에 처리하여 성능을 향상시킬 수 있습니다.

> 멀티 프로세스는 두 개 이상 다수의 프로세서가 협력적으로 하나 이상의 작업을 동시에 처리하는 것으로 각 프로세스 간 메모리 구분이 필요하거나 독립된 주소 공간을 가져야 할 경우 사용한다. 독립된 구조로 안전성이 높은 장점을 가지고 있지만 작업량이 많을 수록 오버헤드가 발생하여 성능 저하가 발생할 수 있다는 단점을 가지고 있다.  
멀티 스레드는 하나의 프로세스를 여러 스레드로 자원을 공유하며 작업을 나누어 수행하는 것이다. 시스템 자원 소무를 감소하고 자원을 효율적으로 관리할 수 있지만, 병목현상, 데드락 등 자원을 공유하기에 동기화 문제가 발생할 수 있다. 또한 하나의 스레드에 문제가 생기면 전체 프로세스가 영향을 받는다.

<br/>

### 동기와 비동기의 차이는 무엇인가요?

동기는 **제어권의 반환과 결과값**이 전달하게 되는 시간이 일치하게 되는 것이고,
비동기는 제어권의 반환과 결과값의 전달 시간이 일치하지 않을 수 있는 것입니다.  
> 동기는 데이터의 요청과 결과가 한 자리에서 동시에 일어나기 때문에 **설계가 매우 간단하고 직관적**이지만 결과가 주어질 때까지 아무것도 못하고 대기해야 한다는 단점이 있습니다.  
비동기는 요청에 따른 결과가 반환되는 시간 동안 **다른 작업을 수행할 수 있지만, 동기식보다 설계가 복잡**합니다.

<br/>

### 뮤텍스와 세마포어의 차이를 설명해주세요.

뮤텍스와 세마포어의 가장 큰 차이점은 **동기화 대상의 개수**입니다. 뮤텍스는 동기화 대상이 1개일 때 사용하지만 세마포어는 동기화 대상이 1개 이상일 때 사용합니다. 세모포어는 뮤텍스가 될 수 있지만, 뮤텍스는 세마포어가 될 수 없습니다. 뮤텍스는 자원 소유가 가능하지만 세마포어는 자원 소유가 불가합니다. 또한 뮤텍스는 소유하고 있는 스레드만이 뮤텍스를 해제할 수 있는 반면, 세마포어는 세마포어가 소유하지 않는 스레드가 세마포어를 해제할 수 있습니다.

> **뮤텍스**는 한 **쓰레드, 프로세스에 의해 소유될 수 있는 Key를 기반으로 한 상호배제기법**입니다. Locking 메커니즘으로 오직 하나의 쓰레드만이 동일한 시점에 뮤텍스를 얻어 임계 영역(Critical Section)에 들어올 수 있습니다. 그리고 오직 이 쓰레드만이 임계 영역에서 나갈 때 뮤텍스를 해제할 수 있습니다.  
**세마포어**는  **현재 공유자원에 접근할 수 있는 쓰레드, 프로세스의 수를 나타내는 값을 두어 상호배제를 달성**하는 기법입니다. wait를 호출하면 세마포어의 카운트를 1줄이고, 세마포어의 카운트가 0보다 작거나 같아질 경우에 락이 실행됩니다. Signaling 메커니즘으로 락을 걸지 않은 쓰레드도 signal을 사용해 락을 해제할 수 있습니다. 세마포어의 카운트를 1로 설정하면 뮤텍스처럼 활용할 수 있습니다.

</br>

### 교착상태(데드락, Deadlock)의 개념과 조건을 설명해주세요.

데드락이란 **두 개 이상의 프로세스나 스레드가 서로 자원을 얻지 못해서 다음 처리를 하지 못하는 상태로 무한히 다음 자원을 기다리게 되는 상태**를 말합니다.  
데드락은 상호 배제(mutual exclusion), 점유 대기(hold and wait), 비선점(no preemition), 순환 대기(circular wait)의 4조건을 모두 만족해야 성립합니다.

> 교착상태가 발생하는 4가지 조건이 있는데 한번에 오직 한개의 작업만 자원에 접근할 수 있는 **상호배제**, 프로세스가 할당된 자원을 가진 생타에서 다른 자원을 기다리는 **점유대기**, 프로세스가 어떤 자원의 사용을 끝낼 때까지 그 자원을 뻇을 수 없는 **비선점**, 각 프로세스가 순환적으로 다음 프로세스가 요구하는 자원을 가지고 있는 **순환대기**가 있습니다. 이 조건 중에 한가지라도 만족하지 않으면, 교착상태는 발생하지 않습니다.
> 이중 순환대기 조건은 점유대기 조건과 비선점 조건을 만족해야 성립하는 조건이므로, 위 4가지 조건은 서로 완전히 독립적인 것은 아닙니다.

<br/>

### 스케줄러가 무엇이고, 단기/중기/장기로 나누는 기준에 대해 설명해주세요.

스케줄러란 **어떤 프로세스에게 자원을 할당할지를 결정하는 운영체제 커널의 모듈**을 지칭합니다.  
**단기 스케줄링**은 프로세서 스케줄러라고 부르며 메인 메모리의 준비상태에 있는 작 업 중에서 실행할 작업을 선택하고 프로세서를 배당하는 일을 합니다.  
**중기 스케줄링**은 현재 생성되어 있는 프로세스 중에 비효율적으로 시스템의 자원을 낭비 하고 있는 프로세스가 있을 경우 보조기억장치로 추방하는 스케줄링입니다.  
**장기 스케줄링**은 작업 스케줄러라고 부르기도 하며 어떤 작업이 시스템에 들어와서 스케 줄링 원칙에 따라 디스크 내의 어떤 작업을 어떤 순서로 메모리에 가져와서 처리할 것인 가를 결정하는 프로그램입니다.  
단기 스케줄러는 CPU와 메모리 사이의 스케줄링을 담당하고 장기 스케줄러는 메모리와 디스크 사이의 스케줄링을 담당합니다. 중기 스케줄러는 스와핑(Swapping)을 통해 프로세스들이 CPU경쟁이 심해지는 것을 방지하는 역할을 합니다.  
현재는 가상메모리 관리를 이용해 장기, 중기 스케줄러는 거의 쓰이지 않습니다.

</br>

### CPU 스케줄러인 FCFS, SJF, SRTF, RR, Priority Scheduling에 대해 간략히 설명해주세요.

CPU 스케줄링은 CPU를 사용중인 프로세스가 자율적으로 반납하는 비선점 스케줄링과 OS가 알고리즘에 따라 적당한 프로세스에게 CPU를 할당하고 필요시에는 회수하는 방식인 선점 스케줄링이 있습니다.  
비선점 스케줄링은 먼저 CPU를 요청하는 프로세스를 먼저 처리하는 **FCFS**(First Come First Served), 버스트 시간이 짧은 프로세스부터 처리하는 **SJF**(Shortest Job First)가 있습니다.    
선점 스케줄링에는 최단 잔여시간을 우선으로 하는 **SRT**(Shortest Remaining Time), 모든 프로세스가 같은 우선순위를 갖고 time slice를 기반으로 스케줄링하는 **RR**(Round Robin)이 있습니다.  
이 외에도 우선순위가 높은 프로세스에 CPU를 먼저 할당하는 **Priority Scheduling**이 있습니다.  
\* Burst time: 프로세스가 CPU를 사용하는 시간  

> `FCFS(First Come First Service)`는 Ready Queue에 먼저 도착한 작업부터 처리하는 알고리즘입니다. 이 방식은 실행 순서에 따라 평균대기시간 차이가 커지게 됩니다. 또한, 중요한 작업이 있다 하더라고 그 작업 보다 먼저 들어온 작업이 끝나기 전까지는 실행될 수 없다는 단점이 있습니다.  
`SJF(Shortest Job First)`는 다음에 실행될 작업 중 수행시간이 가장 짧은 작업을 먼저 처리하는 알고리즘 입니다. 이 방식은 실행시간을 예측한다는 점에서 비현실적이며 계속해서 짧은 프로세스만 처리하므로 긴 프로세스는 뒤로 밀린다는 단점이 있습니다.  
`SRTF(Shortest Remaining Time First)`는 남은 처리시간이 더 짧은 작업을 먼저 처리하는 알고리즘 입니다. 이 방식은 잔여 시간을 계속 추적해야 하므로 overhead가 크며, 구현 및 사용이 비현실적이라는 단점이 있습니다.  
`RR(Round Robin)`은 FCFS의 방식에서 자원 사용 시간(time quantum)을 제한하는 알고리즘입니다. 이 방식은 너무 잦은 context switching은 중앙처리장치 이용률을 저하할 수 있다는 단점이 있습니다.  
`Priority Scheduling`은 각 프로세스에 지정된 우선순위를 기준으로 높은 우선순위를 가진 작업을 먼저 처리합니다. 이 방식은 우선순위가 낮은 프로세스는 뒤로 밀리는 문제가 발생합니다. 이는 일정 시간을 기다리면 프로세스의 우선순위를 높여주는 aging 방식으로 해결할 수 있습니다.


</br>

### Context Switching이 무엇인지 설명해주세요.
Context Switching이란 여러 프로세스를 처리해야 하는 상황에서 현재 진행중인 Task(프로세스 혹은 쓰레드)의 상태를 PCB에 저장하고, 다음에 진행할 Task의 상태 값을 읽어 레지스터에 적재하는 과정을 말합니다.
CPU는 하나의 프로세스만 처리할 수 있습니다. 컴퓨터를 사용할 때 게임을 하면서 음악을 듣고 인터넷 검색을 하는 것은 마치 여러개를 동시에 수행하고 있는 것 같지만 CPU에서 인지하지 못할 정도로 빠르게 프로세스 사이를 왔다갔다 처리를 하면서 하니씩 처리합니다. 이 때, 일어나는 것이 Context Switching입니다. Context Switching 과정은 많은 비용이 들기 때문에, 가능한 최소한으로 발생하도록 설계하는 것이 중요합니다.

</br>

## 데드락의 해결방법에는 어떤 것이 있나요?

예방, 회피, 탐지 및 회복, 무시 방법이 있습니다.
- 예방 : 교착 상태가 발생하기 전에 미리 조치를 취하는 방식으로, 교착 상태 발생 조건 중 하나를 제거함으로써 해결합니다. 자원 낭비가 심하다는 단점이 있습니다.

- 회피 : 교착상태가 발생할 가능성이 있는 자원 할당(Unsafe allocation)을 하지 않고 안전한 상태(Safe state)에서만 자원 요청을 허용하는 방법입니다. 대표적인 예시로는 다익스트라의 은행원 알고리즘이 있습니다. 하지만 오버헤드가 많이 발생한다는 단점이 있습니다.

- 탐지 및 회복 : 자원 할당 그래프를 통해 데드락을 감지하며, 만약 데드락을 감지할 경우 이전 상태로 회복하는 방법입니다.

- 무시 : 교착상태가 많이 발생하지 않고, 해결 비용이 오히려 더 크기에 그냥 무시하는 방법입니다.

> 은행원 알고리즘
> - 운영체제는 안전상태를 유지할 수 있는 요구만을 수락하고 불안정 상태를 초래할 사용자의 요구는 나중에 만족될 수 있을 때까지 계속 거절하는 방법
>
> Safe state
> - safe sequence(교착상태를 발생시키지 않고 자원을 할당하는 순서)가 존재하며 모든 프로세스가 정상적으로 종료될 수 있는 상태를 의미

</br>

---

## Data Structure
### 배열(array)와 연결리스트(linked list)의 각각의 특징과 장단점을 설명해주세요.

Array는 **연속된 메모리 주소**를 할당받게 되는데 이때 index를 갖게됩니다. Array는 index를 가지고 임의 접근이 가능하고 접근과 탐색에 용이하다는 장점을 가지고 있지만 크기를 미리 정해놓았기 때문에 해당 배열 크기 이상의 데이터를 저장할 수 없다는 단점이 있습니다.  
Linked List는 동적 자료구조로 크기를 정할 필요가 없고 배열처럼 연속된 메모리 주소를 할당받지 않습니다. 대신 노드(Node) 안에 데이터가 있고, 다음 데이터를 가리키는 주소를 가지고 있습니다. Linked List는 **크기의 제한이 없으므로 데이터 추가, 삭제가 자유롭다**는 장점이 있지만, 메모리 주소를 할당받지 않았기 때문에 임의로 접근하는 것은 불가능하여 데이터를 탐색할 때 순차적으로 접근해야 한다는 단점이 있습니다.

> **배열은** 메모리상에서 연속적으로 저장되어 있기 때문에, index를 통한 접근이 용이합니다. 배열의 크기는 처음 생성할 때 정하며 이후에는 변경할 수 없다는 특징이 있습니다.  
**연결 리스트**는 여러 개의 노드들이 순차적으로 연결된 형태를 갖는 자료구조이며, 첫번째 노드를 헤드(Head), 마지막 노드를 테일(Tail) 이라고 하며, 이때 각 노드는 데이터와 다음 노드를 가리키는 포인터로 이루어져있습니다. 배열과는 다르게 메모리를 연속적으로 사용하지 않기 때문에 접근은 불리할 수 있으나, **크기의 제한이 없어, 데이터 추가, 삭제가 자유롭다는 특징이 있습니다.**

<br/>

### 캐시의 지역성에 대해 설명해주세요.

캐시 메모리는 적중률(Hit rate)을 극대화하기 위해 데이터 **지역성(Locality)**의 원리를 사용합니다. 지역성(Locality)이란 기억 장치 내의 정보를 균일하게 액세스하는 것이 아닌 **특정 부분을 집중적으로 참조하는 특성**으로, 프로그램이 소규모의 특정 데이터 및 명령어 집합에 반복적으로 액세스하는 경향이 있다는 사실을 의미합니다.  
최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성인 **시간 지역성**과 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성인 **공간 지역성**이 있습니다.

> 캐시 메모리는 CPU의 처리 속도와 메모리의 속도 차이로 인한 병목현상을 완화하기 위해 사용하는 고속 버퍼 메모리입니다. 주기억장치에 있는 데이터를 액세스하려면 비교적 오랜 시간이 걸리게 되는데 이를 줄이기 위해 데이터를 빠르게 액세스할 수 있도록 중간에 캐시 메모리를 사용합니다. 따라서 데이터 요청이 들어오면, 원본 데이터가 담긴 곳에 접근하기 전에 먼저 캐시 내부부터 찾는데, 이때 원하는 데이터가 캐시에 있는 경우에 대한 비율을 캐시 메모리 적중률이라고 합니다.
</br>


### 이진 트리의 시간 복잡도?

최악의 경우, 즉 편향된 이진 트리인 경우엔 시간 복잡도는 O(n)입니다.
그러나 잘 정렬된 이진 트리의 경우 트리높이에 비례하는 O(log n)의 복잡도를 가집니다.

</br>

### 이진 트리의 시간 복잡도가 O(logN)인 이유

이진 탐색 트리에서는 왼쪽 서브트리의 모든 노드 값이 현재 노드 값보다 작고, 오른쪽 서브트리의 모든 노드 값이 현재 노드 값보다 큰 특성을 가집니다. 이런 특성으로 인해 이진 탐색 트리에서의 탐색은 탐색 경로를 절반씩 제거하면서 진행할 수 있습니다. 따라서 이진 탐색 트리의 시간 복잡도는 O(log n)이 됩니다.  
그러나 편향된 이진트리의 경우에는 최악의 경우에 모든 노드를 탐색하게 됩니다. 따라서 최악의 경우에 O(n)의 시간복잡도가 나올 수 있습니다.


> 데이터 개수를 n, 트리의 높이를 d라고 할때, n = 2^d 이고, 이를 정리하면 d = log n 이 됩니다. 트리의 깊이가 곧 시간 복잡도이기에 시간 복잡도는 O(logN)이 됩니다. 


---
## Computer Science
### 0.1 + 1.1 = 1.2 가 false인 이유를 설명해주세요.

컴퓨터에서 실수를 저장할 때 정수부분과 실수부분을 나누어 앞과 뒤의 칸에 **2진법으로** 저장을 하게 되는데, 0.1단위 소숫점을 2진법으로 표현하면 무한히 길어지게 되어 **일정부분을 자르고 저장**을 하게 됩니다. 이때 잘린 만큼의 오차가 있기때문에 0.1+1.1=1.2 가 False 값으로 나오게 됩니다.  
파이썬에서는 `decimal` 모듈을 사용해 해결할 수 있습니다.

<br/>

## Python

### Python은 어떤 특징을 가진 언어인가요?

파이썬은 **interpreter language**라 실행 전에 컴파일할 필요가 없습니다. **동적 타이핑**이기 때문에 실행시간에 자료형을 검사하므로 자료형을 명시할 필요가 없습니다.

>  파이썬은 인터프리터 언어이므로, 실행하기 전에 컴파일을 할 필요가 없습니다. 그러나 컴파일러가 코드를 기계어로 번역해서 실행가능 파일을 만드는 것에 비해, 인터프리터는 코드를 한줄씩 실행할 때마다 번역해서 실행하기 때문에 다른 컴파일 언어에 비해 다소 느리다는 특징이 있습니다.  
또한, 파이썬은 클래스와 구성 및 상속을 함께 정의할 수 있는 **객체** **지향** **프로그래밍(OOP)**입니다**.**

</br>

### @classmethod, @staticmethod, @property은 어떤 것인가요?

@classmethod, @staticmethod, @property는 각각 클래스 메소드, 정적 메소드, 인스턴스 메서드의 동작을 수정하는 데 사용되는 데코레이터입니다.
@classmethod는 첫 번째 인자로 클래스 자신을 받는 메소드를 정의할 때 사용됩니다. 이를 사용하여 클래스 변수에 접근할 수 있고, 클래스 메소드를 통해 클래스 레벨에서의 연산을 수행할 수 있습니다.
@staticmethod는 클래스나 인스턴스와 무관하게 실행될 수 있는 메소드를 정의할 때 사용됩니다. 이를 사용하면, 해당 메소드를 객체를 생성하지 않고 클래스명을 사용하여 직접 호출할 수 있습니다.
@property는 getter 메소드를 간단하게 생성할 수 있게 합니다. 이를 사용하면 인스턴스에 직접 접근하는 대신 메소드를 통해 간접적으로 접근할 수 있습니다. 따라서 코드의 안정성과 유지보수성이 향상될 수 있습니다.  
\* 정적 메소드 ? 클래스에 속하면서 객체에 독립적으로 호출 가능한 메소드  
\* getter 메소드 ? 클래스의 인스턴스 변수 값을 반환하는 메소드 (보통 함수 이름 앞에 'get_'이라는 접두사를 붙인다.)

<br/>

### 얕은 복사(shallow copy)와 깊은 복사(deep copy)의 차이는 무엇인가요?

파이썬에서 얕은 복사(shallow copy)와 깊은 복사(deep copy)의 차이점은 **복사된 객체의 참조 방식**에 있습니다.  
**얕은 복사는 원본 객체의 참조를 복사**하여 새로운 객체를 만듭니다. 이 경우, 새로운 객체와 원본 객체는 같은 메모리 주소를 참조하게 됩니다. 따라서 새로운 객체를 변경하면 원본 객체도 변경됩니다.  
반면에 **깊은 복사는 원본 객체의 값을 복사하여 완전히 새로운 객체를 만듭니다**. 이 경우, 새로운 객체와 원본 객체는 서로 다른 메모리 주소를 참조하게 됩니다. 따라서 새로운 객체를 변경해도 원본 객체는 변경되지 않습니다.  
파이썬에서 얕은 복사는 copy 모듈의 copy 함수를 사용하여 수행할 수 있으며, 깊은 복사는 copy 모듈의 deepcopy 함수를 사용하여 수행할 수 있습니다.

</br>

### 파이썬에서 GIL(Global Interpreter Lock)에 대해 설명해주세요.
GIL이란 Global Interpreter Lock의 약자로 파이썬 인터프리터가 한 스레드만 하나의 바이트코드를 실행 시킬 수 있도록 해주는 Lock입니다. 하나의 스레드에 모든 자원을 허락하고 그 후에는 Lock을 걸어 다른 스레드는 실행할 수 없게 막아버립니다.
파이썬의 모든 객체는 해당 변수가 참조된 수를 저장하기 때문에 멀티스레드인 경우 여러 스레드가 하나의 객체를 사용한다면 reference count를 관리하기 위해 모든 객체에 대해 lock이 필요합니다. 이런 비효율을 막기위해 python에서 GIL을 사용합니다.

>파이썬에서 GIL은 프로세스가 **한 시점에 하나의 스레드에만 모든 자원을 할당하고 다른 스레드는 접근할 수 없게 막는** 역할을 합니다. 이러한 GIL은 파이썬의 메모리 관리를 보호하고 멀티 스레드 환경에서의 데드락(deadlock) 문제를 해결하지만, CPU를 더욱 효율적으로 사용할 수 있는 **멀티 코어 환경에서 파이썬 프로그램의 성능을 제한**하는 요인 중 하나입니다.   
따라서 파이썬에서의 멀티 쓰레드는 각 쓰레드를 병렬적으로 처리하지 못하고, 오히려 이 과정에서 Context Switching 때문에 **싱글 쓰레드보다 안 좋은 성능이 나올 수 있습니다.**  
이러한 GIL의 방해를 받지 않고 병렬 연산을 하기 위한 **가장 대표적인 방법은 멀티 프로세싱을 사용**하는 것입니다. **컨텍스트 스위칭 비용이 크다는 단점**이 있지만 프로세스는 각자 독자적인 메모리를 가지기에 GIL의 영향을 받지 않습니다.   
이외에 단일 스레드의 성능을 높여주는 비동기 처리를 하는 방법이 있습니다.

</br>

### 인터프리터 언어와 컴파일 언어의 차이점을 설명해주세요.
컴파일 언어는 소스코드를 한 번에 기계어로 변환하여 실행 파일을 만드는 방식이고 인터프리터 언어는 소스코드를 기계어로 변환하는 과정 없이 한줄씩 해석하여 바로 명령어를 실행하는 언어를 말합니다.
보통, 컴파일된 프로그램이 인터프리터를 통해 실행하는 것보다 더 빠르게 실행됩니다. 하지만, 인터프리터는 모든 명령에 대해 컴파일이 이뤄지지 않아도 중간에 결과를 확인할 수 있다는 장점이 있습니다.

</br>

### self는 무엇인가요?
self는 인스턴스 메서드(instance method; 멤버함수)의 첫 번째 인자입니다. 현재 객체(instance)를 가리키는 참조로, 해당 메서드가 호출된 객체 자체를 가리킵니다. 이 self 매개변수를 통해 메서드 내에서 클래스의 속성에 접근하고 수정할 수 있습니다.

</br>

### 멤버함수, 정적함수에 대해 설명해주세요.
인스턴스 메서드는 객체마다 각각 동작하는 함수입니다. 반면 정적 메서드는 self 를 매개변수로 받지 않는 함수로 모든 객체들에 공유되는 함수입니다. 정적 메서드 내에서는 인스턴스/클래스 속성에 접근하거나, 인스턴스/클래스 메서드를 호출하는 것이 불가능합니다. 따라서 일반적으로 인스턴스 메서드 호출은 `객체.함수()`로 되지만, 정적 메서드 호출은 `클래스이름.함수()`로 할 수 있습니다.


> 정적메서드를 지원하는 두 가지 방법이 있다. (@staticmethod, @classmethod)
> 
> [파이썬에서는 정적메소드임에도 불구하고 인스턴스에서도 접근이 가능하다.](https://hckcksrl.medium.com/python-%EC%A0%95%EC%A0%81%EB%A9%94%EC%86%8C%EB%93%9C-staticmethod-%EC%99%80-classmethod-6721b0977372)
> ex) 멤버 변수
> ![image](https://github.com/JisooRyu99/Tech_Interview/assets/90206705/d9978035-84e9-4a15-8466-7d2109d2c388)

> ex) 정적 함수
> ![image](https://github.com/JisooRyu99/Tech_Interview/assets/90206705/5ad3b8e5-04ce-4228-9931-8b6a7de071d2)


</br>

### Generator란 무엇인가요?
제너레이터(Generator)는 이터레이터(Iterator)를 생성하는 함수입니다. 이터레이터는 반복 가능한 객체이며, 순차적으로 요소를 반환할 수 있습니다. 제너레이터 함수는 일반 함수와 비슷하지만, return 대신 yield 문을 사용하여 값을 반환합니다. 이 yield 문은 함수 실행을 일시 중지하고 함수 상태를 유지한 채로 값을 반환하며, 다시 호출되면 일시 중지된 위치에서부터 실행을 재개합니다.  
이러한 제너레이터를 사용하면 매우 큰 데이터 집합을 메모리에 한 번에 로드하지 않고 필요한 시점에 값을 생성하고 처리할 수 있어 메모리를 효율적으로 사용할 수 있습니다.

</br>

### 파이썬에서 메모리는 어떻게 관리되나요?
파이썬에서 메모리 관리는 자동으로 이루어집니다. 파이썬은 고수준의 인터프리터 언어로서, 메모리 할당과 해제를 인터프리터가 메모리 관리를 자동으로 처리합니다.   
파이썬 인터프리터는 CPython이라는 구현체를 기반으로 동작하는데, CPython은 힙 메모리 관리를 수행합니다. 힙은 동적으로 할당된 객체들을 저장하는 메모리 영역으로, 가비지 컬렉션이 이 힙 메모리를 관리합니다. 가비지 컬렉션은 프로그램이 동적으로 할당한 메모리 중에서 더 이상 사용하지 않는 객체들을 자동으로 감지하고 해제하는 기능입니다. 이를 통해 개발자는 메모리 할당 및 해제에 대한 명시적인 관리를 신경 쓰지 않고도 프로그램을 작성할 수 있습니다.  
하지만, 대용량의 데이터 처리나 특정한 상황에서 메모리 사용을 최적화해야 할 경우, 리스트 컴프리헨션(List Comprehension), 제너레이터(Generator) 등을 활용하여 메모리 사용량을 줄일 수 있는 방법이 있습니다.

</br>

### 힙(heap)이 동적할당을 구현하는데 사용되는 이유가 무엇인가요?
힙을 사용하면 프로그램은 실행 중에 필요한 만큼의 메모리를 동적으로 할당할 수 있습니다. 정적으로 메모리를 할당하는 경우에는 프로그램 실행 중에 메모리 요구 사항이 변경될 수 있으므로 제한적일 수 있습니다. 힙을 사용하면 필요한 메모리 양을 동적으로 조정할 수 있으므로, 프로그램의 유연성과 확장성을 향상시킬 수 있습니다.  
또한 힙을 사용하면 동적인 데이터 구조를 효율적으로 지원할 수 있습니다. 예를 들어, 힙을 사용하여 가장 우선순위가 높은 요소를 추출할 수 있는 우선순위 큐(priority queue)를 구현할 수 있습니다. 이를 통해 필요한 메모리를 할당하는 성능을 향상시킬 수 있습니다.

</br>

### Garbage Collector(GC)가 무엇인가요?
**Garbage Collector(GC)**는 프로그래밍 언어나 런타임 환경에서 사용되는 메모리 관리 기법 중 하나입니다. Garbage Collector는 동적으로 할당된 메모리 영역 중에서 더 이상 사용되지 않는 객체들을 감지하고 해제하는 역할을 합니다.  
이러한 가비지 컬렉터는 **참조 카운트(reference count) 방식**과 **세대(generation) 기반 방식**이 있습니다.  
참조 카운트(reference count) 방식은 객체가 참조되는 횟수를 추적하여, 해당 객체가 더 이상 참조되지 않을 때 메모리를 해제합니다. 하지만, 객체가 자기 자신을 가르키거나 서로를 참조하는 순환 참조 같은 상황에서는 참조 카운팅만으로는 메모리 누수가 발생할 수 있습니다.   
세대(generation) 기반 방식은 객체를 세대로 분류하고, 가장 빈번한 가비지 컬렉션을 수행하는 0세대부터 시작합니다. 가비지 컬렉션 주기마다 객체의 세대가 증가하며, 높은 세대로 갈수록 가비지 컬렉션 주기가 증가합니다. 이는 오래된 객체는 참조되지 않는 경우가 많아 자주 가비지 수집을 수행하지 않아도 되기 때문입니다. 이를 통해 시스템의 전체적인 성능을 향상시키고 가비지 컬렉션에 소요되는 시간을 최소화할 수 있습니다.  
따라서 파이썬의 GC는 참조 카운트와 세대 기반 가비지 수집을 조합하여 객체의 수명을 관리하고 메모리를 최적화하는 전체적인 메모리 관리 메커니즘을 제공합니다.

</br>

### JSON과 Dictionary의 차이는 무엇인가요?
JSON(JavaScript Object Notation)은 텍스트 기반의 데이터 교환 형식이며, 딕셔너리는 파이썬의 내장 자료구조로 메모리에 직접 구성됩니다. JSON은 주로 데이터 교환을 위해 사용되고, 딕셔너리는 파이썬 프로그램에서 데이터 구조화 및 처리에 사용됩니다. json의 KEY는 항상 문자열이여야 하지만 dictionary의 KEY는 특정 데이터 타입에 구애 안받는다는 차이점도 있습니다. 파이썬에서는 json 모듈을 사용하여 JSON 데이터를 처리하고 딕셔너리와의 상호 변환을 수행할 수 있습니다.


</br>

## *args / **kwargs 는 각각 무엇인가요?

함수의 인자를 몇 개 받을 지 모르는 경우에 사용하는 가변인자를 위한 변수
*args : 인자의 값이 튜플 형태로 저장
**kargs : 인자의 값이 딕셔너리 형태로 저장

> 예를 들어, 다양한 인자의 합을 계산하는 함수를 정의할 때 args를 사용하여 임의의 개수의 인자를 받을 수 있고, kwargs를 사용하여 임의의 개수의 키워드 인자를 받을 수 있습니다.

```python
def my_function(*args):
    for arg in args:
        print(arg)

my_function(1, 2, 3)

def my_function(**kwargs):
    for key, value in kwargs.items():
        print(key, value)

my_function(name="John", age=30)
```
</br>


---

## Data analysis
### 히스토그램의 가장 큰 문제는 무엇인가요?

히스토그램은 분석가의 주관에 따라 데이터를 구간별로 나누어 표현하기 때문에, 데이터의 분포가 왜곡될 수 있습니다. 따라서 적절한 구간 선택은 데이터를 올바르게 해석하기 위해 중요합니다. 또한, 히스토그램은 데이터를 구간별로 나누어 표현하기 때문에, 데이터에 이상치가 존재하더라도 이를 명확하게 식별하기는 어렵다는 문제가 있습니다.

> 히스토그램과 막대그래프의 차이점:
히스토그램은 연속형 데이터에 사용되는 반면, 막대 차트는 범주형 또는 명목형 데이터에 사용됩니다.  그렇기에 시각화한 형태에서도 히스토그램은 막대 사이 틈이 없고, 막대그래프는 틈이 존재합니다.

https://m.blog.naver.com/libido1014/120113775017

https://www.jmp.com/ko_kr/statistics-knowledge-portal/exploratory-data-analysis/histogram.html

</br>


### 좋은 feature란 무엇인가요. 이 feature의 성능을 판단하기 위한 방법에는 어떤 것이 있나요?

지도학습의 경우 좋은 피쳐란 y와 상관성이 높은 특징입니다. 즉 label의 엔트로피를 줄일 수 있는 피쳐입니다. 
이러한 피쳐는 트리모형에서 엔트로피 감소에 주요한 역할을 한 피쳐를 찾아 feature importance를 보는 방법, label과 상관관계를 보는 방법, 통계적 검정을 사용하여 feature의 유의성을 평가하는 방법 등이 있습니다.  
비지도 학습의 경우, 정보를 잘 분리해야하기에 분산이 큰 피쳐가 좋은 피쳐라고 할 수 있습니다. 

</br>

### 히스토그램과 막대그래프의 차이점을 설명해주세요
막대그래프는 범주로 구분되는 데이터를 표현하는데 사용하므로 막대로 표현하려는 범주의 순서는 의도에 따라 바뀔 수 있으며 막대 간에는 일정한 간격을 유지합니다. 반면에 히스토그램은 측정된 연속적인 값으로 표시되는 데이터를 표현하는데 사용하므로 막대그래프와 같이 막대의 순서를 임의적으로 바꿀 수 없으며, 막대 간의 간격 없이 표현합니다. 히스토그램은 데이터의 분포를 보여주는 데 더 적합하고, 막대 그래프는 데이터 간의 차이를 보여주는 데 더 적합합니다.
</br>

### "상관관계는 인과관계를 의미하지 않는다"라는 말이 있습니다. 설명해주실 수 있나요?
상관관계는 변수들이 함께 감소하거나 증가하는 것이고 인과관계는 앞선 변인이 후행되는 변인의 원인이 되는 것입니다. 
즉 상관관계가 있다고 해서 인과관계가 있는 것은 아닙니다. 예를 들어 아이스크림 판매량과 익사사고수가 양의 상관관계를 가지더라도 한변수가 다른 변수의 원인이 되진 않습니다. 더운 날씨일수록 아이스크림을 많이 먹고, 수영을 하기에 익사사고가 늘어나는 것이기 때문입니다. 
> 예시
> - 키와 몸무게는 일정한 상관관계가 있지만 키가 크다고 반드시 몸무게가 많이 나간다고 할 수 없기 때문에 상관관계가 있다고 인과관계가 있다고 할 수 없다.
</br>

