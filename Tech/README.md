- [#Statistics/Math](#statistics-math)
- [#Machine Learning](#Machine-Learning)
- [#Deep Learning](#deep-learning)
- [#Network](#Network)
- [#Database](#Database)
- [#Operating System](#operating-system)
- [#Data Structure](#data-structure)
- [#Computer Science](#computer-science)
- [#Python](#Python)
- [#Database](#Database)

## #Statistics/Math
## #1
### “likelihood”와 “probability”의 차이는 무엇일까요?

Probability는 어떤 trial에서 특정 sample이 나올 가능성을 말합니다. 즉, **시행 전 모든 경우의 수의 가능성은 정해져있고**, 그 총합은 1입니다.  
Likelihood는 **trial을 충분히 수행한 후 그 sample을 토대로 경우의 수의 가능성을 도출**하는 것입니다. 아무리 충분히 수행해도 어디까지나 추론inference이기 때문에 가능성의 합이 1이 되지 않을 수 있습니다.

> 확률은 어떤 시행에서 특정 결과가 나올 가능성을 말하며 총합은 1이다. 반면에 가능도는 실재가 바탕이 되어야 하고 어떤 시행을 충분히 수행한 뒤 그 결과를 토대로 경우의 수의 가능성을 도출하는 것이다. 어디까지나 추론이기 때문에 가능성의 합이 1이 되지 않을 수도 있다.

> 확률(Probability)은 관측값 또는 관측 구간이 **주어진 확률분포** 안에서 얼마나 나타날 수 있는가에 대한 값입니다. 즉, **특정 사건이 일어날 가능성**을 수치화한 것입니다.
반면, 가능도(Likelihood)은 어떤 특정한 값을 관측할 때, 이 관측치가 어떠한 확률분포에서 나왔는가에 관한 값입니다. 즉, 특정 모델이 특정 데이터 세트에 적합한지 측정하는 척도라고 할 수 있습니다.

<br/>

## #2
### 중심극한정리는 왜 유용한걸까요?


**중심극한정리**란 모집단이 정규분포를 따르지 않는 표본이더라도, **표본들의 수가 30개 이상이면 근사적으로 정규분포를 따른다는 이론**입니다.  
중심극한정리가 유용한 이유는 **모집단의 형태에 관계없이 모분산을 모르는 경우에도,** 표본의 수 n이 30 이상인 경우 **표본 평균의 분포가 정규분포를 따르기 가정**할 수 있기 때문입니다.


<br/>

## #3
### 아웃라이어의 판단하는 기준은 무엇인가요?

아웃라이어를 판단하는 기준은 여러 가지가 있을 수 있습니다.

통계학적으로는 임계값을 설정하고, Z-score이 이 값보다 크다면 이상치로 판단하는 방법과, IQR(Inter Quantile Range) 기법으로 데이터를 오름차순으로 정렬했을 때, **IQR는 75%의 지점 – 25%의 지점**인데, 이 “**75%의 지점 + IQR * 1.5**”의 이상이거나 “**25%의 지점 - IQR * 1.5**”의 이하인 데이터들을 이상치라고 판단하는 방법이 있습니다.  
또는 **도메인 지식을 이용**하여, 데이터가 수집된 분야의 전문적인 지식을 바탕으로 아웃라이어를 판단하거나, **EDA**를 통해 데이터에서 동떨어진 극단값을 판별할 수 있습니다.

<br/>

## #4
### 모집단과 표본의 차이는 무엇인가요?

모집단은 통계적 관찰의 대상이 되는 **집단 전체**를 말합니다. 표본은 모집단 중 **관측된 부분집합**을 말합니다.

<br/>

## #1
### 공분산과 상관계수는 무엇일까요?

공분산과 상관계수는 **두 변수 간의 관련성**을 측정하는 데 사용되는 통계적 개념입니다. **공분산**은 두 변수의 변화량이 함께 얼마나 많이 변하는지를 나타내는 값으로,  변수 간에 **양의 상관관계가 있는지, 음의 상관관계**가 있는지를 알 수 있습니다. **상관계수**는 공분산을 **각 변수의 표준편차로 나누어, 각 변수의 척도에 대한 영향을 제거**한 값입니다. **1과 1 사이의 값**을 가지며, 1은 완전한 양의 상관관계, -1은 완전한 음의 상관관계를 나타내며, 0은 상관관계가 없음을 나타냅니다.

</br>

## #2
### 조건부 확률은 무엇일까요?

**조건부 확률**은 **하나의 사건이 일어났을 때, 다른 사건이 일어날 확률**입니다.

> 사건 A가 일어났다는 전제 하에 사건 B가 일어날 확률은 사건 A와 B가 동시에 일어날 확률에 사건 A가 일어날 확률을 나누어 준 값입니다.

</br>

## #3
### Missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?

결측값이 많지 않은 경우에는 제거해주면 되지만, 결측값이 많거나, 데이터가 적은 경우에는 이를 **적절한 값으로 채워 가능한 데이터의 양을 확보**해야 합니다. 이를 통해 모델이 보다 정확한 예측을 하도록 할 수 있습니다. 결측값은 **0이나 평균, 중앙값, 최빈값** 등으로 대체할 수 있으며, **회귀 분석, K-NN 등의 머신러닝 알고리즘을 사용**하여 예측값으로 대체하는 방법이 있습니다.


> 이를 해결하기 위한 가장 쉬운 방법은 누락된 데이터를 제거하는 것입니다. 하지만 중요한 정보를 가진 데이터를 잃을 위험을 가지고 있습니다. 또 한 컬럼에 있는 missing value를 결측 되지 않은 다른 값의 평균이나 중앙값으로 대체하는 것입니다. 이 방법은 쉽고 작은 빠르다는 장점이 있지만 다른 feature 간의 상관관계가 고려되지 않고 인코딩 된 범주형 feature에 대해 안 좋은 결과를 제공한다는 단점을 가지고 있습니다. 최빈값, 0, 임의 값으로 채워넣을 수도 있습니다. 이 방식은 쉽고 범주형 feature에 잘 동작한다는 장점이 있지만 이것 또한 다른 feature 간의 상관관계가 고려되지 않고 데이터에 bias를 만든다는 단점이 있습니다.  
머신러닝을 통해 해결하는 방법으로는 KNN으로 가까운 이웃간의 거리기반 분석을 통해 채워넣을 수 있습니다. mean, median이나 most fequent보다 정확할 때가 많지만 메모리가 많이 필요한 방법입니다.  
Pandas에서는 dropna, fillna를 이용해 누락된 데이터를 제거하거나 채울 수 있습니다.  

</br>

## #1
###  엔트로피(Entropy)에 대해 설명해주세요. 가능하면 정보이득(Information Gain)도요.

엔트로피는 정보 이론에서 사용되는 개념으로 1로 갈수록 불순하고, 0으로 갈수록 불순하지 않다는 의미입니다. 어떤 시스템의 **불확실성** 혹은 **무질서도**를 나타내는 물리량입니다. 따라서 각 클래스에 대한 엔트로피가 낮을수록 해당 클래스의 예측이 확실하다는 것을 의미합니다.  
정보 이득은 어떤 속성을 기준으로 데이터를 분할할 때, 분할 이전과 이후의 엔트로피 차이로 **해당 속성이 얼만큼의 정보를 제공하는지**를 나타내는 개념입니다. 

</br>

## #2
### 로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.

취하는 경우는 **정규성**을 높이고 분석시에 정확한 값을 얻기 위함입니다. 단위수가 너무 큰 값들을 바로 회귀분석 할 경우, 결과를 왜곡할 우려가 있으므로 이를 방지하기 위해 로그함수를 취해줍니다. 또한 로그함수를 취함으로써 **비선형관계의 데이터를 선형으로** 만들 수 있습니다. 이때 로그 함수는 0~1 사이에서는 음수값을 가지므로 log(1+x)와 같은 방법으로 처리해주어야 합니다.

> 사례) 연령 같은 경우에는 숫자의 범위가 약 0세~120세 이하지만, 재산 보유액 같은 경우에는 0원에서 몇 조까지 올라갈 수 있습니다. 이럴 경우 데이터 간 단위가 달라지면 결과값이 이상해 질 수 있기 때문에 log를 이용해 큰 수를 같은 비율의 작은 수로 바꿔줍니다.

</br>

## #3
### 베이즈 정리에 대해 설명해주세요.

현재 주어진 **모수(가정)**에서 이 데이터가 관찰될 가능성인 **가능도(Likelihood)와** 데이터 전체의 분포인 **증거(Evidence)를 바탕으로** 가설에 대해 사전에 세운 확률인 **사전확률을** 실제로 가설이 성립할 확률인 **사후확률로 업데이트**하는 것입니다. 즉, 조건부 확률에 사전확률(prior)을 활용하여 통계적 추론을 하는 방법입니다.  
따라서 데이터가 주어지기 전에 이미 어느 정도 확률값을 예측하고 있을 때 이를 새로 수집한 데이터와 합쳐서 최종 결과에 반영할 수 있습니다.

</br>

## #4
### 신뢰 구간(Confidence Interval;CI)의 정의는 무엇인가요?

신뢰구간은 **모수가 실제로 포함될 것으로 예측되는 범위**입니다. 집단 전체를 연구하는 것을 불가능하므로, 샘플링된 데이터를 기반으로 모수의 범위를 추정하기 위해 사용됩니다. 따라서, 신뢰 구간은 샘플링된 표본이 연구중인 모집단을 얼마나 잘 대표하는지 측정하는 방법입니다. 일반적으로 95% 신뢰수준이 사용됩니다.

</br>

## #5
### 평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?

 대부분의 **데이터가 몰려있는 상황에서는 평균을, 이상치가 많이 있는 상황에는 중앙값을 사용**하는 것이 좋습니다. 평균은 모든 관측값을 반영하기 때문에 극단적인 이상치가 있는 경우, 이에 영향을 받을 수 있습니다. 따라서, 이러한 경우에는 데이터들의 가운데에 위치한 중앙값을 사용하는 것이 해당 집단을 대표하는 값이 될 수 있습니다.

</br>

## #6
### 필요한 표본의 크기를 어떻게 계산합니까?

![](https://user-images.githubusercontent.com/90206705/232524877-537946b1-2acd-4423-be72-4df05cf43c8d.png)

**모집단의 크기 N을 구하고, 신뢰수준 z와 오차범위 e를 선정**하여 표본의 크기를 구할수 있습니다. 이때, 오차범위는 작을 수록 모집단의 특성에 대한 유용한 정보를 제공하지만 모집단에 대한 추론이 틀릴 가능성도 높아지므로 10% 를 넘지 않게 해야 합니다. 일반적으로 신뢰도는 90%, 95%, 99%를, 표준편차는 0.5를 사용합니다. 이때 주의 할 점은 오차 한계를 더 작게 하려면 동일한 모집단에서 표본 크기가 더 커야 하고 더 높은 표집 신뢰 수준을 원한다면 표본 크기도 더 커져야 합니다.

</br>

## #1
### p-value를 모르는 사람에게 설명한다면 어떻게 설명하실 건가요?
- p-value는 실험결과가 우연적으로 발생한 것인지 그렇지 않은지 판단할 때 사용하는 수치입니다. 즉, p-value의 통계학적 정의는 **'귀무가설 하에서 관찰된 통계량만큼의 극단적인 값이 관찰될 확률'** 이고, p-value의 p는 probability입니다.

> - p-value는 가설검정을 할 때 쓰이는 기준이라고 할 수 있습니다. 가설검정은 기존의 주장인 귀무가설과 입증하고자 하는 가설인 대립가설을 설정하여 진행합니다. 이때 귀무가설이 참인데 기각한 경우를 1종 오류라고 하는데, 이러한 1종 오류를 범할 확률이 p-value입니다. 가설검정을 할 때에는 1종 오류를 범할 최대확률인 유의수준을 설정하여 유의 수준보다 p-value가 작다면 실험의 오류가 상한선보다 작으므로 귀무가설을 기각하고 입증하고자 하는 가설인 대립가설을 채택하게 됩니다.

</br>

## #2
### R square의 의미는 무엇인가요?

**회귀분석의 성능 평가 척도** 중 하나로, **결정력(결정계수)** 라고도 합니다. R-squared는 **독립변수가 종속변수를 얼마나 잘 설명하는 지**를 나타냅니다. R-squared는 0과 1 사이 값을 가집니다.  
MSE, RMSE, MAE의 경우 작을수록 좋지만 R-squared 는 클수록 좋습니다. 즉 **1에 가까울수록 독립변수가 종속변수를 잘 설명할 수 있다**는 뜻입니다.

> - $R^2=\frac{SSR}{SST} = 1-\frac{SSE}{SST}$
> - SST: 종속 변수의 총 변동성을 나타내는 제곱 합
> - SSR: 독립 변수들이 설명할 수 있는 종속 변수의 변동성
> - SSE: 회귀식 추정 y의 편차제곱의 합
> - R-squared는 독립변수의 설명력에 관계없이 독립변수가 많으면 많을수록 높아집니다. 이를 해결하기 위한 방법으로는 독립변수 개수에 대한 패널티를 부여하는 Adjusted R-squared (조정 설명계수)가 있습니다. 


</br>

## #3
### 고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요?

**정방행렬인 A에 임의의 벡터 x를 곱한 값이 상수값인 λ(람다)에 임의의 벡터 x를 곱한 값과 같을 때**의 x를 고유 벡터, 람다를 고유값이라고 합니다.
기하하적인 입장에서 보면 **고유값 λ는 변화되는 크기**를 의미하며, **고유벡터 x는 변화되는 방향**을 의미하게 됩니다.
이러한 고유값과 고유벡터는 정방행렬을 분해하는 고유값 분해(EVD), 직사각행렬도 분해할 수 있는 특이값 분해(SVD), 데이터들을 차원 축소시킬 때 기존의 의미를 잘 보존시키는 **주성분 분석(PCA) 등에 활용**할 수 있어 중요하다고 할 수 있습니다.

> PCA는 Eigen value decomposition(고유값 분할)을 통해 새로 만든 '축(관점)'으로 데이터를 바라보는 것입니다. 데이터들의 평균으로 원점을 가정하고 고유값 분할을 통해서 공분산행렬, 고유값, 고유벡터를 구하게 됩니다. 이렇게 고유값 분할을 통해 얻게 된 새로운 다양한 축(고유벡터) 관점에서 가장 큰 분산을 가질 때의 축을 기준으로 데이터를 바라보게 됩니다.


</br>

## #1
### Variance를 구할 때, N대신에 N-1로 나눠주는 이유는 무엇인가?

**표본의 분산은 모집단의 분산보다 작게 측정되기 때문**에 분모를 n이 아니라 n-1로 나눠 보정해줍니다. n-1로 나누는 이유는 표본 분산을 구할 때, 표본 평균과 n-1개의 변수를 알고 있으면 마지막 1개의 변수의 값은 고정되어 자유도가 n-1이 되기 때문입니다.

</br>

---
## #Machine Learning
## #4
### 차원의 저주에 대해 설명하고, 이를 해결하기 위한 방법을 말해주세요.

**Feature에 비해 데이터가 너무 적어서 생기는 현상**으로 과적합의 발생 가능성이 증가하게 됩니다. 이를 해결하기 위해서는 더 많은 데이터를 추가하거나 PCA와 같이 피처를 함축적으로 잘 설명할 수 있도록 저차원으로 매핑하는 차원축소를 하거나, 중요한 변수만 선택하여 차원을 줄이는 방법이 있습니다.

> 이를 해결하기 위해 가장 대표적인 방법은 **Feature Selection**과 **Feature Extraction**이 있습니다. 우선 **Feature Selection**은 우리가 예측하고자 하는 타겟 변수와 관련이 높은 변수들을 추려내는 방법입니다. 이 방법을 이용해 불필요한 변수들을 처냄으로써 모델의 학습 시간을 줄이고 성능 또한 높일 수 있습니다. **Feature Extraction**은 데이터 특성을 가장 잘 표현하는 **주성분**을 추출해 데이터 양을 줄이는 방법이며, 이에 사용되는 대표적인 방법은 바로 **주성분 분석(PCA)** 입니다.

</br>

## #5
### L1, L2 정규화에 대해 설명해주세요.

L1 정규화와 L2 정규화는 가중치(w)에 대한 제약 조건을 추가하여 모델의 과적합을 방지하고 일반화 성능을 향상시키는 방법입니다. 즉, 가중치의 크기를 제한함으로써 모델의 복잡도를 감소시켜 새로운 데이터에 대해 더 일반화된 예측을 하도록 하는 것입니다.  
L1 정규화는 **가중치 값들의 절댓값의 합을 최소화**하여, 불필요한 가중치 값들을 0으로 만들어 모델에서 특정한 입력에만 반응하는 가중치 값들을 제거할 수 있습니다. L2정규화는 **가중치 값들의 제곱합을 최소화**하여 가중치 값들 간의 차이를 줄여줄 수 있습니다.

</br>

## #6
### Cross Validation은 무엇이고 어떻게 해야하나요? (각 종류와 장단점)

Cross Validation은 test 데이터에 overfitting을 방지하기 위해, test와 분리한 train 데이터를 다시 **train-validation 세트로 나누어 실험**을 해 그 평균을 검증값으로 이용하는 것을 말합니다.  
Cross Validation의 종류로는 n개의 data를 균등하게 섞고 k개의 그룹으로 나눠서 1개만 test set, k-1개는 train set으로 사용하는 K-fold CV, 1개의 data sample만 test set, n-1개는 train set으로 사용하는 LOOCV, training set에도 cross validation을 다시 적용해서 best parameter를 찾아 training set에 적용해 train하는 Nested CV 등이 있습니다.  
계층별 k-겹 교차검증(Stratified K-Fold Cross Validation)은 원본 데이터의 레이블 분포를 먼저 고려하여, 이 분포와 동일하게 학습과 검증 데이터 세트를 분할하는 방식입니다.
Group K-Fold Cross Validation은 데이터를 그룹으로 나누어, 특정 그룹에 속한 모든 데이터를 한 폴드의 검증 데이터셋으로 사용하는 방식입니다.

</br>

## #7
### KNN 과 K-means 에 대해서 설명해주세요.

KNN은 지도학습으로, 주변의 **가장 가까운 K개의 데이터를 보고 데이터가 속할 그룹을 판단**하는 classification 알고리즘입니다.  
K-means는 비지도학습으로, **별도의 레이블이 없는 데이터 안에서 데이터 간 유사도를 기준으로 묶는** clustering 알고리즘입니다.

> KNN과 K-means의 가장 큰 차이는 지도학습과 비지도학습입니다. 둘은 모두 K개의 점을 지정하여 거리르 기반으로 구현되는 거리기반 분석 알고리즘입니다.  
먼저 **KNN**(최근접 이웃방법)은 해당 데이터와 가장 가까이 있는 K개의 데이터를 확인하여 새로운 데이터 특성을 확인하는 방법입니다. 지도학습 알고리즘 중 하나로 K가 짝수면 1:1 대응이 될 수도 있기 때문에 K는 홀수를 쓰는 것이 보편적입니다. KNN은 **회귀와 분류 모두 사용 가능**합니다.  
**K-means** 알고리즘은 데이터를 K개의 군집(cluster)으로 묶는 clustering 알고리즘입니다. 여기서 K는 묶을 그룹의 수를 말하고 Means는 데이터로부터 그 데이터가 속한 그룹의 중심까지의 평균 거리를 의미하며 이 값을 최소화 하는 것이 K-means입니다.

</br>

## #8
### XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?

XGBoost는 **Gradient boosting**을 이용한 모델입니다. 병렬 연산을 지원하고 GPU를 사용할 수 있는 옵션이 있기 때문에 학습하는데 소요되는 시간을 절약할 수 있습니다.  
XGBoost는 **빠르고** 다양한 커스텀 옵션으로 **유연성**이 좋습니다. 또한 그리디 알고리즘으로 과적합 방지가 가능해 캐글에서 좋은 성능을 보이고 있습니다.

> **XGBoost**는 속도가 빠르고, 자원 효율성이 높아서 캐글에도 인기가 많습니다. **XGBoost**는 기존 Gradient Tree Boosting 알고리즘에 과적합 방지를 위한 기법이 추가된 지도학습 알고리즘 입니다. **XGBoost**는 기본 학습기를 의사결정나무로 하며 Gradient Boosting과 같이 **Gradient(잔차)를 이용**하여 이전 모델의 약점을 보완하는 방식으로 학습니다. **XGBoost**는 과적합 방지가 잘되고 예측 성능이 좋다는 장점이 있지만 작은 데이터에 대해서는 과적합 가능성이 있다는 단점을 가지고 있습니다.
분류와 회귀문제에 모두 사용가능하며, XGBoost는 자체에 얼리스탑과 같은 과적합 규제 기능이 있어 강한 내구성을 지닙니다. 또한, 다양한 파라미터 옵션을 제공하는 Customizing이 용이하기도 합니다. 이러한 이유들로 기존 GBoost보다 XGBoost가 더 인기 있습니다.

</br>

## #9
### 회귀 / 분류시 알맞은 metric은 무엇인가요?

회귀 문제에서는 실제 값과 모델이 예측하는 값의 차이에 기반을 둔 metric(평가)을 사용합니다. 대표적으로 **RSS(단순 오차 제곱 합), MSE(평균 제곱 오차), MAE(평균 절대값 오차)** 가 있습니다.  
분류 문제에서는 분류 결과의 신뢰도를 나타낼 수 있는 metric을 사용합니다. 대표적으로는 **Accuracy, Precision, Recall와 이를 조합합 F1 score**등이 있습니다.

> [Why is cross entropy not a common evaluation metric for model performance?](https://stats.stackexchange.com/questions/370861/why-is-cross-entropy-not-a-common-evaluation-metric-for-model-performance)

</br>

## #7
### SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?

SVM을 비선형 분류 모델로 사용하기 위해 **저차원 공간의 데이터를 고차원 공간으로 매핑**하여 선형 분리가 가능한 데이터로 변환하여 처리합니다. SVM이 좋은 이유는 SVM은 데이터들을 선형 분리하며 최대 마진의 초평면을 찾는 크게 복잡하지 않은 구조이며, **커널 트릭을 이용**해 차원을 늘리면서 비선형 데이터들에도 좋은 결과를 얻을 수 있습니다. 또한 이진 분류 뿐만 아니라 수치 예측에도 사용될 수 있습니다. **Overfitting 경향이 낮으며** 노이즈 데이터에도 크게 영향을 받지 않습니다.

> SVM 회귀의 경우 SVM 분류와 반대로, 제한된 마진 오류(즉, 도로 밖의 샘플) 안에서 마진 안에 가능한 한 많은 샘플이 들어가도록 학습합니다.

</br>

## #8
###  ROC 커브에 대해 설명해주실 수 있으신가요?

ROC 커브는 거짓 긍정(False Positive)을 피하면서, 참 긍정(True Positive)을 탐지하는 것 사이의 트레이드오프를 관찰하기 위한 지표로 FPR(False Positive Rate)를 x축으로, TPR(True Positive Rate)를 y축으로 나타내는 곡선입니다.  
이러한 ROC curve는 왼쪽 상단에 가까울수록, 그래프의 아래 면적을 의미하는 AUC (Area Under the Curve)는 1에 가까울수록 좋은 성능이라고 판단합니다.

</br>

## #9
### 회귀 / 분류시 알맞은 손실함수와 이에 대한 설명

회귀 문제에서는 **MSE나 MAE** 손실함수를 사용합니다.  MSE는 예측값과 실제값 간의 차이를 제곱한 값의 평균으로, 회귀 문제에서 가장 많이 사용되는 손실함수입니다. **MAE는** 예측값과 실제값 간의 차이의 절댓값의 평균으로, **이상치(outlier)가 있는 데이터에 민감하지 않다**는 특징이 있습니다.  
분류 문제에서는 이진 분류의 경우 Binary **Cross Entropy**를 다중 분류의 경우 Categorical Cross Entropy를 주로 사용합니다. 이외에도 불균형 데이터셋의 성능을 향상시키기 위해 사용되는 Focal loss가 있습니다. Focal loss 는 Categorical Cross Entropy에 (1 - y_pred)^γ를 곱하여  γ (gamma)값이 증가할수록 쉬운 샘플에 대한 가중치가 줄어들어 어려운 샘플에 대해 더 집중적으로 학습할 수 있게 합니다.

> 손실함수(loss funciton)는 딥러닝 모델이 실제 레이블과 가장 가까운 값이 예측되도록 훈련할 때, 모델의 예측값과 실제 레이블 간의 거리를 측정하기 위해 사용되는 함수입니다.

</br>

## #10
### 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?

**뉴턴법은 현재 x값에서 접선이 x축과 만나는 지점으로 x를 이동시켜 가면서 점진적으로 해를 찾아가는 방법**입니다. 초기값을 잘 주면 수렴 속도가 매우 빠르지만, 잘못 주면 시간이 오래 걸리거나 해를 찾지 못할 수 있습니다. 함수가 미분 가능해야하고 미분값이 0이 지점이 없어야합니다.   
반면 **Gradient Descent는 현재 위치에서 함수의 기울기를 이용하여 극소점을 찾아가는 방법**으로 local minimum에 빠질 수 있다는 문제가 있지만, 모든 차원과 모든 공간에서 적용이 가능합니다.  
Gradient Descent는 매 단계에서 함수의 기울기를 계산해야 하므로 계산 비용이 낮지만, 수렴 속도가 상대적으로 느릴 수 있습니다. Newton's Method는 매 단계에서 이차 도함수를 계산해야 하므로 계산 비용이 높아질 수 있지만, 수렴 속도가 빠를 수 있습니다.


</br>

## #11
### 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?

**머신러닝**은 모델에 대한 정교한 가정보다는 데이터의 다양한 피쳐를 사용하여 **높은 예측률**을 달성하고자 합니다. 따라서 머신러닝은 데이터의 복잡한 패턴을 인식하고 이를 통해 예측하는 능력이 뛰어나며, 데이터가 매우 큰 경우에도 높은 정확도로 예측이 가능합니다.  
반면 **통계적 접근방법**은 데이터의 분포와 가정을 통해 **신뢰 가능한 모델**을 만드는게 목적으로 다양한 통계 모델링 기법을 사용합니다. 통계적 접근방법은 데이터의 특성을 파악하고, 변수의 선택과 추정, 가설 검정 등을 수행하며 **어떤 피쳐가 어떤 원인을 주는지 알 수 있다**는 특징이 있습니다. 

</br>

## #4
### 앙상블 방법엔 어떤 것들이 있나요?

앙상블 방법엔 **voting, bagging, boosting, stacking**이 있습니다.  
**Voting**은 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식으로 서로 다른 알고리즘을 여러 개 결합하여 사용합니다.  
**Bagging**은 각각의 데이터로 모델 여러개를 독립적으로 만들어서 각각 모델에서 예측값의 평균이나 최다투표값을 사용합니다.   
**Boosting**은 먼저 모델을 만들고, 그 모델이 약한 데이터에 대해서 새로운 모델(weak learner) 을 만든 후, weak learner를 합쳐서 strong learner를 만듭니다.  
이 외에 모델의 output을 새로운 독립변수로 사용하는 **stacking**이 있습니다.

> **앙상블(Ensemble)** 은 여러개의 모델을 조합해서 더 나은 예측 성능을 달성하는 머신러닝 기법입니다. 이러한 앙상블 방법에는 Voting, Bagging, Boosting, Stacking 등의 방법이 있습니다.  
**보팅(Voting)** 은 여러 개의 예측 모델을 결합하여 다수결 투표를 하는 방법입니다. 보팅을 사용하면 여러 모델이 조합되어 더 일반화된 성능을 보일 수 있습니다.  
**배깅(Bagging, Bootstrap Aggregation)** 이란 샘플을 여러번 뽑아(Bootstrap = 복원 랜덤 샘플링) **여러개의 독립적인 모델을 학습**시켜 각 모델의 예측 결과를 집계하는 **병렬적인** 방법입니다. 이때, 카테고리형 데이터는 투표 방식(Votinig)으로 결과를 집계하며, 연속형 데이터는 평균으로 집계합니다. Bagging을 사용한 대표적인 알고리즘으로는 랜덤 포레스트(Random Forest)가 있습니다. 이러한 배깅을 사용하게 되면, 학습 데이터가 충분하지 않더라도 충분한 학습효과를 주어 높은 bias의 underfitting 문제나, 높은 variance로 인한 overfitting 문제를 해결하는데 도움을 준다는 장점이 있습니다.  
**부스팅(Boosting)** 이란 이전 모델의 잘못 예측한 샘플에 가중치를 높여서 다음 모델을 학습하는 **순차적인** 방법입니다. 오답에 더 집중하여 학습시키기 떄문에 일반적으로 배깅에 비해 정확도가 높은 편이지만, 틀렸던 부분에 대해 반복적으로 학습하므로 오버피팅의 문제가 있으며, 이상치에 취약하고, 속도가 느리다는 단점이 있습니다. Boosting을 사용한 대표적인 알고리즘으로는 그래디언트 부스팅(Gradient Boosting)과 XGBoost, AdaBoost 가 있습니다.  
**스태킹(Stacking)** 이란 여러 개별 모델이 예측한 결과값을 다시 학습 데이터셋으로 사용해서 모델을 만드는 방법입니다. 그러나 같은 데이터셋을 통해 예측한 결과를 기반으로 다시 학습하게 되면 오버피팅 문제점이 있습니다. 따라서 이전 모델의 훈련 데이터에서 검증 데이터를 나누어 학습하는 Cross Validation 방식을 도입하여 이를 해결할 수 있습니다. 이를 통해, 첫 번째 단계에서 학습한 모델들이 하나의 검증 데이터에 대해 과적합되는 것을 방지하고, 더욱 일반화된 모델을 만들 수 있습니다.

</br>

## #5
### 불균형 데이터를 어떻게 해결할 수 있을까요?

불균형 데이터 상태 그대로 예측하게 된다면 **과적합 문제가 발생**할 수 있습니다. 모델은 가중치가 높은 클래스를 더 예측하려고 하기 때문에 accuracy는 높아질 수 있지만 분포가 작은 값에 대한 precision은 낮을 수 있고, 분포가 작은 클래스의 재현율이 낮아지는 문제가 발생할 수 있습니다. 이를 해결하기 위해 일반적으로 대표적으로 **Under Sampling과 Over Sampling**이 있습니다. 이 외에도 소수 클래스 데이터에 높은 가중치를 부여하는 방법, 소수 클래스 데이터를 기반으로 합성 데이터를 생성하여 데이터를 보강하는 방법이 있습니다.

> **Under Sampling**은 Down Sampling라고도 불리며 데이터의 분포가 높은 값을 낮은 값으로 맞춰주는 작업을 거치는 것을 말합니다. 데이터 분포를 확인 후 높은 class를 낮은 class크기에 맞춰주는 작업을 거치게 됩니다. 이런 과정을 거치면 유의미한 데이터만 남길 수 있지만 정보가 유실되는 문제가 생길 수 있습니다. 대표적인 방법으로는 Random Under Sampling, Tomek link, CNN 등이 있습니다.  
**Over sampling**은 Up Sampling라고도 불리며 분포가 작은 클래스의 값을 분포가 큰 클래스로 맞춰주는 샘플링 방법입니다. 분포가 작은 클래스 값을 일련의 과정을 거쳐 생성하는 방법을 뜻합니다. 이런 과정을 거치면 정보의 손실을 막을 수 있지만 여러 유형의 관측치를 다수 추가하기 때문에 오히려 오버피팅을 야기할 수 있습니다. (따라서 새로운 데이터, Test dataset에서의 성능이 나빠지는 결과를 초래할 수 있습니다.) 대표적인 방법으로는 Random Over Sampling, ADASYN(Adaptive Synthetic Sampling), SMOTE 등이 있습니다.

</br>

## #6
### K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)

K-means clustering은 중심값 선정, 거리로 데이터 분류, 분류 완료까지 반복의 과정을 거치기 때문에 **중앙값 초기화를 잘못 할 경우 local minima에 빠질 위험**이 있습니다. 또한 모든 데이터를 거리로만 판단하기 때문에 사전에 주어진 목적이 없어 **결과 해석이 어렵다**는 단점이 있습니다. 노이즈가 많은 경우 효과적이지 않으며, 특히 하나의 군집이 큰 경우에는 해당 군집이 쪼개지면 비슷한 크기들의 클러스터로 형성될 수 있습니다. 데이터의 분포모양이 특이한 경우에도 군집이 잘 이루어지지 않습니다.


</br>

## #2
### A/B 테스트는 무엇인가요?

A/B 테스트란 두 제품 혹은 절차 중 어느 것을 택할지 결정하기 위해 귀무가설과 대립가설을 세워 실험을 진행합니다. A/B test의 진행은 조사 수행, 가설 세우기, 변형, 테스팅, 결과 분석 및 결론 도출 순으로 진행됩니다.

</br>

## #3
### Normalization과 regularization의 차이는?

Normalization은 데이터를 스케일링하여 **값의 범위를 균일화** 해주는 과정이며, 데이터의 상대적 크기 차이를 줄이고 분산을 고르게 분배하여 모델의 성능을 개선합니다.  
반면, Regularization은 모델의 복잡도를 제한하고 일반화 성능을 향상시키는 방법으로, **모델의 가중치를 제한하여 overfitting을 방지**합니다.   
Normalization은 데이터 전처리 과정 중 하나이며, Regularization은 모델 훈련 과정에서 수행됩니다.

</br>

## #4
### F1 score 사용하는 이유를 말해주세요.

F1 score는 **데이터의 라벨이 불균형 구조일 때 모델의 성능을 정확하게 평가**할 수 있고 성능을 하나의 숫자로 나타낼 수 있습니다. Recall과 precision의 **조화평균을 이용**해 산술평균보다 둘 중 높은 값의 영향을 줄일 수 있습니다.

</br>

## #5
### (Super-, Unsuper-, Semi-Super)vised learning이란 무엇인가?

Supervised learning(지도학습)은 입력 데이터와 이에 대응되는 레이블 데이터(정답)를 이용하여 모델을 학습시키는 기계 학습 방법입니다. 즉, 입력과 출력 사이의 관계를 학습하여 새로운 입력 데이터에 대한 출력을 예측하는 모델을 생성합니다. Classification, regression 등이 있습니다.  
Unsupervised learning(비지도학습)은 입력 데이터에 대한 레이블이 없는 상태에서 패턴이나 구조를 발견하는 기계 학습 방법입니다. Clustering, 이상탐지, autoencoder 등이 이에 해당합니다.  
Semi-supervised learning(반지도학습)은 레이블이 있는 데이터와 레이블이 없는 데이터를 모두 사용하여 모델을 학습시키는 기계 학습 방법입니다. 레이블이 있는 데이터를 통해 모델을 학습시킨 후, 레이블이 없는 데이터의 예측 결과에 대해서 직접 검토하여, 잘못된 분류 결과를 수정하거나 레이블을 부여할 수 있습니다.

</br>

### Feature space에서 feature 간의 distance 측정 방법을 설명해주세요.

Feature space에서 distance를 측정하는 방법에는 Euclidian, Cosine, Manhattan 등 여러가지 방법이 있습니다.  
**Euclidian distance**는 가장 흔히 사용되는 방법으로, 각 feature의 차이를 제곱하여 합한 후 제곱근을 취한 값입니다. 차원이 높아질 경우 계산량이 급격히 증가하기 때문에 **2~3차원 정도의 저차원 데이터**에서 쓰일 수 있습니다.  
**Cosine distance**는 내적공간의 두 벡터간 각도와 코사인값을 이용하여 측정된 **벡터간 유사한 정도**입니다. 거리는 고려하지 않고 방향만 고려하기 때문에 **2차원보다 높은 차원의 데이터, 또 벡터의 크기가 중요하지 않은 데이터**에 주로 사용됩니다.   
**Manhattan distance**는 **각 좌표의 차이의 절댓값을 모두 더한 것**을 거리로 사용합니다. 이 거리 측정 방법은 특징 벡터의 각 차원이 서로 독립적인 값을 가지는 경우나 범주형 데이터에 사용될 수 있습니다.   
이 외에도 Hamming, Chebyshev, Jaccard 등 다양한 distance 측정 방법이 있습니다.
> Feature space(특성 공간): 데이터의 특성을 나타내는 변수들로 이루어진 공간
<br/>

### 확률론적 모델링(Probabilistic Modeling)이란 무엇인가요?

데이터의 확률적인 구조와 패턴을 모델링하는 것을 의미합니다. 확률론적 모델링은 일반적으로 확률 분포를 가정하고, 주어진 데이터에 가장 잘 맞는 모델 파라미터를 추정하는 과정을 거칩니다. 대표적인 확률론적 모델링 방법에는 확률적 회귀, 나이브 베이즈, 히든 마르코프 모델, 베이지안 네트워크 등이 있습니다.
<br/>


### 머신러닝 모델의 결과를 해석하는 방법에는 어떤 것들이 있나요?

머신러닝 모델의 결과를 해석하는 방법에는 일반적으로 사용되는 방법에는 다음과 같은 것들이 있습니다.   
feature 중요도를 파악하여 어떤 feature가 모델을 예측하는 데에 많이 기여하는지 분석할 수 있습니다.   
feature 히트맵을 통해 각 특성이 모델의 예측에 미치는 영향을 파악할 수 있습니다.  
의사 결정 트리와 같이 해석이 가능한 모델을 사용하여 모델의 분기 기준들을 파악할 수 있습니다.  
이 외에도 모델의 정확도와 정밀도, 재현성들을 통해 모델의 학습 시간에 따른 성능을 파악할 수 있습니다.
> 머신러닝 모델의 결과를 해석하기 위해서는 **PDP(Partial Dependence Plot)나 SHAP Value**를 사용할 수 있습니다.
PDP는 예측모델을 만들었을 때, 어떤 특성이 예측모델의 타겟변수에 어떤 영향을 미쳤는지 알기 위한 그래프입니다. 특성과 타겟변수의 관계를 전체적으로만 파악할 수 있을뿐 개별 관측치에 대한 설명을 하기에는 부족하다는 한계가 있습니다.  
SHAP Value는 게임이론을 바탕으로 하나의 특성 대한 중요도를 알기위해 여러 특성들의 조합을 구성하고 해당 특성의 유무에 따른 평균적인 변화를 통해 얻어낸 값입니다.  
<br/>

---
## #Deep Learning
## #5
### TensorFlow, PyTorch 특징과 차이가 뭘까요?

Tensorflow는 단일 데이터 흐름으로, **그래프를 만들고 그래프 코드를 성능에 맞게 최적화한 다음 모델을 학습하는 define-and-run** 방식이기 때문에 더 쉽게 다른 언어나 모듈에 적용이 가능합니다.  
PyTorch는 **각 반복 단계에서 즉석으로 그래프를 재생성하는 define-by-run** 방식이라 모델 그래프를 만들 때 고정 상태가 아니므로 데이터에 따라 조절이 가능한 유연성을 갖고 있습니다.

> Tensorflow는 **Define and Run** 방식으로 코드를 직접 돌리는 환경인 세션을 만들고, placeholder를 선언하고 이것으로 계산 그래프를 생성한 후에(Define), 코드를 실행하는 시점에 데이터를 넣어 실행하는(Run) 방식입니다. 이는 모델 구조가 미리 정의되어 있기 때문에, 모델 배포가 쉽다는 장점이 있지만, 모델 구조를 변경하려면 새로운 모델을 정의해야 하기 때문에 유연성이 떨어진다는 단점이 있습니다.  
반면, PyTorch는 **Define by Run** 방식으로 계산 그래프의 선언과 동시에 데이터를 집어넣고 세션도 필요없이 돌리면 되기때문에 코드가 간결하고, 더 유연하게 수정하거나 실험할 수 있다는 장점이 있습니다. 그렇지만, 입력 데이터마다 새로운 계산 그래프를 정의하여 사용한다는 단점이 있습니다.

> TensorFlow에서는 Grappler라는 기본 그래프 최적화 시스템이 있습니다. Grappler는 그래프 모드 (tf.function 내)에서 최적화를 적용하여 그래프 단순화 및 함수 본문 인라인과 같은 기타 고급 최적화를 통해 TensorFlow 계산 성능을 향상하여 절차 간 최적화를 가능하게 합니다

<br/>

## #6
### ReLU로 어떻게 곡선 함수를 근사하나요?

ReLU는 **선형(y=x)과 비선형(y=0)의 결합**이기 때문에 ReLU가 반복해 적용되면 선형부분의 결합으로 곡선 함수를 표현할 수 있습니다. ReLU를 여러 개 결합하면, **특정 지점에서 특정 각도만큼 선형 함수를 구부릴 수 있습니다**. 이 성질을 이용하여 곡선 함수 뿐만 아니라 모든 함수에 근사를 할 수 있게 됩니다.


<br/>


## #7
### Data Normalization은 무엇이고 왜 필요한가요?

**Feature들의 분포(scale)을 조절하여 균일하게** 만드는 방법입니다. 즉, 개별 피처의 크기를 모두 똑같은 단위로 변경하는 것입니다.

정규화를 하는 이유는 피쳐들간의 스케일이 심하게 차이가 나는 경우, 값이 큰 피처가 더 중요하게 여겨질 수 있기 때문입니다. 데이터 정규화를 하게 되면 **학습속도가 개선되며, 오버피팅을 억제**할 수 있다는 장점을 얻을 수 있습니다.

이러한 정규화하는 방법으로는 대표적으로 **최소-최대 정규화(min-max norm)와  Z-점수 정규화**가 있습니다.

<br/>

## #8
### 활성화 함수가 왜 필요한가요?

Activation function은 **선형 함수를 비선형함수로** 만들어 표현력을 더 키워주는 함수입니다. Activation function이 없을 경우 layer를 깊게 쌓아도 선형함수기 때문에 의미가 없습니다. Activation function을 이용해 **모델의 복잡도를 높이고** 복잡한 비선형적인 문제를 해결할 수 있게 만듭니다.

<br/>

## #9
### Gradient Descent에 대해서 중학생이 이해할 수 있게 쉽게 설명해주세요.

Gradient Descent는 **함수의 최솟값을 찾기 위해 기울기(경사도)를 이용**하여 함수를 내려가는 것입니다. 이때, 경사도의 반대 방향으로 내려가면서 최솟값에 점점 가까워질 수 있습니다.

딥러닝에서 Gradient Descent는 모델의 가중치(weight)를 업데이트하는 데에 사용됩니다. 모델의 손실(loss)을 최소화하기 위해, Gradient Descent를 사용하여 모델의 가중치를 업데이트하면서 최적의 모델을 학습할 수 있습니다.


<br/>

## #10
### 오버피팅일 경우 어떻게 대처해야 할까요? 알고계신 방법들을 전부 말해주세요.

Overfitting이 일어날 경우에는 augmentation를 이용해 **데이터의 절대적인 양을 늘리거나 regularization, dropout을 사용하고, early stopping**으로 일정 횟수 이상 validation loss가 증가하는 시점부터 overfitting이 발생했다고 판단하고 이에 학습을 종료시킵니다. 또는 **Batch Normalization**으로 데이터 분포를 통일 시켜줘서 과적합을 방지합니다.

> Overfitting을 막기 위해 여러 방법이 있습니다. 
> 1. 데이터의 양이 적을 경우, 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현상이 발생할 확률이 늘어납니다. 이럴 경우 기존의 데이터를 증강시켜 데이터를 늘리고 데이터가 많을수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있습니다.
> 2. 복잡한 모델은 간단한 모델보다 과적합될 가능성이 높으므로 정규화를 통해서 복잡한 모델를 좀 더 간단하게 하는 방법이 있습니다.
> 3. Dropout을 사용해 학습과정마다 일정 비율의 뉴런만 사용하는 방법을 사용합니다.
> 4. 일정 횟수 이상 validation loss가 증가하는 시점부터 overfitting이 발생했다고 판단하고 이에 학습을 종료시킵니다,
> 5. Batch Normalization으로 데이터 분포를 통일 시켜줘서 과적합을 방지합니다. 

</br>

## #11
### Dropout은 무엇이며 왜 사용하는 지 말해주세요.

Dropout은 서로 연결된 layer에서 **0에서 1사이의 확률로 뉴런을 제거(drop)** 하는 기법입니다. 꺼지는 뉴런의 종류와 개수는 오로지 랜덤하게 Dropout rate에 따라 결정이됩니다.
Dropout은 어떤 특정한 설명변수 Feature만을 과도하게 집중하여 학습함으로써 발생할 수 있는 overfitting을 방지하기 위해 사용됩니다. 학습 데이터에 의해 각 node들이 **co-adaptation**되는 현상을 방지하고 매 학습마다 drop 되는 뉴런이 달라지기 때문에 서로 다른 모델들을 **앙상블 하는 것과 같은 효과**가 있습니다.

> co-adaptation: 어느 시점에서 같은 층의 두 개 이상의 노드의 입력 및 출력 연결강도가 같아지면, 아무리 학습이 진행되어도 그 노드들은 같은 일을 수행하게 되어 불필요한 중복이 생기는 문제

</br>

## #12
### GD가 Local Minima 문제를 피하는 방법을 말해주세요.

초기값을 잘 선정하거나, LambdaLR, CosineAnnealingLR 등 **학습률을 조절하는 학습률 스케줄링(learning rate scheduling)** 기법을 사용할 수 있습니다. 또는 Momentum을 사용하여 이전에 이동했던 방향과 크기를 고려하여 local minima에 빠지는 것을 방지할 수 있습니다. 이 외도 Gradient descent에서 파생된 Adagrad, Adadelta, RMSprop, Adam 등 다른 optimizer를 사용하는 방법이 있습니다.

</br>

## #13
### Batch Normalization은 무엇인가요?

학습 과정에서 **Mini-batch마다 평균과 분산을 활용하여 데이터의 분포를 정규화**를 하고, scale factor와 shift factor를 이용하여 새로운 값을 생성하는 것입니다. 이때 scale factor와 shift factor는 다른 레이어에서 weight를 학습하듯이 역전파에서 학습됩니다.
기존에는 learning rate를 높게 잡을 경우 gradient가 explode/vanish 하거나, local minima에 빠지는 문제가 있었습니다. 그러나 Batch Normalization을 사용할 경우 **parameter의 scale에 영향을 받지 않게 되어, learning rate를 크게 잡을 수 있어 안정적으로 빠른 학습**을 할 수 있습니다.   
여기서 중요한 것은 Batch Normalization은 학습 단계와 추론 단계에서 조금 다르게 적용되어야 합니다.

</br>

## #14
### BN 적용해서 학습 이후 실제 사용시에 주의할 점은 무엇인가요(코드적인 부분 포함)?

Inference 시 input을 이용해 BN을 하면 모델이 train에서 input의 분포를 추정한 의미가 없어지기 때문에, inference 시에는 결과를 deterministic하게 만들기 위해 **미리 저장해둔 mini-batch의 moving average를 이용**해 정규화를 합니다.  
Batch normalization을 적용할 때는 **activation function 앞에 적용**해야 합니다. 일반적으로 Hidden Layer - Batch Normalization - Activiation Function 순서로 적용합니다. Batch Normalization 적용 후 ReLU와 같은 활성화 함수를 적용하면 데이터의 절반가량이 음수이기에 의미 없는 값을 가지게 될 수 있습니다.  

Pytorch에서 BatchNormalization을 사용하는 대표적인 방법은 torch.nn.BatchNorm1d와 torch.nn.BatchNorm2d를 사용하는 것입니다.  
이 때, 차이점은 BatchNorm1d의 경우 Input과 Output이 (N, C) 또는 (N, C, L)의 형태를 가지고 BatchNorm2d의 경우 Input과 Output이 (N, C, H, W)의 형태를 가집니다. 여기서 N은 Batch의 크기를 말하고 C는 Channel을 말합니다. BatchNorm1d에서의 L은 Length을 뜻하고 BatchNorm2d에서의 H와 W 각각은 height와 width를 뜻합니다.
Inference 시에는 BATCHNORM함수에 track_running_stats라는 파라미터에 False의 값을 주어야 합니다.

</br>

## #12 
### Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?

딥러닝에서 가중치를 잘 초기화하는 것은 기울기 소실이나 local minima 등의 문제를 방지 가능합니다.  
**LeCun** 초기화은 **입력 노드 수를 고려**한 정규 분포와 균등 분포를 따르는 방법입니다.  
**Xavier** 초기화는 **입력 노드의 수와 출력 노드 수를 고려**하여 가중치를 초기화하는 방법입니다. Sigmoid 나 tanh 함수와는 좋은 결과를 보여주지만 ReLU 함수와 사용할 경우 0에 수렴하는 문제가 발생하는 초기화 방법입니다.  
**He** 초기화는 ReLU 와 함께 많이 사용되는 방법으로, LeCun 방법과 같이 **입력 노드의 수만을 고려하지만 상수 부분은 Xavier 초기화의 방법을 사용**합니다.

</br>

## #13
### 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)

**Sigmoid**는 **입력을 0 ~ 1 사이로 mapping**합니다. 하지만 입력값이 커질수록 미분값이 0에 수렴해 **gradient vanishing** 문제가 생기며 **zero-centered하지 않아** 학습이 느려집니다.  
**Tanh**는 sigmoid를 변형한 쌍곡선함수로, **입력을 -1 ~ 1 사이로 mapping해 zero-center 문제는 해결**했지만 gradient vanishing 문제는 해결하지 못했습니다.  
**ReLU**는 입력이 양수일 경우 **saturate 문제가 해결**되며, 단순히 max 함수를 사용해 **속도가 빠릅니다**. 입력이 음수일 경우에도 saturate 문제를 해결한 함수로는 **LeakyReLU**가 있습니다.  
분류 문제에는 **softmax**를 사용하기도 합니다.


</br>

## #14
### 효율적인 GPU의 사용을 위해 어떻게 dataloader를 조절할 수 있는가?

PyTorch의 DataLoader는 **`batch_size`** 로 batch의 크기를 조절하여 데이터를 미니 배치로 분할하고, 이들을 병렬적으로 로딩하고 전처리하는 함수로 데이터를 GPU로 로드할 수 있으며, 이를 통해 모델 학습 시간을 단축할 수 있습니다.  
DataLoader의 **`num_workers`** 하이퍼파라미터 값이 높을수록 DataLoader는 더 많은 프로세스를 생성하여 데이터 로딩 및 전처리를 병렬로 처리합니다. 이때, 각 워커는 CPU를 사용하여 데이터를 로드하고 전처리합니다. 이를 GPU 메모리로 이동시킬 때, **`pin_memory`** 파라미터를 사용하여 CPU 메모리와 GPU 메모리 간 데이터 이동을 최적화할 수 있습니다. **`pin_memory=True`** 로 설정하면 DataLoader는 Tensor를 CUDA 고정 메모리에 복사하여 GPU 메모리로 전송합니다.

</br>

## #15
### 하이퍼 파라미터는 무엇인가요?

하이퍼 파라미터(Hyper-parameter)는 모델링할 때, **사용자가 직접 세팅해주는 값**으로 learning rate, epoch나 SVM에서의 C, sigma 값, KNN에서의 K값 등이 있습니다.
하이퍼 파라미터는 정해진 최적의 값이 없으며, 사용자의 **선험적 지식을 기반으로 설정(휴리스틱)** 합니다. 이러한 하이퍼 파라미터 튜닝 기법에는 Manual Search, Grid Search, Random Search, Bayesian Optimization 등이 있습니다.

</br>

## #16
### 미니배치를 작게 할때의 장단점은?

전체 데이터를 쪼개서 여러 번 학습하기 때문에, 전체 training 데이터 셋을 배치로 사용것보다 계산량이 적어(즉 메모리 사용량이 적어), **학습 속도가 빠르다**는 장점이 있습니다. 반면에 big batch보다 느리고 loss function의 최솟값을 찾기 위해 자주 step의 방향을 바꿔야 하므로 **학습이 불안정**한 단점이 있습니다.

> 하지만 요즘엔 Adam optimizer나 batch normalization(BN)등의 기법을 사용함으로써 학습안정화가 정말 잘 되는 네트워크가 많습니다. 따라서 batch size가 커도 local minimum을 잘 지나쳐 global minimum으로 수렴할 수 있게되었습니다. 따라서 최근에는 batch size를 줄 수 있는만큼 최대한 크게줘야 좋다고 할 수 있습니다. batch size가 클수록 BN을 더욱 정확하게 계산할 수 있어 BN의 효과를 더욱 잘 누릴 수 있게되고 그렇게 되면 학습속도가 빨라지며 learning rate의 hyper parameter 설정에서 꽤나 자유로워질 수 있게됩니다.


</br>

## #7
### Adam Optimizer의 동작은?

Momentum과 RMSProp를 합친 방식입니다. **Momentum** 방식과 같이 현재 기울기와 이전 기울기의 가중합을 이용하여 파라미터 업데이트를 수행하여 **기울기 방향을 조절**하고, **RMSProp**과 같이 기울기 제곱값의 이동 평균을 이용하여 **학습 속도를 조절**합니다. 따라서 기존의 최적화 알고리즘에 비해 더 빠르고 안정적으로 수렴할 수 있습니다. 오래전 time step에서의 값은 적게 반영하고 최근 step의 값을 많이 반영하기 위한 moving average를 하이퍼파라미터로 사용합니다. 

</br>

## #8
### 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?

딥러닝은 머신러닝 알고리즘 중 하나인 **인공신경망(Artificial Neural Network, ANN)을 사용하는 모델**입니다. 머신러닝은 데이터의 특성을 사람이 직접 추출한 후 모델을 학습시키는 반면, 딥러닝은 **모델이 스스로 데이터의 특성을 추출하여 학습을 한다는 차이**가 있습니다.

</br>

## #9
### Objective Function, Loss Function, Cost Function의 차이는 무엇인가요?

Loss function은 하나의 input 데이터의 오차를 계산하는 함수입니다. Cost function은 이러한 Loss를 총 데이터에 대해 평균을 낸 값입니다. Objective function은 가장 일반화된 용어로 학습을 통해 최적화하려는 모든 종류의 함수를 의미합니다. 

> **Loss function ⊂ Cost Function ⊂ Objective Function**.  
> - 모델을 학습할 때는 비용(cost), 즉 오류를 최소화하는 방향으로 진행하게 됩니다. 비용이 최소화되는 곳이 성능이 잘 나오는 부분이며, 가능한 비용이 적은 부분을 찾는 것이 최적화 방법입니다. 이 비용 혹은 손실이 얼마나 있는지 나타내는 것이 비용함수, 손실함수라고 할 수 있습니다.
목적 함수는 매개변수를 최적화하는 함수입니다. 손실함수는 데이터 각각에 대해 학습한 매개 변수로 이루어지는 확률 함수 결과 확률 값과 실제 확률 값의 오차를 측정하는 함수입니다. 비용함수는 데이터 집합에 대한 오차 측정 함수입니다. 
즉, Objective Function가 가장 상위 개념이고 Cost Function과 Loss Function은 Object Function의 한 예라 볼 수 있습니다.


</br>

## #6
### SGD에서 Stochastic의 의미는?

"Stochastic"이라는 단어는 임의의 확률과 연결된 시스템 또는 프로세스를 의미합니다. 따라서 SGD에서는 각 반복에 대한 전체 데이터 세트 대신 **무작위로 하나의 샘플이 선택**됩니다. 이 방법은 고차원 최적화 문제에서 매우 높은 계산 부담을 줄여 더 빠른 반복을 달성하면서 수렴 속도가 낮아집니다. 이것은 실제 그라디언트(전체 데이터 세트에서 계산된)를 그것의 추정치(무작위로 선택된 데이터의 하위 집합에서 계산된)로 대체하는 경사 하강법 최적화의 확률적 근사치로 볼 수 있습니다.

</br>

## #7
### Test 세트가 오염되었다는 말의 뜻은?

Test data에 Training data와 일치하거나, 매우 유사한 데이터들이 포함되어, **test에 overfit하여 unseen data에 대한 성능이 떨어질 수 있다**는 의미입니다.

</br>

## #8
### Bias는 왜 있는걸까?

Bias는 타겟과 예측값이 얼마나 멀리 떨어져 있는가입니다. 모델의 복잡도가 낮거나 학습이 덜 된 경우 bias가 커집니다.   
반면 학습 단계에서 Bias는 모델에서 데이터의 편향을 보정하는 역할을 합니다. Bias를 통해 Activation Function을 좌우로 움직일 수 있어, 더욱 좋은 학습을 시킬 수 있습니다.

</br>

## #9
### Back Propagation에 대해서 쉽게 설명 한다면?

Back Propagation(역전파)은 신경망에서 가중치와 바이어스를 조정하기 위해 **예측값과 실제값의 차이를 역으로 전파하는 알고리즘**입니다. 따라서 먼저, 입력층에서 출력층까지 순전파를 수행하여 예측값을 구한 후, 예측값과 실제값의 차이를 계산하여 손실(loss)을 구합니다. 이 손실값을 역으로 전파하면서 각 레이어의 가중치와 바이어스를 조정합니다. 이 과정을 반복하면 손실이 최소화되는 최적의 가중치와 바이어스를 찾을 수 있습니다. Back Propagation은 딥러닝에서 학습의 핵심 알고리즘이며, 경사 하강법과 함께 사용되어 네트워크의 가중치를 조정하여 예측 성능을 향상시킬 수 있습니다.


</br>

### 원-핫 인코딩이란?

원-핫 인코딩은 각 범주형 변수를 이진 벡터로 변환하는 방식입니다. 따라서 각 범주에 대해 하나의 이진 변수를 만들고, 해당하는 범주에 해당하는 변수만 1로 표시하고 나머지는 0으로 표시하는 방식입니다. 이러한 원-핫 인코딩은 범주 간의 관계를 없애고 각 범주를 독립적으로 처리할 수 있도록 도와주며, 데이터 형태는 0-1로 이루어졌기 때문에 컴퓨터가 인식하고 학습하기에 용이합니다. 그러나 원-핫 인코딩은 범주의 개수에 따라 변수의 차원이 증가하며, 고차원 데이터에 대해 메모리와 계산 비용이 증가할 수 있는 단점이 있습니다.

### Ordinal Encoding과 OneHotEncoding는 언제 사용해야할까요? 어떠한 특징들이 있나요?

Ordinal Encoding은 범주형 데이터를 순서에 따라 숫자로 매핑하는 방법입니다. 반면, OneHotEncoding은 각 카테고리에 대해 하나의 이진 변수를 만들고, 해당하는 카테고리에 해당하는 변수만 1로 표시하고 나머지는 0으로 표시하는 방식입니다.  
Ordinal Encoding은 이해하고 구현하기 쉽기 때문에 데이터에 순서가 있는 경우 유용합니다. 그러나 데이터에 순서가 없는 경우 오류가 발생할 수 있습니다. OneHotEncoding은 카테고리간 독립성을 가정하기 때문에 데이터에 순서가 없는 경우에 적합하지만, 데이터가 많으면 계산 비용이 많이 들고 데이터에 불필요한 차원을 추가할 수도 있습니다.

### n-gram은 무엇인가요?

n-gram은 텍스트 또는 문장을 n개의 연속된 단어 또는 문자의 그룹으로 분할하는 방법입니다. 여기서 "n"은 연속된 요소의 개수를 의미합니다. n-gram은 이전 단어들을 기반으로 다음 단어를 예측하는 데 사용할 수 있으며, 텍스트 분류, 문서 유사도 계산, 텍스트 생성 등의 작업에서도 유용하게 활용됩니다. 그러나 큰 n 값은 데이터의 희소성 문제를 야기할 수 있으며, 작은 n 값은 단어 간 상관 관계를 캡처하지 못할 수 있습니다. 따라서, 적절한 n 값 선택은 n-gram을 사용할 때 고려해야 합니다. 또한, 앞 문장의 맥락을 놓처 문장 내 전체 단어를 고려하는 언어 모델과 비교했을 때 정확도가 낮다는 한계가 있습니다.

### Non-Linearity라는 말의 의미와 그 필요성은?

선형함수는 입력값과 출력값이 비례하는 함수(1차 함수)입니다. 이러한 선형함수를 제외한 함수는 비선형 함수입니다. 이때 비선형성은 모델이 여러 레이어를 거치면서 보다 복잡한 패턴을 학습할 수 있게 해줍니다. 만약 선형함수로 여러 레이어를 쌓는다면, 이는 결국 하나의 선형 레이어를 거친 모델과 동일하게 됩니다.

### GD 중에 때때로 Loss가 증가하는 이유는?

손실이 증가하는 이유는 몇 가지가 있습니다. 학습률 크기로 인해 파라미터를 너무 크게 조정할 수 있으며, 지역 최소값에 도달하여 손실이 증가할 수 있습니다. 또한 모델이 학습 데이터에 지나치게 적합되어 새로운 데이터에 대한 일반화 성능이 저하되는 경우 손실이 증가할 수 있고, 그래디언트 계산의 불안정성으로 인해 그래디언트 소실 또는 폭주 문제가 발생할 수 있습니다.  
이 문제를 해결하기 위해서는 적절한 학습률 설정, 초기 파라미터 선택, 정규화 등의 방법을 고려해야 합니다.  
이 외에도 각 optimization 전략에 따라 gradient가 양수인 방향으로도 parameter update step을 가져가는 경우가 생길 수 있으며, 이 경우에는 Loss가 일시적으로 증가할 수 있습니다.



---
## #Network
## #10
### OSI 7계층을 설명하시오
OSI 7 계층은 **네트워크에서 통신이 일어나는 과정을 7단계**로 나눈 것을 말합니다. 7계층으로 나눈 이유는 통신이 일어나는 과정을 단계별로 파악하기 위함과 **통신 과정 중에 특정한 곳에 이상이 생길 경우**에 다른 단계의 장비 및 소프트웨어 등을 건드리지 않고 **통신 장애를 일으킨 단계에서 해결할 수 있기 때문**입니다. 계층으로는 물리 계층, 데이터 링크 계층, 네트워크 계층, 전승 계층, 세션 계층, 표현 계층, 응용 프로그램 계층이 있습니다.

> OSI(Open Systems Interconnection Reference Model) 7계층이란, 통신 접속에서 완료까지의 과정을 7단계로 정의한 국제 통신 표준 규약입니다.  
**물리** 계층(physical layer)은 0과 1의 전기적 신호를 전송하는데 필요한 기능을 제공합니다. 장비로는 통신 케이블, 허브가 있습니다.  
데이터링크 계층(Data-Link Layer)은 프레임(Frame)이라는 단위로 MAC 주소를 가지고 같은 네트워크에 있는 컴퓨터들이 데이터를 주고받게 해주는 모듈입니다. 장비로는 브릿지, 스위치가 있습니다.  
네트워크 계층(Network Layer)은 목적지 컴퓨터로 데이터를 전송하기 위해 네트워크 간의 IP 주소를 통해 최적의 경로로 데이터 전달하는 라우팅 기능을 제공합니다. 장비로는 라우터, L3 스위치가 있습니다.  
\* IP : 각 컴퓨터들이 갖는 고유한 주소    
전송 계층(Transport Layer)은 데이터 전송을 위해서 Port 번호를 사용하여, 목적지 컴퓨터의 최종 도착지라고 할 수 있는 프로세스까지 데이터가 전송될 수 있게 하는 모듈입니다.  
세션 계층(Session Layer)은 통신 장치 간 상호작용과 동기화를 제공하여, 데이터 교환과 에러 발생 시 복구를 관리해주는 모듈입니다.  
표현 계층(Presentation Layer)은 세션 계층 간의 주고받는 인터페이스를 일관성있게 제공하는 역할을 합니다.  
애플리케이션 계층(Application Layer)은 사용자가 다른 컴퓨터 간의 통신을 가능하게 하며, 이를 위해 다양한 프로그램과 프로토콜을 사용합니다.

</br>

## #11
### TCP/IP의 각 계층을 설명해주세요.

TCP/IP 4계층은 애플리케이션 계층, 전송 계층, 인터넷 계층, 네트워크 접근 계층으로 이루어져있습니다. TCP/IP는 2개의 계층으로 구분됩니다. 메세지나 파일을 작은 패킷으로 나누거나 재조립하여 송수신에 반영하는 일을 담당하는 TCP(상위계층), 각 패킷의 주소 부분들을 처리하여 패킷들이 목적지로 정확히 송수신되도록 기능하는 IP(하위계층)으로 나눠집니다.  
하지만, 인터넷 개발 이후 꾸준히 표준이 갱신되면서 하위 레이어가 다시 세분화되었고, 오른쪽의 TCP/IP Updated 모델이 탄생했습니다. TCP/IP Updated의 5계층 모델은 Link를 다시 두 레이어로 세분화하고, Internet 명칭을 Network로 다시 변경했다는 차이가 있습니다.

</br>

## #12
### OSI 7계층와 TCP/IP 계층의 차이를 설명해주세요.

OSI와 TCP/IP는 **둘 다 네트워크 통신 모델의 표준**입니다.  
**OSI** 7계층은 네트워크 전송 시 데이터 표준을 정리한 개념적 모델로 통신에는 **실질적으로 사용되지 않습니다**.  
**TCP/IP는 OSI을 상업적이고 실무적으로 이용될 수 있도록 단순화**한 것이라고 할 수 있습니다. 따라서 현대의 인터넷은 TCP/IP 모델을 표준으로 따르고 있습니다.


</br>

## #13
### TCP와 UDP의 차이는?

TCP와 UDP는 모두 데이터를 보내기 위해 사용하는 프로토콜입니다.  
TCP는 패킷을 전송하기 위한 논리적 경로를 배정하는 연결형 서비스로, 3-way handshaking으로 전송의 신뢰성을 보장하지만 속도는 UDP보다 느립니다. 파일 전송 등 연속성보다 **신뢰성이 중요할 때 사용**됩니다.  
UDP는 데이터를 데이터그램 단위로 처리하기 때문에 각각 패킷이 서로 다른 경로로 독립적으로 처리됩니다. UDP헤더의 checksum으로 최소한의 오류만을 검출하기 때문에 신뢰성이 낮지만, TCP보다 속도가 빠르고 네트워크 부하가 적습니다. 신뢰성보단 **연속성이 중요한 스트리밍 등에 사용**됩니다.

> - 3-way handshake는 TCP/IP 프로토콜에서 연결을 설정하는 과정으로, 클라이언트가 서버에게 SYN 패킷을 보내고, 서버가 클라이언트에게 SYN-ACK 패킷을 보내고, 클라이언트가 다시 서버에게 ACK 패킷을 보내는 과정입니다. 3-way handshake를 이용하는 이유는 TCP/IP 프로토콜에서 두 호스트 간에 신뢰성 있는 연결을 설정하기 위해 사용하는 방법으로, 양쪽 호스트가 상대방과 통신 가능한 상태인지 확인하고 연결을 설정하기 때문입니다.

</br>

## #14
### REST와 RESTful의 개념을 설명하고 차이를 말해주세요.

REST(Representational State Transfer)란 HTTP 프로토콜을 기반으로 분산 시스템에서 웹 서비스를 구현하기 위한 아키텍처 스타일을 말합니다. RESTful은 REST 아키텍처 스타일을 따르는 웹 서비스를 구현하는 것을 말하며, 이를 구현하기 위해 몇 가지 규칙을 준수해야 합니다. RESTful 웹 서비스는 자원을 URL로 표현하고, HTTP 메서드를 사용하여 자원에 대한 CRUD(Create, Read, Update, Delete) 작업을 수행합니다. 또한, 클라이언트-서버 구조, 상태가 없음(Stateless), 캐시가능(Cacheable), 계층화(Layered System) 등의 제약 조건을 준수하여야 합니다.


</br>

### HTTP GET과 POST 메서드를 비교/설명해주세요

**GET은 서버의 리소스에서 데이터를 요청**할 때, **POST는 서버의 리소스 변경을 요청**할 때 사용합니다.  
GET을 통한 요청은 URL 주소 끝에 `?` 이후로 데이터를 추가하여 전송합니다. 이 방식은 url 이라는 공간에 담겨가기 때문에 전송할 수 있는 데이터의 크기가 제한적이며, 보안이 필요한 데이터에 대해서는 **데이터가 그대로 url에 노출**되는 문제가 있습니다. 그러나 **GET 요청은 브라우저에서 캐시를 사용하므로, 같은 요청을 반복할 경우 빠르게 응답할 수 있습니다.**   
POST는 전송할 데이터를 HTTP 메시지 body 부분에 담아서 서버로 보냅니다. **POST 요청은 URL에 정보가 노출되지 않으므로**, GET 요청과 달리 보안 취약점이 적습니다. 또한 POST 요청은 데이터를 전송할 수 있기 때문에, 서버에 데이터를 변경하는 요청에 적합합니다. 하지만 POST 요청도 개발자 도구나 fiddler 같은 툴로 요청 내용을 확인할 수 있기 때문에 민감한 데이터의 경우에는 반드시 암호화해 전송해야 합니다.

### 3-way handshake에 대해 설명해주세요.

![](https://gmlwjd9405.github.io/images/network/3-way-handshaking.png)

3-way handshake는 TCP/IP 프로토콜에서 연결을 설정하는 과정으로, **클라이언트가 서버에게 SYN 패킷을 보내고, 서버가 클라이언트에게 SYN-ACK 패킷을 보내고, 클라이언트가 다시 서버에게 ACK 패킷을 보내면 연결이 성립**되는 과정 입니다. 3-way handshake를 이용하는 이유는 TCP/IP 프로토콜에서 **두 호스트 간에 신뢰성 있는 연결을 설정하기 위해 사용하는 방법**으로, **양쪽 호스트가 상대방과 통신 가능한 상태인지 확인하고 연결을 설정**하기 때문입니다.
- SYN : SYNchronize sequence number ; 클라이언트가 서버에 연결 요청을 보낼 때 사용
- ACK : ACKnowledgement ; 수신 측이 데이터를 올바르게 수신했음을 송신 측에 알리는 역할

> + **왜 2 way handshake는 안될까?**  
  TCP는 양방향 연결이기 때문에 클라이언트가 서버에게 존재를 알리고 서버에서도 클라이언트에게 존재를 알리고 대답을 얻어야된다.   
  1 단계. 클라이언트가 서버에게 존재를 알린다.  
  2단계. 서버가 클라이언트의 존재를 알았다고 대답을 하면서 클라이언트에게 내 존재를 알린다.   
3단계. 클라이언트가 서버의 존재를 알았다고 대답을 한다.

### HTTP와 HTTPS에 대해서 설명하고 차이점에 대해 설명해주세요.

HTTP (Hypertext Transfer Protocol)와 HTTPS (Hypertext Transfer Protocol **Secure**)는 **인터넷에서 데이터를 전송하기 위한 프로토콜**입니다. 특히, **HTTPS는 HTTP에 데이터 암호화가 추가된 프로토콜**입니다.   
즉, **HTTP 통신하는 소켓 부분을 `SSL(Secure Socket Layer)` 또는 `TLS(Transport Layer Security)`라는 프로토콜로 대체**한 것이라고 볼 수 있습니다. 따라서 HTTP 는 원래 TCP 와 직접 통신했지만, HTTPS 에서 HTTP 는 SSL 과 통신하고 **SSL 이 TCP 와 통신** 하게 됩니다. 이러한 SSL (Secure Sockets Layer) 또는 TLS (Transport Layer Security) 프로토콜을 사용하여 데이터를 암호화하여, **데이터가 전송되는 동안 중간에 누군가가 데이터를 볼 수 없도록 보호**합니다.   
이러한 HTTPS를 이용하면 암호화/복호화의 과정이 필요하기 때문에 HTTP보다 속도가 느립니다. 또한 HTTPS는 인증서를 발급하고 유지하기 위한 추가 비용이 발생합니다.  
하지만 최근에는 하드웨어의 발달로 인해 HTTPS를 사용하더라도 속도 저하가 거의 일어나지 않게 되면서, 모든 웹페이지에서 HTTPS를 적용하는 방향으로 바뀌어가고 있다고 알고있습니다.


---

## #Database
## #13
### 데이터베이스를 설계할 때 가장 중요한 것이 무엇이라고 생각하나요?  

**데이터베이스의 목적과 사용자 요구 사항을 잘 이해하고 이를 바탕으로 효율적이고 일관된 데이터 모델을 설계**하는 것입니다. 따라서 데이터베이스를 설계할 때는 어떤 데이터가 어떻게 사용되는지, 어떤 종류의 데이터가 필요한지, 어떤 연산이 자주 수행되는지 등을 고려해야 합니다.
또한 데이터베이스의 설계는 **데이터의 일관성, 무결성, 보안** 등을 보장해야 합니다. 이를 위해, **테이블 간의 관계**를 잘 설정하고, **적절한 제약 조건**을 설정하여 **데이터의 무결성**을 유지하고, 보안 측면에서도 **적절한 접근 권한 설정**을 고려해 주어야 합니다.
마지막으로 데이터베이스는 변화하는 요구 사항에 대응할 수 있도록 설계되어야 합니다. 이를 위해 유지보수 및 확장성을 고려하여 데이터베이스를 설계해야 합니다.

</br>

## #14
### 최소 신장 트리에 대해서 설명해 주세요.

신장트리는 **최소한의 간선으로 그래프 내의 모든 정점을 포함하도록 하는 트리**입니다. 최소 신장 트리란 spanning tree 중 간선의 **weight의 합이 최소가 되는 트리**를 말합니다. 사이클이 존재해서는 안되며 최단 경로를 보장하지 않는다는 특징이 있습니다. 통신망, 유통망 등 **길이, 구축 비용, 전송 시간 등을 최소로 구축하는 경우에 사용**할 수 있으며 **Kruskal 등의 알고리즘을 이용해 구현**할 수 있습니다.


</br>

## #12
### 데이터 베이스에서 인덱스(색인)이란?

데이터베이스에서 인덱스란 테이블에 대한 검색 속도를 향상시켜주는 자료구조로, 테이블의 컬럼 값과 해당 레코드의 물리적인 주소를 매핑하는 것입니다. 이 매핑 정보를 이용하여 데이터베이스에서 특정 값을 빠르게 찾을 수 있습니다. 이러한 인덱스는 Btree, B+tree, Hash, Bitmap로 구현됩니다. 데이터를 추가/변경/삭제할 경우 이에 대응하는 인덱스를 수정해야 하기 때문에 데이터베이스의 성능에 많은 부담을 줍니다.


</br>

### 데이터 베이스에서 정규화는 무엇인가요?

**정규화**는 데이터베이스 설계에서 **중복을 최소화하고 데이터를 구조화하는 과정**으로, 데이터의 무결성과 일관성을 유지하며 데이터 저장 및 검색 효율성을 향상시키기 위해 수행됩니다.  
1차 정규화부터 5차 정규화까지 존재하며, 각 정규화 단계마다 특정한 조건을 만족시켜야 합니다. 이때 정규화를 수행하게 될수록 제약이 많아지며, 보다 데이터의 무결성과 일관성을 유지할 수 있습니다.  
이러한 정규화는 3차까지 수행하면 대부분의 중복과 종속성 문제를 해결할 수 있기 때문에, **일반적으로 3차 정규화이나 BCNF(Boyce-Codd; 보이스-코드 정규화)까지 수행**합니다. 더 높은 정규화를 적용하면 데이터베이스의 구조가 복잡해져서 유지보수가 어려워지고, 새로운 요구사항에 대응하기도 어려워질 수 있기 때문입니다.

### 반정규화에 대해 설명해주세요.

반정규화(Denormalization)란 정규화된 엔터티, 속성, 관계에 대해 **시스템의 성능향상과 개발과 운영의 단순화를 위해 중복, 통합, 분리 등을 수행하는 데이터 모델링의 기법**을 말합니다. 반정규화를 할 경우 **조회(select) 속도를 향상시키지만, 데이터 모델의 유연성은 낮아**집니다.  
데이터를 조회할 때 디스크 I/O량이 많아서 성능이 저하되거나 경로가 너무 멀어 조인으로 인한 성능저하가 예상되거나 칼럼을 계산하여 읽을 때 성능이 저하될 것이 예상되는 경우 반정규화를 수행하게 됩니다.

## OS
### 페이지 교체 알고리즘과 어떤 종류들이 있는지 설명해주세요.

페이징 기법으로 메모리를 관리하는 운영체제에서 필요한 페이지가 주기억장치에 적재되지 않았을 시 어떤 페이지 프레임을 선택해 교체할 것인지 결정하는 방법을 페이지 교체 알고리즘이라고 합니다. 
종류로는 OPT, FIFO, LRU, LFU 등이 있습니다.
> - OPT - Optimal: 앞으로 가장 오랫동안 사용되지 않을 페이지 교체
> - FIFO - First In First Out: 가장 먼저 들어온 페이지를 교체
> - LRU - Least Recently Used: 가장 오랫동안 사용되지 않은 페이지를 교체
> - LFU - Least Frequently Used: 참조 횟수가 가장 적은 페이지 교체
> - MFU - Most Frequently Used: 참조 횟수가 가장 많은 페이지 교체
> - NUR - Not Used Recently: 최근에 사용하지 않은 페이지 교체
> - SCR - Second Chance Replacement: FIFO에서 한 번 더 기회를 주고 교체
> - 클럭 알고리즘: SCR과 동일


---
## #Operating System
## #10
### 프로세스와 스레드의 차이(Process vs Thread)를 알려주세요.

프로세스는 프로그램을 실행해 운영체제로부터 자원을 할당받은 **작업의 단위**이고, 스레드는 프로세스가 할당받은 자원을 이용하는 **실행 흐름의 단위**입니다. 스레드는 프로세스 내 여러개 생길 수 있으며, **자원 공유가 가능**합니다.

> 프로세스는 운영체제로부터 시스템 자원을 할당받는 작업의 단위로 **메모리에 올라와 실행되고 있는 프로그램의 인스턴스**를 의미합니다. 프로세스는 각각 독립된 메모리 영역(**Code, Data, Stack, Heap**의 구조)을 할당받습니다.  
스레드는 프로세스가 할당받은 자원을 이용하는 **실행의 단위**로 프로세스와는 다른 **더 작은 실행 단위 개념**입니다. 스레드는 프로세스의 코드에 정의된 절차에 따라 실행되는 특정한 수행 경로입니다. 이러한 스레드는 컴퓨터의 중앙 처리 장치(CPU)의 서로 다른 코어에서 프로그램의 여러 부분을 동시에 실행할 수 있기 때문에 병렬 처리를 위해 사용될 수 있습니다.

<br/>

## #11
### 멀티 프로세스와 멀티 스레드를 사용하는 이유를 각각 설명해주세요.

멀티 프로세스는 안정성이 높고 병렬 처리에 특화되어 있으며, 멀티 스레드는 빠른 속도와 경제성을 보유하고 있습니다.

> 멀티 프로세스는 **안정성이 높아** 하나의 프로세스가 잘못돼도 다른 프로세스는 작동하지만, **context switching** 비용이 발생합니다.  
멀티 스레드는 시스템 자원 소모 감소, 처리비용 감소, 스레드간 **자원공유** 등의 장점이 있지만 디버깅이 어렵고, **동기화 이슈발생**, 하나의 스레드의 오류로 전체 프로세스에 문제가 생길 수 있다는 단점이 있습니다.

> **멀티 프로세스는** 하나의 응용프로그램을 **여러 개의 프로세스에서 동시에 실행하여 다중 작업(multi-tasking)을 지원**하는 방법입니다. 각 프로세스는 별도의 메모리 공간을 할당받으므로 안정성이 높고, 하나의 프로세스에 문제가 생겨도 다른 프로세스는 정상적으로 동작한다는 장점이 있습니다.  
**멀티 스레드는** 하나의 응용프로그램을 **하나의 프로세스 안에서 여러 개의 스레드를 동시에 실행하여 다중 작업을 지원**하는 방법입니다. 스레드 간 메모리를 공유하므로 자원을 효율적으로 관리할 수 있지만, 동시에 메모리에 접근하는 경우 데이터 불일치 문제가 발생할 수 있습니다.   
멀티 프로세스와 멀티 스레드 모두 여러 작업을 동시에 처리하여 성능을 향상시킬 수 있습니다.

> 멀티 프로세스는 두 개 이상 다수의 프로세서가 협력적으로 하나 이상의 작업을 동시에 처리하는 것으로 각 프로세스 간 메모리 구분이 필요하거나 독립된 주소 공간을 가져야 할 경우 사용한다. 독립된 구조로 안전성이 높은 장점을 가지고 있지만 작업량이 많을 수록 오버헤드가 발생하여 성능 저하가 발생할 수 있다는 단점을 가지고 있다.  
멀티 스레드는 하나의 프로세스를 여러 스레드로 자원을 공유하며 작업을 나누어 수행하는 것이다. 시스템 자원 소무를 감소하고 자원을 효율적으로 관리할 수 있지만, 병목현상, 데드락 등 자원을 공유하기에 동기화 문제가 발생할 수 있다. 또한 하나의 스레드에 문제가 생기면 전체 프로세스가 영향을 받는다.

<br/>

## #12
### 동기와 비동기의 차이는 무엇인가요?

동기는 **제어권의 반환과 결과값**이 전달하게 되는 시간이 일치하게 되는 것이고,
비동기는 제어권의 반환과 결과값의 전달 시간이 일치하지 않을 수 있는 것입니다.  
> 동기는 데이터의 요청과 결과가 한 자리에서 동시에 일어나기 때문에 **설계가 매우 간단하고 직관적**이지만 결과가 주어질 때까지 아무것도 못하고 대기해야 한다는 단점이 있습니다.  
비동기는 요청에 따른 결과가 반환되는 시간 동안 **다른 작업을 수행할 수 있지만, 동기식보다 설계가 복잡**합니다.

<br/>

## #15
### 뮤텍스와 세마포어의 차이를 설명해주세요.

뮤텍스와 세마포어의 가장 큰 차이점은 **동기화 대상의 개수**입니다. 뮤텍스는 동기화 대상이 1개일 때 사용하지만 세마포어는 동기화 대상이 1개 이상일 때 사용합니다. 세모포어는 뮤텍스가 될 수 있지만, 뮤텍스는 세마포어가 될 수 없습니다. 뮤텍스는 자원 소유가 가능하지만 세마포어는 자원 소유가 불가합니다. 또한 뮤텍스는 소유하고 있는 스레드만이 뮤텍스를 해제할 수 있는 반면, 세마포어는 세마포어가 소유하지 않는 스레드가 세마포어를 해제할 수 있습니다.

> **뮤텍스**는 한 **쓰레드, 프로세스에 의해 소유될 수 있는 Key를 기반으로 한 상호배제기법**입니다. Locking 메커니즘으로 오직 하나의 쓰레드만이 동일한 시점에 뮤텍스를 얻어 임계 영역(Critical Section)에 들어올 수 있습니다. 그리고 오직 이 쓰레드만이 임계 영역에서 나갈 때 뮤텍스를 해제할 수 있습니다.  
**세마포어**는  **현재 공유자원에 접근할 수 있는 쓰레드, 프로세스의 수를 나타내는 값을 두어 상호배제를 달성**하는 기법입니다. wait를 호출하면 세마포어의 카운트를 1줄이고, 세마포어의 카운트가 0보다 작거나 같아질 경우에 락이 실행됩니다. Signaling 메커니즘으로 락을 걸지 않은 쓰레드도 signal을 사용해 락을 해제할 수 있습니다. 세마포어의 카운트를 1로 설정하면 뮤텍스처럼 활용할 수 있습니다.

</br>

## #16
### 교착상태(데드락, Deadlock)의 개념과 조건을 설명해주세요.

데드락이란 **두 개 이상의 프로세스나 스레드가 서로 자원을 얻지 못해서 다음 처리를 하지 못하는 상태로 무한히 다음 자원을 기다리게 되는 상태**를 말합니다.  
데드락은 상호 배제(mutual exclusion), 점유 대기(hold and wait), 비선점(no preemition), 순환 대기(circular wait)의 4조건을 모두 만족해야 성립합니다.

> 교착상태가 발생하는 4가지 조건이 있는데 한번에 오직 한개의 작업만 자원에 접근할 수 있는 **상호배제**, 프로세스가 할당된 자원을 가진 생타에서 다른 자원을 기다리는 **점유대기**, 프로세스가 어떤 자원의 사용을 끝낼 때까지 그 자원을 뻇을 수 없는 **비선점**, 각 프로세스가 순환적으로 다음 프로세스가 요구하는 자원을 가지고 있는 **순환대기**가 있습니다. 이 조건 중에 한가지라도 만족하지 않으면, 교착상태는 발생하지 않습니다.
> 이중 순환대기 조건은 점유대기 조건과 비선점 조건을 만족해야 성립하는 조건이므로, 위 4가지 조건은 서로 완전히 독립적인 것은 아닙니다.

<br/>

## #10
### 스케줄러가 무엇이고, 단기/중기/장기로 나누는 기준에 대해 설명해주세요.

스케줄러란 **어떤 프로세스에게 자원을 할당할지를 결정하는 운영체제 커널의 모듈**을 지칭합니다.  
**단기 스케줄링**은 프로세서 스케줄러라고 부르며 메인 메모리의 준비상태에 있는 작 업 중에서 실행할 작업을 선택하고 프로세서를 배당하는 일을 합니다.  
**중기 스케줄링**은 현재 생성되어 있는 프로세스 중에 비효율적으로 시스템의 자원을 낭비 하고 있는 프로세스가 있을 경우 보조기억장치로 추방하는 스케줄링입니다.  
**장기 스케줄링**은 작업 스케줄러라고 부르기도 하며 어떤 작업이 시스템에 들어와서 스케 줄링 원칙에 따라 디스크 내의 어떤 작업을 어떤 순서로 메모리에 가져와서 처리할 것인 가를 결정하는 프로그램입니다.  
단기 스케줄러는 CPU와 메모리 사이의 스케줄링을 담당하고 장기 스케줄러는 메모리와 디스크 사이의 스케줄링을 담당합니다. 중기 스케줄러는 스와핑(Swapping)을 통해 프로세스들이 CPU경쟁이 심해지는 것을 방지하는 역할을 합니다.  
현재는 가상메모리 관리를 이용해 장기, 중기 스케줄러는 거의 쓰이지 않습니다.

</br>

## #11
### CPU 스케줄러인 FCFS, SJF, SRTF, RR, Priority Scheduling에 대해 간략히 설명해주세요.

CPU 스케줄링은 CPU를 사용중인 프로세스가 자율적으로 반납하는 비선점 스케줄링과 OS가 알고리즘에 따라 적당한 프로세스에게 CPU를 할당하고 필요시에는 회수하는 방식인 선점 스케줄링이 있습니다.  
비선점 스케줄링은 먼저 CPU를 요청하는 프로세스를 먼저 처리하는 **FCFS**(First Come First Served), 버스트 시간이 짧은 프로세스부터 처리하는 **SJF**(Shortest Job First)가 있습니다.    
선점 스케줄링에는 최단 잔여시간을 우선으로 하는 **SRT**(Shortest Remaining Time), 모든 프로세스가 같은 우선순위를 갖고 time slice를 기반으로 스케줄링하는 **RR**(Round Robin)이 있습니다.  
이 외에도 우선순위가 높은 프로세스에 CPU를 먼저 할당하는 **Priority Scheduling**이 있습니다.  
\* Burst time: 프로세스가 CPU를 사용하는 시간  

> `FCFS(First Come First Service)`는 Ready Queue에 먼저 도착한 작업부터 처리하는 알고리즘입니다. 이 방식은 실행 순서에 따라 평균대기시간 차이가 커지게 됩니다. 또한, 중요한 작업이 있다 하더라고 그 작업 보다 먼저 들어온 작업이 끝나기 전까지는 실행될 수 없다는 단점이 있습니다.  
`SJF(Shortest Job First)`는 다음에 실행될 작업 중 수행시간이 가장 짧은 작업을 먼저 처리하는 알고리즘 입니다. 이 방식은 실행시간을 예측한다는 점에서 비현실적이며 계속해서 짧은 프로세스만 처리하므로 긴 프로세스는 뒤로 밀린다는 단점이 있습니다.  
`SRTF(Shortest Remaining Time First)`는 남은 처리시간이 더 짧은 작업을 먼저 처리하는 알고리즘 입니다. 이 방식은 잔여 시간을 계속 추적해야 하므로 overhead가 크며, 구현 및 사용이 비현실적이라는 단점이 있습니다.  
`RR(Round Robin)`은 FCFS의 방식에서 자원 사용 시간(time quantum)을 제한하는 알고리즘입니다. 이 방식은 너무 잦은 context switching은 중앙처리장치 이용률을 저하할 수 있다는 단점이 있습니다.  
`Priority Scheduling`은 각 프로세스에 지정된 우선순위를 기준으로 높은 우선순위를 가진 작업을 먼저 처리합니다. 이 방식은 우선순위가 낮은 프로세스는 뒤로 밀리는 문제가 발생합니다. 이는 일정 시간을 기다리면 프로세스의 우선순위를 높여주는 aging 방식으로 해결할 수 있습니다.


</br>

---

## #Data Structure
### 배열(array)와 연결리스트(linked list)의 각각의 특징과 장단점을 설명해주세요.

Array는 **연속된 메모리 주소**를 할당받게 되는데 이때 index를 갖게됩니다. Array는 index를 가지고 임의 접근이 가능하고 접근과 탐색에 용이하다는 장점을 가지고 있지만 크기를 미리 정해놓았기 때문에 해당 배열 크기 이상의 데이터를 저장할 수 없다는 단점이 있습니다.  
Linked List는 동적 자료구조로 크기를 정할 필요가 없고 배열처럼 연속된 메모리 주소를 할당받지 않습니다. 대신 노드(Node) 안에 데이터가 있고, 다음 데이터를 가리키는 주소를 가지고 있습니다. Linked List는 **크기의 제한이 없으므로 데이터 추가, 삭제가 자유롭다**는 장점이 있지만, 메모리 주소를 할당받지 않았기 때문에 임의로 접근하는 것은 불가능하여 데이터를 탐색할 때 순차적으로 접근해야 한다는 단점이 있습니다.

> **배열은** 메모리상에서 연속적으로 저장되어 있기 때문에, index를 통한 접근이 용이합니다. 배열의 크기는 처음 생성할 때 정하며 이후에는 변경할 수 없다는 특징이 있습니다.  
**연결 리스트**는 여러 개의 노드들이 순차적으로 연결된 형태를 갖는 자료구조이며, 첫번째 노드를 헤드(Head), 마지막 노드를 테일(Tail) 이라고 하며, 이때 각 노드는 데이터와 다음 노드를 가리키는 포인터로 이루어져있습니다. 배열과는 다르게 메모리를 연속적으로 사용하지 않기 때문에 접근은 불리할 수 있으나, **크기의 제한이 없어, 데이터 추가, 삭제가 자유롭다는 특징이 있습니다.**

<br/>

## #17
### 캐시의 지역성에 대해 설명해주세요.

캐시 메모리는 적중률(Hit rate)을 극대화하기 위해 데이터 **지역성(Locality)**의 원리를 사용합니다. 지역성(Locality)이란 기억 장치 내의 정보를 균일하게 액세스하는 것이 아닌 **특정 부분을 집중적으로 참조하는 특성**으로, 프로그램이 소규모의 특정 데이터 및 명령어 집합에 반복적으로 액세스하는 경향이 있다는 사실을 의미합니다.  
최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성인 **시간 지역성**과 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성인 **공간 지역성**이 있습니다.

> 캐시 메모리는 CPU의 처리 속도와 메모리의 속도 차이로 인한 병목현상을 완화하기 위해 사용하는 고속 버퍼 메모리입니다. 주기억장치에 있는 데이터를 액세스하려면 비교적 오랜 시간이 걸리게 되는데 이를 줄이기 위해 데이터를 빠르게 액세스할 수 있도록 중간에 캐시 메모리를 사용합니다. 따라서 데이터 요청이 들어오면, 원본 데이터가 담긴 곳에 접근하기 전에 먼저 캐시 내부부터 찾는데, 이때 원하는 데이터가 캐시에 있는 경우에 대한 비율을 캐시 메모리 적중률이라고 합니다.

---
## #Computer Science
### 0.1 + 1.1 = 1.2 가 false인 이유를 설명해주세요.

컴퓨터에서 실수를 저장할 때 정수부분과 실수부분을 나누어 앞과 뒤의 칸에 **2진법으로** 저장을 하게 되는데, 0.1단위 소숫점을 2진법으로 표현하면 무한히 길어지게 되어 **일정부분을 자르고 저장**을 하게 됩니다. 이때 잘린 만큼의 오차가 있기때문에 0.1+1.1=1.2 가 False 값으로 나오게 됩니다.  
파이썬에서는 `decimal` 모듈을 사용해 해결할 수 있습니다.

<br/>

## #Python
## #15
### Python은 어떤 특징을 가진 언어인가요?

파이썬은 **interpreter language**라 실행 전에 컴파일할 필요가 없습니다. **동적 타이핑**이기 때문에 실행시간에 자료형을 검사하므로 자료형을 명시할 필요가 없습니다.

>  파이썬은 인터프리터 언어이므로, 실행하기 전에 컴파일을 할 필요가 없습니다. 그러나 컴파일러가 코드를 기계어로 번역해서 실행가능 파일을 만드는 것에 비해, 인터프리터는 코드를 한줄씩 실행할 때마다 번역해서 실행하기 때문에 다른 컴파일 언어에 비해 다소 느리다는 특징이 있습니다.  
또한, 파이썬은 클래스와 구성 및 상속을 함께 정의할 수 있는 **객체** **지향** **프로그래밍(OOP)**입니다**.**

## #15
### @classmethod, @staticmethod, @property은 어떤 것인가요?

@classmethod, @staticmethod, @property는 각각 클래스 메소드, 정적 메소드, 인스턴스 메서드의 동작을 수정하는 데 사용되는 데코레이터입니다.
@classmethod는 첫 번째 인자로 클래스 자신을 받는 메소드를 정의할 때 사용됩니다. 이를 사용하여 클래스 변수에 접근할 수 있고, 클래스 메소드를 통해 클래스 레벨에서의 연산을 수행할 수 있습니다.
@staticmethod는 클래스나 인스턴스와 무관하게 실행될 수 있는 메소드를 정의할 때 사용됩니다. 이를 사용하면, 해당 메소드를 객체를 생성하지 않고 클래스명을 사용하여 직접 호출할 수 있습니다.
@property는 getter 메소드를 간단하게 생성할 수 있게 합니다. 이를 사용하면 인스턴스에 직접 접근하는 대신 메소드를 통해 간접적으로 접근할 수 있습니다. 따라서 코드의 안정성과 유지보수성이 향상될 수 있습니다.  
\* 정적 메소드 ? 클래스에 속하면서 객체에 독립적으로 호출 가능한 메소드  
\* getter 메소드 ? 클래스의 인스턴스 변수 값을 반환하는 메소드 (보통 함수 이름 앞에 'get_'이라는 접두사를 붙인다.)
<br/>

### 얕은 복사(shallow copy)와 깊은 복사(deep copy)의 차이는 무엇인가요?

파이썬에서 얕은 복사(shallow copy)와 깊은 복사(deep copy)의 차이점은 **복사된 객체의 참조 방식**에 있습니다.  
**얕은 복사는 원본 객체의 참조를 복사**하여 새로운 객체를 만듭니다. 이 경우, 새로운 객체와 원본 객체는 같은 메모리 주소를 참조하게 됩니다. 따라서 새로운 객체를 변경하면 원본 객체도 변경됩니다.  
반면에 **깊은 복사는 원본 객체의 값을 복사하여 완전히 새로운 객체를 만듭니다**. 이 경우, 새로운 객체와 원본 객체는 서로 다른 메모리 주소를 참조하게 됩니다. 따라서 새로운 객체를 변경해도 원본 객체는 변경되지 않습니다.  
파이썬에서 얕은 복사는 copy 모듈의 copy 함수를 사용하여 수행할 수 있으며, 깊은 복사는 copy 모듈의 deepcopy 함수를 사용하여 수행할 수 있습니다.


## #Database
### NoSQL과 RDBMS의 차이점을 설명해주세요.

**RDBMS**는 관계형 데이터베이스 관리 시스템을 의미합니다. 다른 테이블과 관계를 맺고 모여있는 집합체로 이해할 수 있습니다. 이러한 관계를 나타내기 위해 **외래 키(foreign key)** 를 사용한 테이블 간 Join이 가능하다는게 RDBMS의 가장 큰 특징입니다. 정해진 스키마에 따라 데이터를 저장하여야 하므로 **정확한 데이터 구조를 보장**하는 장점이 있습니다. 반면에 테이블간 관계를 맺고 있어 시스템이 커질 경우 JOIN문이 많은 복잡한 쿼리가 만들어질 수 있고 스키마로 인해 데이터가 유연하지 못해 나중에 스키마가 변경 될 경우 번거롭고 어렵다는 단점이 있습니다.  
**NoSQL**은 RDBMS와는 달리 테이블 간 관계를 정의하지 않습니다. 데이터 테이블은 그냥 하나의 테이블이며 따라서 일반적으로 테이블 간 Join도 불가능합니다. 빅테이터의 등장으로 인해 데이터와 트래픽이 기하급수적으로 증가함에 따라 RDBMS에 단점인 성능을 향상시키기 위해 등장했고 데이터 일관성은 포기하되 비용을 고려하여 여러 대의 데이터에 분산하여 저장하는 Scale-Out을 목표로 등장했습니다. **스키마가 없기 때문에 유연하며 자유로운 데이터 구조**를 가질 수 있고 언지든 저장된 데이터를 조정하고 새로운 필드를 추가할 수 있다는 장점이 있습니다. 반면에, 스키마가 존재하지 않기에 명확한 데이터 구조를 보장하지 않으며 데이터 구조 결정하기가 어려울 수 있다는 단점이 있습니다.
