# 추가 질문
- Gradient descent 종류, DL에서는 어떻게 사용되는지  
Gradient descent에서 mini-batch를 주로 쓰는 이유
  - 종류 : 경사하강법(Gradient descent), 배치 경사하강법(Batch gradient descent),확률적 경사 하강법(SGD),미니배치 경사 하강법(Minibatch gradient descent)
  - Batch 보다 빠르고 SGD 보다 낮은 오차율을 가진다는 장점이 있음, 데이터가 많아질 때 길어지는 시간이나 데이터의 손실을 줄이기 위해서입니다.


- Activation function 종류


- DL에서 likelihood가 어떻게 쓰이는지
  - 데이터를 잘 설명하는 모델(θ)을 찾고자 하고, 그러려면 likelihood를 높여야 함

- 쓰레드가 프로세스 내부에서 어떤 메모리를 공유하는지 (공유안하는 건 왜 그러는지)
  - 스레드는 프로세스 안에 존재하며 프로세스의 자원을 공유합니다. 이때 공유하는 자원의 영역은 code, data, heap영역이며 각 스레드는 고유의 stack메모리 영역과 레지스터을 갖게 됩니다.

- ReLU가 미분 못하는 건 어떻게 해결하는지 알고 있나??  
  - 큰 문제 없어서 일반적으론 0으로 쓴다. x가 0 이하일 경우 다 미분값 0이라 0으로 써도 상관 없을 것 같음.
 
- ReLU가 왜 비선형인지 
  - 선형성이 없기 때문에 --> 선형성이란 뭔지 설명
  - 선형성을 가지는 조건은 3가지인데 1. 선형적인 그래프, 2. 동차성, 3. 가산성입니다. 여기서 ReLU는  가산성이 성립하지 않아 선형성을 가지지 않으므로 비선형 함수임을 알 수 있습니다.


- 배열은 정적 자료구조인데 파이썬에서는 배열에 append로 추가가 가능하다. 왜 그런지 설명 가능한가?  
  - 파이썬에서는 아예 정적 배열을 지원하지 않고 동적 배열만을 지원한다. 
  - [파이썬 배열 구현체](https://seoyeonhwng.medium.com/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A6%AC%EC%8A%A4%ED%8A%B8-%EB%82%B4%EB%B6%80-%EA%B5%AC%EC%A1%B0-f04847b58286)  

- Data normalization을 PyTorch에서 어떻게 쓰는지
  - torchvision의 transforms를 활용하여 정규화를 적용할 수 있습니다.

---
# 그 외
- 들어봤는데 모르겠다는 것보다는 그냥 모르겠다고 하는게 좋을것 같다.
